{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: Cécile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "import re\n",
    "import string as s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "#     print(len(original))\n",
    "#     print(len(lemma))\n",
    "#     print(len(t))\n",
    "#     print(len(pos))\n",
    "#     print(len(new_pos))\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_adj = ['acoustic', 'accented', 'artificial', 'attentional', 'autoregressive', 'bidirectional', 'bilingual', 'cross-lingual', 'fluent',\n",
    "            'gated', 'generated', 'intelligible', 'labelled', 'phonetic', 'monolingual', 'multilingual', 'multispeaker', 'neural',\n",
    "            'substantial', 'supervised', 'training', 'unlabelled', 'unsupervised']\n",
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['lemma'][i+2] == 'by' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+5] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+6] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+6] = text_dataframe['tokens'][i+6]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+5] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '[' + text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i + 4] = text_dataframe['tokens'][i + 4] + ']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A' or text_dataframe['pattern'][i + 3] == 'V') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'voice' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i >= 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                    text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                    text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['accent', 'accuracy', 'activation', 'adaptation', 'algorithm', 'alignment', 'approach', \n",
    "          'architecture', 'attribute', 'boundary', 'cell', 'class', 'classifier', 'cluster', 'component', \n",
    "          'concatenation', 'content', 'contour', 'control', 'conversion', 'coverage', 'detection', \n",
    "          'detection', 'device', 'dictionary', 'embedding', 'encoding', 'engineering', 'entry', 'error', \n",
    "          'evaluation', 'experiment', 'expertise', 'file', 'filter', 'form', 'framework', 'function', \n",
    "          'generation', 'identification', 'implementation', 'improvement', 'inference', 'input', 'kernel', 'layer', 'learning', \n",
    "          'length', 'location', 'mapping', 'method', 'model', 'module', 'naturalness', 'network', \n",
    "          'nonlinearity', 'optimization', 'output', 'pair', 'parameter', 'pipeline', 'posterior', 'prediction', \n",
    "          'process', 'processing', 'quality', 'realization', 'recognition', 'representation', 'research', \n",
    "          'result', 'sample', 'score', 'sequence', 'set', 'setting', 'signal', 'string', 'study', 'symbol', \n",
    "          'synthesis', 'synthesizer', 'system', 'task', 'technique', 'technique', 'technology', 'token', 'tool', \n",
    "          'toolkit', 'training', 'transcription', 'transfer', 'transform', 'translation', 'value', 'generator',\n",
    "         'corpora', 'tilt', 'knowledge', 'category', 'track', 'tagger', 'unit', 'label']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i >= 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' ’', '’')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging_IOB(string):\n",
    "    \"\"\"Tagging the terms into IOB and POS\"\"\"\n",
    "    is_term = False \n",
    "    string_tag = string.split(' ')\n",
    "    annotated = []\n",
    "    for k,l in enumerate(string_tag):\n",
    "        if '[' in l and ']' in l:\n",
    "            for i,j in enumerate(l):\n",
    "                if l[i] == ']':\n",
    "                    annotated.append(l[:i]+ ' (B)]'+l[i+1:])\n",
    "        else:\n",
    "            if '[' in l and is_term is False:\n",
    "                annotated.append(l+ ' (B)')\n",
    "                is_term = True\n",
    "            elif is_term and ']' not in l:\n",
    "                annotated.append(l+ ' (I)')\n",
    "            elif is_term and ']' in l:\n",
    "                for m,n in enumerate(l):\n",
    "                    if l[m] == ']':\n",
    "                        annotated.append(l[:m]+ ' (I)]'+l[m+1:])\n",
    "                is_term = False\n",
    "            else:\n",
    "                if \"\\n\" not in l and l != 0 and l not in s.punctuation:\n",
    "                    annotated.append(l+ ' (O)')\n",
    "                else:\n",
    "                    annotated.append(l)\n",
    "    iob_string = ' '.join(annotated)\n",
    "#    print(iob_string)\n",
    "    return iob_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tags(string):\n",
    "    tagged = []\n",
    "    doc = spacy_nlp(string)\n",
    "    for el in doc:\n",
    "        if el.text not in ['B','O','I','\\n'] and el.text not in s.punctuation and len(el.text) != 0:\n",
    "            tagged.append(el.text+' '+el.pos_)\n",
    "        else:\n",
    "            tagged.append(el.text)\n",
    "    all_tags = ' '.join(tagged)\n",
    "    all_tags = all_tags.replace('( ', '(')\n",
    "    all_tags = all_tags.replace(' )', ')')\n",
    "    all_tags = all_tags.replace('[ ', '[')\n",
    "    all_tags = all_tags.replace(' ]', ']')\n",
    "    all_tags = all_tags.replace('SPACE','')\n",
    "#    print(all_tags)\n",
    "    return all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning to Speak Fluently in a [Foreign Language] : [Multilingual Speech Synthesis] and cross [- language voice] cloning \n",
      "\n",
      "\n",
      " Abstract \n",
      " We present a multispeaker, [multilingual text-to-speech (TTS) synthesis model] based on [Tacotron] that is able to produce [high quality speech] in [multiple languages]. \n",
      " Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish [speech] using an [English speaker]’s voice, without training on any bilingual or parallel examples. \n",
      " Such transfer works across distantly related languages, e.g. English and Mandarin. \n",
      " Critical to achieving this result are : using a [phonemic input representation] to encourage sharing of model capacity across languages, and incorporating an [adversarial loss] term to encourage the model to disentangle its representation of [speaker identity] (which is perfectly correlated with language in the [training data]) from the [speech content]. \n",
      " Further scaling up the model by training on [multiple speakers] of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize [intelligible speech] for [training speakers] in all languages seen during training, and in native or foreign accents. \n",
      " Index Terms : [speech synthesis], [end-to-end], [adversarial loss] \n",
      "\n",
      " Introduction \n",
      " Recent [end-to-end neural TTS models] have been extended to enable control of [speaker identity] as well as [unlabelled speech attributes], e.g. [prosody], by conditioning synthesis on latent representations in addition to text. \n",
      " Extending such models to support multiple, [unrelated languages] is nontrivial when using [language-dependent input representations] or model components, especially when the amount of training data [per] language is imbalanced. \n",
      " For example, there is no overlap in the text representation between languages like Mandarin and English. Furthermore, recordings from [bilingual speakers] are expensive to collect. It is therefore most common for each speaker in the training set to speak only one language, so [speaker identity] is perfectly correlated with language. \n",
      " This makes it difficult to transfer voices across [different languages], a [desirable feature] when the number of available [training voices] for a [particular language] is small. \n",
      " Moreover, for languages with borrowed or shared words, such as proper nouns in Spanish (ES) and English (EN), pronunciations of the same text might be different. \n",
      " This adds more ambiguity when a naively trained model sometimes generates [accented speech] for a [particular speaker]. \n",
      " Zen et al. proposed a speaker and [language factorization] for [HMM-based parametric TTS system], aiming to transfer a voice from one language to others. \n",
      " proposed a multilingual parametric [neural TTS system], which used a unified [input representation] and shared parameters across languages, however the   voices used for each language were disjoint. \n",
      " described a similar bilingual Chinese and English [neural TTS system] trained on [speech] from a [bilingual speaker], allowing it to [synthesize speech] in both languages using the [same voice]. \n",
      " studied learning pronunciation from a [bilingual TTS model]. \n",
      " Most recently, presented a [multilingual neural TTS model] which supports voice cloning across English, Spanish, and German. \n",
      "\n",
      "                                         \n",
      " Figure : Overview of the components of the proposed model. \n",
      " Dashed lines denote sampling via reparameterization   during training. \n",
      " The prior mean is always use during inference. \n",
      "                                   \n",
      " It used [language-specific text] and [speaker encoders], and incorporated a secondary fine-tuning step to optimize a [speaker identity]-preserving loss, ensuring that the model could output a [consistent voice] regardless of language. \n",
      " We also note that the sound quality is not on par with recent [neural TTS systems], potentially because of its use of the [WORLD vocoder] for [waveform synthesis]. \n",
      " Our work is most similar to, which describes a [multilingual TTS model] based on [Tacotron 2] which uses a Unicode encoding “ [byte] ” [input representation] to train a model on one speaker of each of English, Spanish, and Mandarin. \n",
      " In this paper, we evaluate different [input representations], scale up the number of [training speakers] for each language, and extend the model to support [cross-lingual voice cloning]. \n",
      " The model is trained in a single stage, with no [language-specific components], and obtains naturalness on par with baseline monolingual models. \n",
      " Our contributions include : \n",
      " Evaluating the effect of using different text [input representations] in a [multilingual TTS model]. \n",
      " Introducing a [per]-input [token speaker-adversarial loss] to enable [cross-lingual voice transfer] when only one [training speaker] is available for each language. \n",
      " Incorporating an [explicit language embedding] to the input, which enables moderate control of [speech accent], independent of [speaker identity], when the [training data] contains [multiple speakers per] language. \n",
      " We evaluate the contribution of each component, and demonstrate the proposed model’s ability to disentangle speakers from languages and consistently synthesize [high quality speech] for all speakers, despite the perfect correlation to the [original language] in the [training data]. \n",
      "                                         \n",
      " Model Structure \n",
      " We base our [multilingual TTS model] on [Tacotron 2], which uses an [attention-based sequence-to-sequence model] to generate a sequence of log-[mel spectrogram] frames based on an input text sequence. \n",
      " The architecture is illustrated in Figure. \n",
      " It augments the base [Tacotron 2 model] with [additional speaker] and, optionally, [language embedding] inputs (bottom right), an adversarially-trained [speaker classifier] (top right), and a [variational autoencoder]-style [residual encoder] (top left) which conditions the [decoder] on a latent embedding computed from the [target spectrogram] during training (top left). \n",
      " Finally, similar to [Tacotron 2], we separately train a [WaveRNN neural vocoder]. \n",
      "\n",
      " [Input representations] \n",
      " [End-to-end TTS models] have typically used character or [phoneme]   [input representations], or hybrids between them. \n",
      " Recently, proposed using inputs derived from the UTF-8 [byte encoding] in [multilingual settings]. \n",
      " We evaluate the effects of using these representations for [multilingual TTS]. \n",
      "\n",
      " Characters / [Graphemes] \n",
      " Embeddings corresponding to each character or [grapheme] are the default inputs for [end-to-end TTS models], requiring the model to implicitly learn how to pronounce input words (i.e. [grapheme-to-phoneme conversion]) as part of the synthesis task. \n",
      " Extending a [grapheme-based input vocabulary] to a [multilingual setting] is straightforward, by simply concatenating \n",
      " [grapheme sets] in the [training corpus] for each language. \n",
      " This can grow quickly for languages with large alphabets, e.g. our Mandarin vocabulary contains over 4.5k tokens. \n",
      " We simply concatenate all [graphemes] appearing in the [training corpus], leading to a total of 4,619 tokens. \n",
      " Equivalent [graphemes] are shared across languages. \n",
      " During inference all previously unseen characters are mapped to a special out-of-vocabulary (OOV) symbol. \n",
      "\n",
      " UTF-8 Encoded [Bytes] \n",
      " Following   we experiment with an [input representation] based on the UTF-8 text encoding, which uses 256 possible values as each input token where the mapping from [graphemes] to bytes is [language-dependent]. \n",
      " For languages with single-[byte] characters (e.g., English), this representation is equivalent to the [grapheme representation]. \n",
      " However, for languages with multi-[byte] characters (such as Mandarin) the [TTS model] must learn to attend to a consistent [sequence of bytes] to correctly generate the corresponding [speech]. \n",
      " On the other hand, using a UTF-8 [byte representation] may promote sharing of representations between languages due to the smaller number of input tokens. \n",
      "\n",
      " [Phonemes] \n",
      " Using [phoneme inputs] simplifies the [TTS task], as the model no longer needs to learn complicated pronunciation rules for languages such as English. \n",
      " Similar to our [grapheme-based model], equivalent [phonemes] are shared across languages. We concatenate all possible [phoneme symbols], for a total of 88 tokens. \n",
      " To support Mandarin, we include tone information by learning [phoneme-independent embeddings] for each of the 4 possible tones, and broadcast each tone embedding to all [phoneme embeddings] inside the corresponding syllable. \n",
      " For English and Spanish, tone embeddings are replaced by stress embeddings which include primary and secondary stresses. A special symbol is used when there is no tone or stress. \n",
      "\n",
      " [Residual encoder] \n",
      " Following, we augment the [TTS model] by incorporating a [variational autoencoder]-like [residual encoder] which encodes the latent factors in the [training audio], e.g. [prosody] or background noise, which is not well-explained by the conditioning inputs : the text representation, speaker, and [language embeddings]. \n",
      " We follow the structure from, except we use a standard single [Gaussian prior distribution] and reduce the latent dimension to 16. \n",
      " In our experiments, we observe that feeding in the prior mean (all zeros) during inference, significantly improves stability of [cross-lingual speaker transfer] and leads to improved naturalness as shown by [MOS evaluations] in Section. \n",
      "\n",
      " Adversarial training \n",
      " One of the challenges for [multilingual TTS] is [data sparsity], where some languages may only have [training data] for a [few speakers]. \n",
      " In the extreme case where there is only one speaker [per] language in the [training data], the [speaker identity] is essentially the same as the language i d. \n",
      " To encourage the model to learn disentangled representations of the text and [speaker identity], we proactively discourage the text encoding ts from also capturing [speaker information]. \n",
      " We employ domain adversarial training to encourage ti to encode text in a speaker-independent manner by introducing a [speaker classifier] based on the text encoding and a [gradient reversal layer]. \n",
      " Note that the [speaker classifier] is optimized with a different objective than the rest of the model : Lspeaker (ψs ; ti) = N \n",
      " Í log p(si | ti), where si is the [speaker label] i and ψs are the parameters for [speaker classifier]. \n",
      " To train the full model, we insert a [gradient reversal layer]   prior to this [speaker classifier], which scales the gradient by −λ. \n",
      " Following, we also explore inserting another adversarial layer on top of the [variational autoencoder] to encourage it to learn speaker-independent representations. \n",
      " However, we found that this layer has no effect after decreasing the latent space dimension. \n",
      "                  \n",
      " We impose this [adversarial loss] separately on each element of the encoded text sequence, in order to encourage the model to learn a speaker and [language-independent text embedding space]. \n",
      " In contrast to, which disentangled [speaker identity] from background noise, some input tokens are highly [language-dependent] which can lead to unstable adversarial classifier gradients. \n",
      " We address this by clipping gradients computed at the [reversal layer] to limit the impact of such outliers. \n",
      "\n",
      " Experiments \n",
      " We train models using a [proprietary dataset] composed of [high quality speech] in three languages : 385 hours of English (EN) from 84 [professional voice] actors with accents from the United States, Great Britain, Australia, and Singapore ;   97 hours of Spanish (ES) from 3 [female speakers] include Castilian and US Spanish ; 68 hours of Mandarin (CN) from 5 speakers. \n",
      "\n",
      " Model and training setup \n",
      " The [synthesizer network] uses the [Tacotron 2 architecture], with additional inputs consisting of learned speaker (64-dim) and [language embeddings] (3-dim), concatenated and passed to the [decoder] at each step. \n",
      " The generated [speech] is represented as a sequence of 128-dim log-[mel spectrogram] frames, computed from 50ms windows shifted by 12.5ms. \n",
      " The [variational residual encoder architecture] closely follows the attribute [encoder] in. \n",
      " It maps a variable length [mel spectrogram] to two [vectors] parameterizing the mean and log variance of the [Gaussian posterior]. \n",
      " The [speaker classifiers] are fully-connected networks with one 256 unit hidden layer followed by a [softmax] predicting the [speaker identity]. \n",
      " The synthesizer and [speaker classifier] are trained with weight 1.0 and 0.02 respectively. \n",
      " As described in the previous section we apply gradient clipping with factor 0.5 to the [gradient reversal layer]. \n",
      "\n",
      " Table : [Speaker similarity Mean Opinion Score] ([MOS]) comparing [ground truth audio] from speakers of [different languages]. \n",
      " Raters are [native speakers] of the [target language]. \n",
      "\n",
      " Table : Naturalness [MOS] of monolingual and [multilingual models synthesizing speech] of in [different languages]. \n",
      "\n",
      " Table : Naturalness and [speaker similarity MOS] of cross [- language voice] cloning of an EN [source speaker]. Models which use different [input representations] are compared, with and without the speaker-[adversarial loss]. fail : raters complained that too many utterances were spoken in the [wrong language]. \n",
      "                                                                           \n",
      "\n",
      " The entire model is trained jointly with a [batch size] of 256, using the [Adam optimizer] configured with an initial [learning rate] of 10−3, and an exponential decay that halves the [learning rate] every 12.5k steps, starting at 50k steps. \n",
      " [Waveforms] are synthesized using a [WaveRNN vocoder] which generates 16-bit signals sampled at 24 kHz conditioned on [spectrograms] predicted by the [TTS model]. \n",
      " We synthesize 100 samples [per model], and have each one rated by 6 raters. \n",
      "\n",
      " Evaluation \n",
      " To evaluate [synthesized speech], we rely on crowdsourced [Mean Opinion Score (MOS) evaluations] of [speech naturalness] via subjective listening tests. \n",
      " Ratings follow the Absolute Category Rating scale, with scores from 1 to 5 in 0.5 point increments. \n",
      " For cross [- language voice] cloning, we also evaluate whether the [synthesized speech] resembles the identity of the [reference speaker] by pairing each synthesized utterance with a reference utterance from the [same speaker] for subjective [MOS evaluation] of [speaker similarity], as in. \n",
      " Although rater instructions explicitly asked for the content to be ignored, note that this similarity evaluation is more challenging than the one in because the reference and target examples are spoken in [different languages], and raters are not bilingual. \n",
      " We found that low fidelity [audio] tended to result in high variance similarity [MOS] so we always use [WaveRNN outputs]. \n",
      " For each language, we chose one speaker to use for similarity tests. \n",
      " As shown in Table, the [EN speaker] is found to be dissimilar to the ES and [CN speakers] ([MOS] below 2.0), while the ES and [CN speakers] are slightly similar ([MOS] around 2.0). \n",
      " The [CN speaker] has more natural variability compared to EN and ES, leading to a lower self similarity. \n",
      " The scores are consistent when EN and CN raters evaluate the same EN and CN test set. \n",
      " The observation is consistent with : raters are able to discriminate between speakers across languages. \n",
      " However, when rating [synthetic speech], we observed that English speaking raters often considered “ heavy accented ” synthetic CN [speech] to sound more similar to the target EN speaker, compared to more [fluent speech] from the [same speaker]. \n",
      " This indicates that accent and [speaker identity] are not fully disentangled. \n",
      " We encourage readers to listen to samples on the companion webpage. \n",
      "\n",
      " Comparing [input representations] \n",
      " We first build and evaluate models comparing the performance of different text [input representations]. \n",
      " For all three languages, [byte]-based models always use a 256-dim [softmax output]. \n",
      " Monolingual character and [phoneme models] each use a different input vocabulary corresponding to the [training language]. \n",
      "\n",
      " Some raters gave low fidelity [audio] lower scores, treating \" blurriness \" as a property of the speaker. Others gave higher scores because they recognized such [audio] as synthetic and had lower expectations. \n",
      " ttp://google.github.io/tacotron/publications/multilingual \n",
      "\n",
      " Table compares monolingual and [multilingual model] performance using different [input representations]. \n",
      " For Mandarin, the [phoneme-based model] performs significantly better than char or [byte]-based variants due to rare and OOV words. \n",
      " Compared to the monolingual system, [multilingual phoneme-based systems] have similar performance on ES and CN but are slightly worse on EN. \n",
      " CN has a larger gap to ground truth (top) due to unseen word segmentation (for simplicity, we did n’t add word boundary during training). \n",
      " The multispeaker model (bottom) performs about the same as the [single speaker per-language variant] (middle). \n",
      " Overall, when using [phoneme inputs] all the languages obtain [MOS scores] above 4.0. \n",
      "\n",
      " Cross [- language voice] cloning \n",
      " We evaluate how well the multispeaker models can be used to clone a speaker’s voice into a [new language] by simply passing in [speaker embeddings] corresponding to a [different language] from the input text. \n",
      " Table shows voice cloning performance from an EN speaker in the [most data]-poor scenario (129 hours), where only a [single speaker] is available for each [training language] (1EN 1ES 1CN) without using the speaker-[adversarial loss]. \n",
      " Using [byte inputs] 3 it was possible to clone the [EN speaker] to ES with high similarity [MOS], albeit with significantly reduced naturalness. \n",
      " However, cloning the [EN voice] to CN failed4, as did cloning to ES and CN using [phoneme inputs]. \n",
      "   \n",
      " Using character or [byte inputs] led to similar results. \n",
      " We did n’t run listening tests because it was clear that synthesizing EN text using the [CN speaker embedding] did n’t affect the model output. \n",
      "\n",
      "\n",
      " Table : Naturalness and [speaker similarity MOS] of cross [- language voice] cloning of the full [multilingual model] using [phoneme inputs]. \n",
      "\n",
      "\n",
      " Table : Effect of [EN speaker] cloning with no [residual encoder]. \n",
      "\n",
      "\n",
      " Figure : Visualizing the effect of [voice cloning] and accent control, using 2D PCA of [speaker embeddings] computed from [speech] synthesized with [different speaker], text, and language i d combinations. \n",
      " Embeddings cluster together (bottom left and right), implying high similarity, when the speaker’s [original language] matches the [language embedding], regardless of the [text language]. However, using language i d from the text (squares), modifying the speaker’s accent to speak fluently, hurts similarity compared to the [native language] and accent (circles). \n",
      "     \n",
      " Adding the [adversarial speaker classifier] enabled crosslanguage cloning of the EN speaker to CN with very high similarity [MOS] for both [byte] and [phoneme models]. \n",
      " However, naturalness [MOS] remains much lower than using the [native speaker] identity, with the naturalness listening test failing entirely in the CN case with [byte inputs] as a result of rater comments that the [speech] sounded like a [foreign language]. \n",
      " According to rater comments on the [phoneme system], most of the degradation came from mismatched accent and pronunciation, not fidelity. \n",
      " CN raters commented that it sounded like “ a foreigner speaking Chinese ”. \n",
      " More interestingly, few ES raters commented that “ The voice does not sound robotic but instead sounds like an English [native speaker] who is learning to pronounce the words in Spanish. ” \n",
      " Based on these results, we only use [phoneme inputs] in the following experiments since this guarantees that pronunciations are correct and results in more [fluent speech]. \n",
      " Table evaluates voice cloning performance of the full [multilingual model] (84EN 3ES 5CN), which is trained on the [full dataset] with increased [speaker coverage], and uses the speaker-[adversarial loss] and speaker / [language embeddings]. \n",
      " Incorporating the [adversarial loss] forces the text representation to be [less language-specific], instead relying on the [language embedding] to capture [language-dependent information]. \n",
      " Across all [language pairs], the model [synthesizes speech] in all voices with naturalness [MOS] above 3.85, demonstrating that increasing [training speaker] diversity improves generalization. \n",
      " In most cases synthesizing EN and ES [speech] (except EN-to-ES) approaches the ground truth scores. In contrast, naturalness of CN [speech] is consistently lower than the ground truth. \n",
      "\n",
      " The high naturalness and similarity [MOS scores] in the top row of Table indicate that the model is able to successfully transfer the EN voice to both ES and CN almost without accent.             \n",
      " When consistently conditioning on the [EN language embedding] regardless of the [target language] (second row), the model produces more English accented ES and CN [speech], which leads to lower naturalness but higher similarity [MOS scores]. \n",
      " Also see Figure and the demo for accent transfer [audio] examples.                                                                                            \n",
      " We see that cloning the [CN voice] to [other languages] (bottom row) has the lowest similarity [MOS], although the scores are still much higher than different-[speaker similarity MOS] in the off-diagonals of Table indicating that there is some degree of transfer. \n",
      " This is a consequence of the [low speaker] coverage of CN compared to EN in the [training data], as well as the large distance between CN and [other languages].                                                                        \n",
      " Finally, Table demonstrates the importance of training using a [variational residual encoder] to stabilize the model output. \n",
      " Naturalness [MOS] decreases by 0.4 points for EN-to-CN cloning without the [residual encoder] (bottom row). \n",
      " In informal comparisons of the outputs of the two models we find that the model without the [residual encoder] tends to skip rare words or inserts unnatural pauses in the output [speech]. \n",
      " This indicates the VAE prior learns a mode which helps stabilize attention. \n",
      "\n",
      " Conclusions \n",
      " We describe extensions to the [Tacotron 2 neural TTS model] which allow training of a [multilingual model] trained only on [monolingual speakers], which is able to synthesize [high quality speech] in three languages, and transfer [training voices] across languages. \n",
      " Furthermore, the model learns to speak [foreign languages] with moderate control of accent, and, as demonstrated on the companion webpage, has rudimentary support for code switching. \n",
      " In future work we plan to investigate methods for scaling up to leverage large amounts of low quality [training data], and support many [more speakers] and languages. \n",
      "\n",
      " Acknowledgements \n",
      " We thank Ami Patel, Amanda Ritchart-Scott, Ryan Li, Siamak Tazari, Yutian Chen, Paul McCartney, Eric Battenberg, Toby Hawker, and Rob Clark for discussions and helpful feedback. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('lexicon.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/3.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    text_dataframe.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    annotate(data, text_dataframe)\n",
    "    annotation = construct_annotated_text(text_dataframe)\n",
    "    print(annotation)\n",
    "    iob_text = tagging_IOB(annotation)\n",
    "    pos_text = POS_tags(iob_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
