{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: Cécile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "#     print(len(original))\n",
    "#     print(len(lemma))\n",
    "#     print(len(t))\n",
    "#     print(len(pos))\n",
    "#     print(len(new_pos))\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['lemma'][i+2] == 'by' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+5] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+6] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+6] = text_dataframe['tokens'][i+6]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+5] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '[' + text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i + 4] = text_dataframe['tokens'][i + 4] + ']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A' or text_dataframe['pattern'][i + 3] == 'V') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'voice' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['system', 'model', 'synthesis', 'translation', 'recognition', 'signal', 'research', \n",
    "          'processing', 'conversion', 'technique', 'accuracy', 'synthesizer', 'architecture', \n",
    "          'form', 'transcription', 'alignment', 'optimization', 'task', 'function','token',\n",
    "         'activation', 'layer', 'experiment', 'output', 'representation', 'setting', 'control', \n",
    "         'network', 'quality', 'file', 'learning', 'framework', 'transform', 'sequence', 'length',\n",
    "         'tool', 'boundary', 'detection', 'kernel', 'engineering', 'detection', 'device', 'parameter',\n",
    "         'implementation', 'component', 'dictionary', 'pair', 'encoding', 'cell', 'error', 'result',\n",
    "         'pipeline', 'score', 'prediction', 'inference', 'approach', 'expertise', 'adaptation', 'technique',\n",
    "         'process', 'method','training']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' ’', '’')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer Learning from [Speaker Verification] to Multispeaker [Text-To-Speech Synthesis] \n",
      "\n",
      "\n",
      " Abstract \n",
      "\n",
      " We describe a [neural network-based system] for [text-to-speech (TTS) synthesis] that is able to generate [speech audio] in the [voice] of [different speakers], including those unseen during training. \n",
      " Our system consists of three independently trained components : a [speaker encoder network], trained on a [speaker verification] task using an [independent dataset] of noisy [speech] without transcripts from thousands of speakers, to generate a fixed-dimensional embedding [vector] from only seconds of reference [speech] from a [target speaker] ; a [sequence-to-sequence synthesis network] based on [Tacotron] 2 that generates a [mel spectrogram] from text, conditioned on the [speaker embedding] ; an auto-regressive [WaveNet-based vocoder network] that converts the [mel spectrogram] into time domain [waveform] samples. \n",
      " We demonstrate that the proposed model is able to transfer the knowledge of [speaker variability] learned by the discriminatively-trained [speaker encoder] to the multispeaker [TTS task], and is able to synthesize natural [speech] from speakers unseen during training. \n",
      " We quantify the importance of training the [speaker encoder] on a large and [diverse speaker] set in order to obtain the best generalization performance. \n",
      " Finally, we show that randomly sampled [speaker embeddings] can be used to [synthesize speech] in the [voice] of [novel speakers] dissimilar from those used in training, indicating that the model has learned a high [quality speaker] representation. \n",
      "\n",
      " Introduction \n",
      "\n",
      " The goal of this work is to build a [TTS system] which can generate natural [speech] for a variety of speakers in a data efficient manner. \n",
      " We specifically address a zero-shot learning setting, where a few seconds of untranscribed reference [audio] from a [target speaker] is used to synthesize new [speech] in that [speaker’s voice], without updating any model parameters. \n",
      " Such systems have accessibility applications, such as restoring the ability to communicate naturally to users who have lost their [voice] and are therefore unable to provide many new training examples. \n",
      " They could also enable new applications, such as transferring a [voice] across languages for more natural [speech-to-speech translation], or generating realistic [speech] from text in low resource settings. \n",
      " However, it is also important to note the potential for misuse of this technology, for example impersonating someone’s [voice] without their consent. \n",
      " In order to address safety concerns consistent with principles such as, we verify that [voices] generated by the proposed model can easily be distinguished from [real voices]. \n",
      " Synthesizing natural [speech] requires training on a large number of high quality [speech]-transcript pairs, and supporting [many speakers] usually uses tens of minutes of [training data per] speaker. \n",
      " Recording a large amount of high [quality data] for [many speakers] is impractical. \n",
      " Our approach is to [decouple speaker] modeling from [speech synthesis] by independently training a speaker-discriminative embedding network that captures the space of [speaker characteristics] and training a high quality [TTS model] on a [smaller dataset] conditioned on the representation learned by the first network. \n",
      " Decoupling the networks enables them to be trained on [independent data], which reduces the need to obtain high quality multispeaker [training data]. \n",
      " We train the speaker embedding network on a [speaker verification] task to determine if two different utterances were spoken by the [same speaker]. \n",
      " In contrast to the subsequent [TTS model], this network is trained on untranscribed [speech] containing reverberation and background noise from a large number of speakers. \n",
      " We demonstrate that the [speaker encoder] and synthesis networks can be trained on unbalanced and disjoint sets of speakers and still generalize well. \n",
      " We train the synthesis network on. \n",
      " 2000 speakers and show that training the [encoder] on a much larger set of 18000 speakers improves adaptation quality, and further enables synthesis of completely [novel speakers] by sampling from the embedding prior. \n",
      " There has been significant interest in [end-to-end training] of [TTS models], which are trained directly from text-[audio pairs], without depending on hand crafted intermediate representations. \n",
      " [Tacotron] 2 used [WaveNet] as a [vocoder] to invert [spectrograms] generated by an [encoder-decoder architecture] with attention, obtaining naturalness approaching that of human [speech] by combining Tacotron’sprosody with [WaveNet]’s [audio quality]. \n",
      " It only supported a [single speaker]. \n",
      " Gibiansky et al. ntroduced a multispeaker variation of [Tacotron] which learned low-[dimensional speaker] embedding for each [training speaker]. \n",
      " [Deep Voice] 3proposed a fully [convolutional encoder-decoder architecture] which scaled up to support over 2,400 speakers from [LibriSpeech]. \n",
      " These systems learn a fixed set of [speaker embeddings] and therefore only support synthesis of [voices] seen during training. \n",
      " In contrast, VoiceLoop proposed a novel architecture based on a fixed size memory buffer which can generate [speech] from [voices] unseen during training. \n",
      " Obtaining good results required tens of minutes of enrollment [speech] and transcripts for a [new speaker]. \n",
      " Recent extensions have enabled few-[shot speaker] adaptation where only a few seconds of [speech per] speaker (without transcripts) can be used to generate new [speech] in that speaker’s [voice]. \n",
      " extends [Deep Voice] 3, comparing a [speaker adaptation] method similar to where the model parameters (including [speaker embedding]) are fine-tuned on a small amount of [adaptation data] to a [speaker encoding] method which uses a [neural network] to predict speaker embedding directly from a [spectrogram]. \n",
      " The latter approach is significantly [more data] efficient, obtaining higher naturalness using small amounts of [adaptation data], in as few as one or two utterances. \n",
      " It is also significantly more computationally efficient since it does not require hundreds of backpropagation iterations. \n",
      " Nachmani et al.   similarly extended VoiceLoop to utilize a [target speaker] encoding network to predict a [speaker embedding]. \n",
      " This network is trained jointly with the synthesis network using a contrastive triplet loss to ensure that embeddings predicted from utterances by the [same speaker] are closer than embeddings computed from [different speakers]. \n",
      " In addition, a cycle-consistency loss is used to ensure that the [synthesized speech] encodes to a similar embedding as the adaptation utterance. \n",
      " A similar [spectrogram encoder network], trained without a triplet loss, was shown to work for transferring target [prosody] to [synthesized speech]. \n",
      " In this paper we demonstrate that training a similar [encoder] to discriminate between speakers leads to reliable transfer of [speaker characteristics]. \n",
      " Our work is most similar to the [speaker encoding] models in, except that we utilize a network independently-trained for a [speaker verification] task on a [large dataset] of untranscribed [audio] from tens of thousands of speakers, using a state-of-the-art generalized [end-to-end loss]. \n",
      " incorporated a [similar speaker]-discriminative representation into their model, however all components were trained jointly. \n",
      " In contrast, we explore transfer learning from a pre-trained [speaker verification] model. \n",
      " Doddipatla et al. used a similar transfer learning configuration where a [speaker embedding] computed from a pre-trained [speaker classifier] was used to condition a [TTS system]. \n",
      " In this paper we utilize an [end-to-end synthesis network] which does not rely on intermediate [linguistic features], and a substantially [different speaker] embedding network which is not limited to a closed set of speakers. \n",
      " Furthermore, we analyze how quality varies with the number of speakers in the training set, and find that zero-shot transfer requires training on thousands of speakers, many more than were used in. \n",
      "\n",
      " Multispeaker [speech synthesis model] \n",
      "\n",
      " Our system is composed of three independently trained [neural networks], illustrated in Figure 1 : (1) a [recurrent speaker encoder], based on, which computes a fixed dimensional [vector] from a [speech signal], (2) a [sequence-to-sequence synthesizer], based on, which predicts a [mel spectrogram] from a [sequence of grapheme] or [phoneme] inputs, conditioned on the speaker embedding [vector], and (3) an autoregressive [WaveNet vocoder], which converts the [spectrogram] into time domain [waveforms]. \n",
      "                                                    \n",
      " Figure : Model overview. Each of the three components are trained independently. \n",
      "\n",
      " [Speaker encoder] \n",
      "\n",
      " The [speaker encoder] is used to condition the synthesis network on a reference [speech signal] from the desired [target speaker]. \n",
      " Critical to good generalization is the use of a representation which captures the characteristics of [different speakers], and the ability to identify these characteristics using only a short adaptation signal, independent of its [phonetic] content and background noise. \n",
      " These requirements are satisfied using a speaker-discriminative model trained on a text-[independent speaker] verification task. \n",
      " We follow, which proposed a highly scalable and accurate [neural network framework] for [speaker verification]. \n",
      " The network maps a sequence of log-[mel spectrogram] frames computed from a [speech] utterance of arbitrary length, to a fixed-dimensional embedding [vector], known as d-[vector]. \n",
      " The network is trained to optimize a generalized [end-to-end speaker verification] loss, so that embeddings of utterances from the [same speaker] have high cosine similarity, while those of utterances from [different speakers] are far apart in the embedding space. \n",
      " The [training dataset] consists of [speech audio] examples segmented into 1.6 seconds and [associated speaker] identity [labels] ; no transcripts are used. \n",
      " Input 40-channel log-[mel spectrograms] are passed to a network consisting of a stack of 3 [LSTM layers] of 768 cells, each followed by a projection to 256 dimensions. \n",
      " The final embedding is created by L2 -normalizing the output of the top layer at the final frame. \n",
      " During inference, an arbitrary length utterance is broken into 800ms windows, overlapped by 50 %. \n",
      " The network is run independently on each window, and the outputs are averaged and normalized to create the final utterance embedding. \n",
      " Although the network is not optimized directly to learn a representation which captures [speaker characteristics] relevant to synthesis, we find that training on a [speaker discrimination] task leads to an embedding which is directly suitable for conditioning the synthesis network on [speaker identity]. \n",
      "\n",
      " Synthesizer \n",
      "\n",
      " We extend the recurrent [sequence-to-sequence] with attention [Tacotron] 2 architecture to support [multiple speakers] following a scheme similar to [ 8 ]. \n",
      " An embedding [vector] for the [target speaker] is concatenated with the synthesizer [encoder output] at each time step. \n",
      " In contrast to, we find that simply passing embeddings to the [attention layer], as in Figure 1, converges across [different speakers]. \n",
      " We compare two variants of this model, one which computes the embedding using the [speaker encoder], and a baseline which optimizes a fixed embedding for each speaker in the training set, essentially learning a lookup table of [speaker embeddings] similar to. \n",
      " The synthesizer is trained on pairs of text transcript and target [audio]. \n",
      " At the input, we map the text to a [sequence of phonemes], which leads to faster convergence and improved pronunciation of rare words and proper nouns. \n",
      " The network is trained in a transfer learning configuration, using a pretrained [speaker encoder] (whose parameters are frozen) to extract a speaker embedding from the target [audio], i.e. the [speaker reference] signal is the same as the target [speech] during training. \n",
      " No [explicit speaker] identifier [labels] are used during training. \n",
      " [Target spectrogram features] are computed from 50ms windows computed with a 12.5ms step, passed through an 80-channel [mel-scale filterbank] followed by log dynamic range compression. \n",
      " We extend by augmenting the L2 loss on the predicted [spectrogram] with an additional L1 loss. \n",
      " In practice, we found this combined loss to be more robust on noisy [training data]. \n",
      " In contrast to, we do n’t introduce additional loss terms based on the [speaker embedding]. \n",
      "\n",
      " See https://google.github.io/tacotron/publications/speaker_adaptation for samples. \n",
      "\n",
      " Figure : Example synthesis of a sentence in [different voices] using the proposed system. [Mel spectrograms] are visualized for reference utterances used to generate [speaker embeddings] (left), and the corresponding synthesizer outputs (right). \n",
      " The [text-to-spectrogram alignment] is shown in red. \n",
      " Three speakers held out of the train sets are used : one male (top) and two female (center and bottom). \n",
      " \n",
      " [Neural vocoder] \n",
      " \n",
      " We use the sample-by-sample autoregressive [WaveNet] as a [vocoder] to invert synthesized [mel spectrograms] emitted by the synthesis network into time-domain [waveforms]. \n",
      " The architecture is the same as that described in, composed of 30 dilated convolution layers. \n",
      " The network is not directly conditioned on the output of the [speaker encoder]. \n",
      " The [mel spectrogram] predicted by the [synthesizer network] captures all of the relevant detail needed for high quality synthesis of a variety of [voices], allowing a multispeaker [vocoder] to be constructed by simply training on data from [many speakers]. \n",
      " \n",
      " Inference and zero-[shot speaker] adaptation \n",
      "\n",
      " During inference the model is conditioned using arbitrary untranscribed [speech audio], which does not need to match the text to be synthesized. \n",
      " Since the speaker characteristics to use for synthesis are inferred from [audio], it can be conditioned on [audio] from speakers that are outside the training set. \n",
      " In practice we find that using a single [audio clip] of a few seconds duration is sufficient to synthesize new [speech] with the corresponding [speaker characteristics], representing zero-shot adaptation to [novel speakers]. \n",
      " In Section 3 we evaluate how well this process generalizes to previously [unseen speakers]. \n",
      " An example of the inference process is visualized in Figure 2, which shows [spectrograms] synthesized using several different 5 [second speaker] reference utterances. \n",
      " Compared to those of the female (center and bottom) speakers, the synthesized male (top) [speaker spectrogram] has noticeably lower [fundamental frequency], visible in the denser harmonic spacing (horizontal stripes) in low frequencies, as well as formants, visible in the mid-frequency peaks present during vowel sounds such as the ‘ i’ at 0.3 seconds – the top male F2 is in [mel] channel 35, whereas the F2 of the [middle speaker] appears closer to channel 40. \n",
      " Similar differences are also visible in sibilant sounds, e.g. the ‘s’ at 0.4 seconds contains more energy in lower frequencies in the [male voice] than in the [female voices]. \n",
      " Finally, the characteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen by the longer signal duration in the bottom row compared to the top two. \n",
      " Similar observations can be made about the corresponding reference utterance [spectrograms] in the right column. \n",
      "\n",
      " Table : [Speech] naturalness [Mean Opinion Score] ([MOS]) with 95 % confidence intervals. \n",
      "\n",
      "\n",
      " Experiments \n",
      "\n",
      " We used two [public datasets] for training the [speech synthesis] and [vocoder networks]. \n",
      " VCTK contains 44 hours of clean [speech] from 109 speakers, the majority of which have British accents. \n",
      " We downsampled the [audio] to 24 kHz, trimmed leading and trailing silence (reducing the median duration from 3.3 seconds to 1.8 seconds), and split into three subsets : train, validation (containing the [same speakers] as the train set) and test (containing 11 speakers held out from the train and validation sets). \n",
      " [LibriSpeech] consists of the union of the two “ clean ” training sets, comprising 436 hours of [speech] from 1,172 speakers, sampled at 16 kHz. \n",
      " The majority of [speech] is US English, however since it is sourced from [audio] books, the tone and style of [speech] can differ significantly between utterances from the [same speaker]. \n",
      " We resegmented the data into shorter utterances by force aligning the [audio] to the transcript using an [ASR model] and breaking segments on silence, reducing the median duration from 14 to 5 seconds. \n",
      " As in the [original dataset], there is no punctuation in transcripts. \n",
      " The [speaker sets] are completely disjoint among the train, validation, and test sets. \n",
      " Many recordings in the [LibriSpeech clean corpus] contain noticeable environmental and stationary background noise. \n",
      " We preprocessed the [target spectrogram] using a simple [spectral] subtraction denoising procedure, where the background noise [spectrum] of an utterance was estimated as the 10th percentile of the energy in each frequency band across the full signal. \n",
      " This process was only used on the synthesis target ; the original noisy [speech] was passed to the [speaker encoder]. \n",
      " We trained separate synthesis and [vocoder networks] for each of these two corpora. \n",
      " Throughout this section, we used synthesis networks trained on [phoneme] inputs, in order to control for pronunciation in subjective evaluations. \n",
      " For the [VCTK dataset], whose [audio] is quite clean, we found that the [vocoder] trained on ground truth [mel spectrograms] worked well. \n",
      " However for [LibriSpeech], which is noisier, we found it necessary to train the [vocoder] on [spectrograms] predicted by the [synthesizer network]. \n",
      " No denoising was performed on the target [waveform] for [vocoder training]. \n",
      " The [speaker encoder] was trained on a [proprietary voice search corpus] containing 36 M utterances with median duration of 3.9 seconds from 18000 [English speakers] in the United States. \n",
      " This dataset is not transcribed, but contains anonymized [speaker identities]. \n",
      " It is never used to train synthesis networks. \n",
      " We primarily rely on crowdsourced [Mean Opinion Score] ([MOS]) evaluations based on subjective listening tests. \n",
      " All our [MOS] evaluations are aligned to the Absolute Category Rating scale, with rating scores from 1 to 5 in 0.5 point increments. \n",
      " We use this framework to evaluate [synthesized speech] along two dimensions : its naturalness and similarity to real [speech] from the [target speaker]. \n",
      "\n",
      " [Speech] naturalness \n",
      "\n",
      " We compared the naturalness of [synthesized speech] using synthesizers and [vocoders] trained on VCTK and [LibriSpeech]. \n",
      " We constructed an evaluation set of 100 phrases which do not appear in any training sets, and evaluated two sets of speakers for each model : one composed of speakers included in the train set (Seen), and another composed of those that were held out (Unseen). \n",
      " We used 11 seen and [unseen speakers] for VCTK and 10 seen and [unseen speakers] for [LibriSpeech] (Appendix D). \n",
      " For each speaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the [speaker embedding] (see Appendix C). \n",
      " Each phrase was synthesized for each speaker, for a total of about 1,000 synthesized utterances [per] evaluation. \n",
      " Each sample was rated by a single rater, and each evaluation was conducted independently : the outputs of different models were not compared directly. \n",
      " Results are shown in Table 1, comparing the proposed model to baseline multispeaker models that utilize a lookup table of [speaker embeddings] similar to, but otherwise have identical architectures to the proposed [synthesizer network]. \n",
      " The proposed model achieved about 4.0 [MOS] in all datasets, with the VCTK model obtaining a [MOS] about 0.2 points higher than the [LibriSpeech model] when evaluated on seen speakers. \n",
      " This is the consequence of two drawbacks of the [LibriSpeech dataset] : (i) the lack of punctuation in transcripts, which makes it difficult for the model to learn to pause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the synthesizer has learned to reproduce, despite denoising the training targets as described above. \n",
      " Most importantly, the [audio] generated by our model for [unseen speakers] is deemed to be at least as natural as that generated for seen speakers. \n",
      " Surprisingly, the [MOS] on [unseen speakers] is higher than that of seen speakers, by as much as 0.2 points on [LibriSpeech]. \n",
      " This is a consequence of the randomly selected reference utterance for each speaker, which sometimes contains uneven and non-neutral [prosody]. \n",
      " In informal listening tests we found that the [prosody] of the [synthesized speech] sometimes mimics that of the reference, similar to. \n",
      " This effect is larger on [LibriSpeech], which contains more varied [prosody]. \n",
      " This suggests that additional care must be taken to [disentangle speaker] identity from [prosody] within the synthesis network, perhaps by integrating a [prosody encoder] as in, or by training on randomly paired reference and target utterances from the [same speaker]. \n",
      "\n",
      " Table : [Speaker similarity Mean Opinion Score] ([MOS]) with 95 % confidence intervals. \n",
      "\n",
      "\n",
      "\n",
      " [Speaker similarity] \n",
      "\n",
      " To evaluate how well the [synthesized speech] matches that from the [target speaker], we paired each synthesized utterance with a randomly selected ground truth utterance from the [same speaker]. \n",
      " Each pair is rated by one rater with the following instructions : “ You should not judge the content, grammar, or [audio quality] of the sentences ; instead, just focus on the similarity of the speakers to one another. ” \n",
      " Results are shown in Table. \n",
      " The scores for the VCTK model tend to be higher than those for [LibriSpeech], reflecting the cleaner nature of the dataset. \n",
      " This is also evident in the higher ground truth baselines on VCTK. \n",
      " For [seen speakers] on VCTK, the proposed model performs about as well as the baseline which uses an embedding lookup table for [speaker conditioning]. \n",
      " However, on [LibriSpeech], the proposed model obtained a lower similarity [MOS] than the baseline, which is likely due to the wider degree of within-[speaker variation] (Appendix B), and background noise level in the dataset. \n",
      " On [unseen speakers], the proposed model obtains lower similarity between ground truth and [synthesized speech]. \n",
      " On VCTK, the similarity score of 3.28 is between “ moderately similar ” and “ very similar ” on the evaluation scale. \n",
      " Informally, it is clear that the proposed model is able to transfer the broad strokes of the [speaker characteristics] for [unseen speakers], clearly reflecting the correct gender, [pitch], and formant ranges (as also visualized in Figure 2). \n",
      " But the significantly reduced similarity scores on [unseen speakers] suggests that some nuances, e.g. related to characteristic [prosody], are lost. \n",
      " The [speaker encoder] is trained only on North American accented [speech]. \n",
      " As a result, accent mismatch constrains our performance on [speaker similarity] on VCTK since the rater instructions did not specify how to judge accents, so raters may consider a pair to be from [different speakers] if the accents do not match. \n",
      " Indeed, examination of rater comments shows that our model sometimes produced a different accent than the ground truth, which led to lower scores. \n",
      " However, a few raters commented that the tone and inflection of the [voices] sounded very similar despite differences in accent. \n",
      " As an initial evaluation of the ability to generalize to out of [domain speakers], we used synthesizers trained on VCTK and [LibriSpeech] to synthesize speakers from the [other dataset]. \n",
      " We only varied the train set of the synthesizer and [vocoder networks] ; both models used an [identical speaker encoder]. \n",
      " As shown in Table 3, the models were able to generate [speech] with the same degree of naturalness as on unseen, but in-domain, speakers shown in Table 1. \n",
      " However, the [LibriSpeech model] synthesized [VCTK speakers] with significantly [higher speaker] similarity than the VCTK model is able to synthesize [LibriSpeech speakers]. \n",
      " The better generalization of the [LibriSpeech model] suggests that training the synthesizer on only 100 speakers is insufficient to enable high [quality speaker] transfer. \n",
      "\n",
      " Table : Cross [- dataset] evaluation on naturalness and [speaker similarity] for [unseen speakers]. \n",
      "\n",
      "\n",
      "\n",
      " [Speaker verification] \n",
      "\n",
      " As an objective metric of the degree of [speaker similarity] between synthesized and ground truth [audio] for [unseen speakers], we evaluated the ability of a [limited speaker] verification system to distinguish synthetic from real [speech]. \n",
      " We trained a new eval-only [speaker encoder] with the same network topology as Section, but using a different training set of 28 M utterances from 113000 speakers. \n",
      " Using a different model for evaluation ensured that metrics were not only valid on a [specific speaker] embedding space. \n",
      " We enroll the [voices] of 21 [real speakers] : 11 speakers from VCTK, and 10 from [LibriSpeech], and score synthesized [waveforms] against the set of enrolled speakers. \n",
      " All enrollment and [verification speakers] are unseen during synthesizer training. \n",
      " [Speaker verification] equal error rates (SV-EERs) are estimated by pairing each test utterance with each [enrollment speaker]. \n",
      " We synthesized 100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation. \n",
      " As shown in Table, as long as the synthesizer was trained on a sufficiently large set of speakers, i.e. on [LibriSpeech], the [synthesized speech] is typically most similar to the ground [truth voices]. \n",
      " The [LibriSpeech synthesizer] obtains similar EERs of 5-6 % using [reference speakers] from both datasets, whereas the one trained on VCTK performs much worse, especially on out-of-domain [LibriSpeech speakers]. \n",
      " These results are consistent with the subjective evaluation in Table. \n",
      " To measure the difficulty of discriminating between real and [synthetic speech] for the [same speaker], we performed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic versions of the 10 real [LibriSpeech speakers]. \n",
      " On this 20 [voice discrimination] task we obtain an EER of 2.86 %, demonstrating that, while the [synthetic speech] tends to be close to the [target speaker] (cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances for the [same speaker] (similarity > 0.7). \n",
      " From this we can conclude that the proposed model can generate [speech] that resembles the [target speaker], but not well enough to be confusable with a [real speaker]. \n",
      "\n",
      " Speaker embedding space \n",
      "\n",
      " Visualizing the speaker embedding space further contextualizes the quantitive results described in Section and. \n",
      " As shown in Figure 3, [different speakers] are well separated from each other in the speaker embedding space. \n",
      " The PCA visualization (left) shows that synthesized utterances tend to lie very close to real [speech] from the [same speaker] in the embedding space. \n",
      " However, synthetic utterances are still easily distinguishable from the real human [speech] as demonstrated by the t-SNE visualization (right) where utterances from each [synthetic speaker] form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker. \n",
      " Speakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with all [female speakers] appearing on the left, and all [male speakers] appearing on the right. \n",
      " This is an indication that the [speaker encoder] has learned a reasonable representation of [speaker space]. \n",
      "\n",
      " Figure : Visualization of [speaker embeddings] extracted from [LibriSpeech] utterances. \n",
      " Each color corresponds to a [different speaker]. \n",
      " Real and synthetic utterances appear nearby when they are from the [same speaker], however real and synthetic utterances consistently form distinct clusters. \n",
      "\n",
      " Table : Performance using [speaker encoders] (SEs) trained on [different datasets]. \n",
      " Synthesizers are all trained on [LibriSpeech] Clean and evaluated on held out speakers. \n",
      "\n",
      " Number of [speaker encoder training speakers] \n",
      "\n",
      " It is likely that the ability of the proposed model to generalize well across a wide variety of speakers is based on the quality of the representation learned by the [speaker encoder]. \n",
      " We therefore explored the effect of the [speaker encoder training] set on synthesis quality. \n",
      " We made use of three additional training sets : [LibriSpeech] Other, which contains 461 hours of [speech] from a set of 1,166 [speakers disjoint] from those in the clean subsets, [VoxCeleb], and VoxCeleb2 which contain 139000 utterances from 1,211 speakers, and 1.09 M utterances from 5,994 speakers, respectively. \n",
      " Table   compares the performance of the proposed model as a function of the number of speakers used to train the [speaker encoder]. \n",
      " This measures the importance of [speaker diversity] when training the [speaker encoder]. \n",
      " To avoid overfitting, the [speaker encoders] trained on [small datasets] (top two rows) use a smaller [network architecture] (256-dim [LSTM cells] with 64-dim projections) and output 64 [dimensional speaker] embeddings. \n",
      " We first evaluate the [speaker encoder] trained on [LibriSpeech] Clean and Other sets, each of which contain a similar number of speakers. \n",
      " In Clean, the [speaker encoder] and synthesizer are trained on the [same data], a baseline similar to the non-fine tuned [speaker encoder] from, except that it is trained discriminatively as in. \n",
      " This matched condition gives a slightly better naturalness and a similar similarity score. \n",
      " As the number of [training speakers] increases, both naturalness and similarity improve significantly. \n",
      " The objective EER results also improve alongside the subjective evaluations. \n",
      " These results have an important implication for multispeaker [TTS training]. \n",
      " The [data requirement] for the [speaker encoder] is much cheaper than full [TTS training] since no transcripts are necessary, and the [audio quality] can be lower than for [TTS training]. \n",
      " We have shown that it is possible to synthesize very natural [TTS] by combining a [speaker encoder network] trained on large amounts of [untranscribed data] with a [TTS network] trained on a smaller set of high [quality data]. \n",
      "\n",
      " Table : [Speech] from [fictitious speakers] compared to their nearest neighbors in the train sets. \n",
      "\n",
      " [Fictitious speakers] \n",
      "\n",
      " Bypassing the [speaker encoder network] and conditioning the synthesizer on random points in the speaker embedding space results in [speech] from [fictitious speakers] which are not present in the train or test sets of either the synthesizer or the [speaker encoder]. \n",
      " This is demonstrated in Table, which compares 10 [such speakers], generated from uniformly sampled points on the surface of the unit hypersphere, to their nearest neighbors in the training sets of the component networks. \n",
      " SV-EERs are computed using the same setup as Section after enrolling [voices] of the 10 nearest neighbors. \n",
      " Even though these speakers are totally fictitious, the synthesizer and the [vocoder] are able to generate [audio] as natural as for seen or unseen [real speakers]. \n",
      " The low cosine similarity to the nearest neighbor training utterances and very high EER indicate that they are indeed distinct from the [training speakers]. \n",
      "\n",
      " Conclusion \n",
      "\n",
      " We present a [neural network-based system] for multispeaker [TTS synthesis]. \n",
      " The system combines an independently trained [speaker encoder network] with a [sequence-to-sequence TTS synthesis] network and [neural vocoder] based on [Tacotron] 2. \n",
      " By leveraging the knowledge learned by the [discriminative speaker encoder], the synthesizer is able to generate high quality [speech] not only for speakers seen during training, but also for speakers never seen before. \n",
      " Through evaluations based on a [speaker verification] system as well as subjective listening tests, we demonstrated that the [synthesized speech] is reasonably similar to real [speech] from the [target speakers], even on such [unseen speakers]. \n",
      " We ran experiments to analyze the impact of the amount of data used to train the different components, and found that, given [sufficient speaker] diversity in the synthesizer training set, [speaker transfer] quality could be significantly improved by increasing the amount of [speaker encoder training data]. \n",
      " Transfer learning is critical to achieving these results. \n",
      " By separating the training of the [speaker encoder] and the synthesizer, the system significantly lowers the requirements for multispeaker [TTS training data]. \n",
      " It requires neither [speaker identity labels] for the synthesizer [training data], nor high quality clean [speech] or transcripts for the [speaker encoder training data]. \n",
      " In addition, training the components independently significantly simplifies the training configuration of the [synthesizer network] compared to since it does not require additional triplet or contrastive losses. \n",
      " However, modeling [speaker variation] using a low dimensional [vector] limits the ability to leverage large amounts of reference [speech]. \n",
      " Improving [speaker similarity] given more than a few seconds of reference [speech] requires a model adaptation approach as in, and more recently in. \n",
      " Finally, we demonstrate that the model is able to generate realistic [speech] from [fictitious speakers] that are dissimilar from the training set, implying that the model has learned to utilize a realistic representation of the space of [speaker variation]. \n",
      " The proposed model does not attain [human-level naturalness], despite the use of a [WaveNet vocoder] (along with its very high inference cost), in contrast to the [single speaker] results from. \n",
      " This is a consequence of the additional difficulty of generating [speech] for a variety of speakers given significantly [less data per] speaker, as well as the use of datasets with [lower data] quality. \n",
      " An additional limitation lies in the model’s inability to transfer accents. \n",
      " Given sufficient [training data], this could be addressed by conditioning the synthesizer on [independent speaker] and accent embeddings. \n",
      " Finally, we note that the model is also not able to completely isolate the [speaker voice] from the [prosody] of the reference [audio], a similar trend to that observed in. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('tts-lexicon4.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/7.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    text_dataframe.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    annotate(data, text_dataframe)\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
