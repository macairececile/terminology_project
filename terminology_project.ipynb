{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: Cécile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "from termcolor import colored\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon and store in dataframe\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def lemma_lexicon(dataframe):\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el)\n",
    "        tmp = [token.lemma_.lower() for token in doc]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    dataframe['lemma'].replace(lemma)\n",
    "    return dataframe\n",
    "  \n",
    "def select_data(dataframe):\n",
    "    \"\"\"We keep only columns pattern, pilot and lemma\"\"\"\n",
    "    return dataframe[['pattern', 'pilot', 'lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    text = read_file(file)\n",
    "    doc = spacy_nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_.lower())\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "    frame = pd.DataFrame({'tokens':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link terms with text\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    rules(terms_dataframe, text_dataframe)\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Cas pour 4\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        i += 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        i += 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                        i += 1\n",
    "            elif token == term[0]:\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules for a lemma according to its pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+3:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'to' and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    if text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    if len(t) == 5:\n",
    "                        if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    elif len(t) == 4:\n",
    "                        if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    elif len(t) == 3:\n",
    "                        if text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    elif len(t) == 2:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract : [[Grapheme] - to - [phoneme] conversion] is the process of generating [pronunciation] for [words] based on their written form . \n",
      " It has a highly essential role for natural [language [processing]] , [[text] - to - [speech [synthesis]]] and [automatic [speech [recognition]]] [systems] . \n",
      " In this paper , we investigate [convolutional [neural networks]] ( CNN ) for [G2P conversion] . \n",
      " We propose a novel CNN - based [[sequence] - to - [sequence]] ( [seq2seq] ) architecture for [G2P conversion] . \n",
      " Our approach includes an end - to - end CNN [G2P conversion] with [residual connections] and , furthermore , a [model] that utilizes a [convolutional [neural network]] ( with and without [residual connections] ) as encoder and [Bi - [LSTM]] as a [decoder] . \n",
      " We compare our approach with state - of - the - art methods , including [Encoder - [Decoder]] [LSTM] and [Encoder - [Decoder]] [Bi - [LSTM]] . \n",
      " [Training] and inference times , [phoneme] and [word [error]] rates were evaluated on the public [CMUDict] [dataset] for US English , and the best performing [convolutional [neural network]] - based architecture was also evaluated on the [NetTalk] [dataset] . \n",
      " Our method approaches the accuracy of previous state - of - the - art [results] in [terms] of [[phoneme] error] rate . \n",
      " Keywords : [[grapheme] - to - [phoneme]] ( G2P ) ; [encoder - [decoder]] ; [LSTM] ; [1D [convolution]] ; [Bi - [LSTM]] ; residual architecture \n",
      "\n",
      "\n",
      "\n",
      " Introduction \n",
      " The process of [[grapheme] - to - [phoneme]] ( G2P ) conversion generates a [phonetic] transcription from the written form of [words] . \n",
      " The spelling of a [word] is called a [grapheme] [sequence] ( or [graphemes] ) , the [phonetic] form is called a [[phoneme] [sequence]] ( or [phonemes] ) . \n",
      " It is essential to develop a phonemic [lexicon] in [[text] - to - [speech]] ( TTS ) and [automatic [speech [recognition]]] ( ASR ) [systems] . \n",
      " For this purpose , G2P techniques are used , and getting state - of - the - art performance in these [systems] depends on the accuracy of [G2P conversion] . \n",
      " For instance , in ASR [acoustic [[models]]] , the [pronunciation] [lexicons] and [language [[models]]] are critical [components] . \n",
      " [Acoustic] and [language [[models]]] are built automatically from large corpora . \n",
      " [Pronunciation] [lexicons] are the middle [layer] between [acoustic] and [language [[models]]] . \n",
      " For a new [speech [recognition]] task , the performance of the overall [system] depends on the quality of the [pronunciation] [component] . \n",
      " In other [words] , the [system] ’s performance depends on G2P accuracy . \n",
      " For example , the [G2P conversion] of [word] ‘ [speaker] ’ is ‘S P IY K ER ’ . \n",
      " In [TTS [systems]] , a high - quality G2P [model] is also an essential part and has a great influence on the overall quality . \n",
      " Inaccurate [G2P conversion] [results] in unnatural [pronunciation] or even incomprehensible [synthetic [speech]] . \n",
      "\n",
      "Abstract : [Grapheme - to - phoneme conversion] is the process of generating [pronunciation] for [words] based on their written form . \n",
      " It has a highly essential role for natural [language processing] , [text - to - speech synthesis] and [automatic speech recognition] [systems] . \n",
      " In this paper , we investigate [convolutional neural networks] ( CNN ) for [G2P conversion] . \n",
      " We propose a novel CNN - based [sequence - to - sequence] ( [seq2seq] ) architecture for [G2P conversion] . \n",
      " Our approach includes an end - to - end CNN [G2P conversion] with [residual connections] and , furthermore , a [model] that utilizes a [convolutional neural network] ( with and without [residual connections] ) as encoder and [Bi - LSTM] as a [decoder] . \n",
      " We compare our approach with state - of - the - art methods , including [Encoder - Decoder] [LSTM] and [Encoder - Decoder] [Bi - LSTM] . \n",
      " [Training] and inference times , [phoneme] and [word error] rates were evaluated on the public [CMUDict] [dataset] for US English , and the best performing [convolutional neural network] - based architecture was also evaluated on the [NetTalk] [dataset] . \n",
      " Our method approaches the accuracy of previous state - of - the - art [results] in [terms] of [phoneme error] rate . \n",
      " Keywords : [grapheme - to - phoneme] ( G2P ) ; [encoder - decoder] ; [LSTM] ; [1D convolution] ; [Bi - LSTM] ; residual architecture \n",
      "\n",
      "\n",
      "\n",
      " Introduction \n",
      " The process of [grapheme - to - phoneme] ( G2P ) conversion generates a [phonetic] transcription from the written form of [words] . \n",
      " The spelling of a [word] is called a [grapheme] [sequence] ( or [graphemes] ) , the [phonetic] form is called a [phoneme sequence] ( or [phonemes] ) . \n",
      " It is essential to develop a phonemic [lexicon] in [text - to - speech] ( TTS ) and [automatic speech recognition] ( ASR ) [systems] . \n",
      " For this purpose , G2P techniques are used , and getting state - of - the - art performance in these [systems] depends on the accuracy of [G2P conversion] . \n",
      " For instance , in ASR [acoustic models] , the [pronunciation] [lexicons] and [language models] are critical [components] . \n",
      " [Acoustic] and [language models] are built automatically from large corpora . \n",
      " [Pronunciation] [lexicons] are the middle [layer] between [acoustic] and [language models] . \n",
      " For a new [speech recognition] task , the performance of the overall [system] depends on the quality of the [pronunciation] [component] . \n",
      " In other [words] , the [system] ’s performance depends on G2P accuracy . \n",
      " For example , the [G2P conversion] of [word] ‘ [speaker] ’ is ‘S P IY K ER ’ . \n",
      " In [TTS systems] , a high - quality G2P [model] is also an essential part and has a great influence on the overall quality . \n",
      " Inaccurate [G2P conversion] [results] in unnatural [pronunciation] or even incomprehensible [synthetic speech] . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    init_data = read_data('tts-lexicon2.tsv')\n",
    "    change_lemma = lemma_lexicon(init_data)\n",
    "    data = select_data(change_lemma)\n",
    "    text_dataframe = lemma_posttag('test.txt')\n",
    "    annotate(data, text_dataframe)\n",
    "#     print(text_dataframe.tail(50))\n",
    "    print(' '.join(text_dataframe['tokens'].to_list()))\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
