{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: Cécile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "#     print(len(original))\n",
    "#     print(len(lemma))\n",
    "#     print(len(t))\n",
    "#     print(len(pos))\n",
    "#     print(len(new_pos))\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_adj = ['multilingual', 'autoregressive', 'monolingual', 'supervised', 'unsupervised', 'acoustic', 'phonetic', 'cross-lingual',\n",
    "           'intelligible', 'unlabelled', 'labelled', 'accented', 'bilingual', 'training', 'generated', 'fluent',\n",
    "           'neural', 'artificial', 'bidirectional', 'gated', 'attentional', 'substantial']\n",
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['lemma'][i+2] == 'by' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+5] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+6] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+6] = text_dataframe['tokens'][i+6]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+5] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '[' + text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i + 4] = text_dataframe['tokens'][i + 4] + ']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A' or text_dataframe['pattern'][i + 3] == 'V') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'voice' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i >= 3: \n",
    "                if text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                    text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                    text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['accent', 'accuracy', 'activation', 'adaptation', 'algorithm', 'alignment', 'approach', \n",
    "          'architecture', 'attribute', 'boundary', 'cell', 'class', 'classifier', 'cluster', 'component', \n",
    "          'concatenation', 'content', 'contour', 'control', 'conversion', 'coverage', 'detection', \n",
    "          'detection', 'device', 'dictionary', 'embedding', 'encoding', 'engineering', 'entry', 'error', \n",
    "          'evaluation', 'experiment', 'expertise', 'file', 'filter', 'form', 'framework', 'function', \n",
    "          'generation', 'implementation', 'improvement', 'inference', 'input', 'kernel', 'layer', 'learning', \n",
    "          'length', 'location', 'mapping', 'method', 'model', 'module', 'naturalness', 'network', \n",
    "          'nonlinearity', 'optimization', 'output', 'pair', 'parameter', 'pipeline', 'posterior', 'prediction', \n",
    "          'process', 'processing', 'quality', 'realization', 'recognition', 'representation', 'research', \n",
    "          'result', 'sample', 'score', 'sequence', 'set', 'setting', 'signal', 'string', 'study', 'symbol', \n",
    "          'synthesis', 'synthesizer', 'system', 'task', 'technique', 'technique', 'technology', 'token', 'tool', \n",
    "          'toolkit', 'training', 'transcription', 'transfer', 'transform', 'translation', 'value']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i >= 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' ’', '’')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised and [lightly-supervised learning] for rapid construction of [TTS systems] in [multiple languages] from ‘ found’ data : evaluation and analysis \n",
      "\n",
      " Abstract \n",
      "\n",
      " This paper presents techniques for building [text-to-speech frontends] in a way that avoids the need for [language-specific expert knowledge], but instead relies on universal resources (such as the Unicode [character database]) and unsupervised learning from [unannotated data] to ease system development. \n",
      " The acquisition of [expert language-specific knowledge] and expert [annotated data] is a major bottleneck in the development of corpusbased [TTS systems] in [new languages]. \n",
      " The methods presented here side-step the need for such resources as pronunciation lexicons, [phonetic feature] sets, part of [speech] tagged data, etc. \n",
      " The paper explains how the techniques introduced are applied to the 14 languages of a corpus of ‘ found’ audiobook data. \n",
      " Results of an evaluation of the intelligibility of the systems resulting from applying these novel techniques to this data are presented. \n",
      " Index Terms : [multilingual speech synthesis], unsupervised learning, [vector] space model, [text-to-speech], [audiobook data]   \n",
      "\n",
      " Introduction \n",
      "\n",
      " Collecting and annotating the data necessary for training a corpus-based [text-to-speech (TTS) conversion] system in a [new language] requires considerable time and expert knowledge. \n",
      " Conventionally, [audio data] for training a synthesiser [back-end] (or [waveform] generator) will be gathered during a speciallyarranged recording session. \n",
      " For this, a recording script must be prepared, a suitable studio must be found, a [voice talent] must be recruited and [speech recording] must be carefully supervised. \n",
      " One of the primary goals of the [Simple4All] 1 project is to reduce the time and expert knowledge needed to produce new [TTS systems]. \n",
      " In we presented a [toolkit] – developed as part of this project – for segmenting and aligning existing freely-available recordings (audiobooks), circumventing to some extent the need to engineer purpose-recorded [speech] corpora. \n",
      " The outcome of applying those tools to audiobooks in 14 languages is what we have released under the name of the [Tundra corpus]. \n",
      " However, the problems associated with [TTS data]-collection do not stop when we have obtained transcribed [speech data] for training a synthesiser [back-end]. \n",
      " [TTS systems] also require a [front-end] (or text analysis module), which accepts input text and outputs a representation of an utterance suitable for input into the [back-end]. \n",
      " [TTS systems] generally represent utterances in terms of units and features based on [linguistic knowledge], such as [phonemes], syllables, lexical stress, phrase boundaries etc. \n",
      " The components of the [front-end] that predict these from input text are either made up of hand-written rules or statistical modules ; acquiring the expert knowledge required either to manually specify those rules, or to annotate a learning sample on which to train the [statistical models], represents a major obstacle to creating a [TTS system] for a new [target language] and requires highly specialised knowledge. \n",
      " Such non-trivial tasks include, for example, specifying a [phoneme]-set or part of [speech] (POS) tag-set for a language where one has not already been defined ; annotating plain text with POS tags, as required to train a [POS tagger] and annotating the surface forms of words with [phonemes] to build a pronunciation lexicon. \n",
      " The [toolkit] we are developing in [Simple4All] includes tools for constructing [TTS front-ends] which make as few implicit assumptions about the [target language] as possible, and which can be configured with minimal effort and expert knowledge to suit arbitrary new [target languages]. \n",
      " To this end, the modules rely on resources which are intended to be universal, such as the Unicode [character database], and employ unsupervised learning so that unlabelled text resources can be exploited without the need for costly annotation. \n",
      " The current paper presents these tools and explains how they were applied to the data of the [Tundra corpus] to produce [TTS systems] in 14 languages. \n",
      " We present the results of a listening test of the intelligibility of those systems, and thus evaluate the entire pipeline implemented by our [toolkit], which begins with raw found data and ends with trained [TTS systems]. \n",
      " An initial public version of tools for this whole pipeline (for segmenting and aligning found data and for producing [TTS systems] with minimal expert knowledge) is due to be released in November 2013. \n",
      " In prior work addressing the bottleneck in [TTS system] construction represented by the [front-end], unified systems aimed at producing complete systems have generally taken the strategy of providing infrastructure to ease the collection by non-experts of the conventional resources necessary for system construction. \n",
      " This infrastructure might take the form of user-friendly development environments, or training and on-going support. \n",
      " Prior work has also presented unsupervised methods for building systems based on letters rather than [phonemes], induction of [phone-sets], syllable-like units, or lexicons. \n",
      " However, this work has not been presented as an integrated framework for producing [end-to-end TTS systems]. \n",
      " Furthermore, despite the significant work on unsupervised learning in [Natural Language Processing] and Information Retrieval, potentially useful techniques developed in those fields have not been applied to the problem of [TTS front-end induction]. \n",
      "\n",
      " Database \n",
      "\n",
      " The [Tundra corpus] is a standardised [multilingual corpus] designed for [text-to-speech research] with imperfect or found data. \n",
      " It consists of 14 audiobooks in 14 [different languages] (Bulgarian, Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Polish, Portuguese, Romanian, Russian and Spanish) and amounts to approximately 60 hours of [speech]. \n",
      " A complete list of the audiobooks with their sources and durations can be found here http://tundra.simple4all.org. \n",
      " The corpus provides utterance-level alignments obtained with a [lightly supervised process] described in and. \n",
      " The accuracy of the alignment method, as described in is of 7 % SER and 0.8 % [WER], therefore some light [post-processing] is required in order to eliminate some of the erroneous utterances. \n",
      " Initial segmentation of the audiobooks into utterancesize chunks was performed using the [lightly supervised] GMMbased VAD described in. \n",
      " As most of the used audiobooks are recorded in non-specialised environments, the [speech data] underwent a light cleaning process : normalising the DC offset, applying a multi-band noise gate removal and an RMS-based deverberation method, as described in. \n",
      "\n",
      " System Construction \n",
      "\n",
      " For each of the 14 languages of the [Tundra corpus], a [TTS system] was trained with no reliance of [language-specific expertise]. \n",
      " Although speaker and recording differences mean that meaningful comparison between languages is difficult, we wished to make the training conditions for the 14 voices as uniform as possible. \n",
      " Therefore, we selected a 1 hour subset of each of the languages’ data on which to train voices for this evaluation : the method of [data selection] we used is explained in Section 3.1. \n",
      " Then text analysis and [waveform generation] components were trained on that selected data as explained in Sections 3.2 and 3.3, respectively. \n",
      "\n",
      " [Lightly-supervised data selection] \n",
      "\n",
      " Our principal current interest in [audiobook data] is that it presents a source of ‘ found’ data from which [TTS training databases] can be harvested without the need to construct a recording script, recruit a [native speaker] of the [target language], and supervise the recording of a script from scratch. \n",
      " In the present work, therefore, we ignore the other possible advantage of using [audiobook data] : that harnessing the variety of speaking styles present in audiobooks might enable us to produce less ‘ mechanical’-sounding [TTS systems]. \n",
      " Although this is a longer-term goal, we here follow an approach similar to the one presented in, which aims to select a neutral subset of a database containing diverse [speech]. \n",
      " In that paper, 9 utterancelevel [acoustic features] are used along with several textual cues to exclude diverse [speech] from the training set. \n",
      " Thresholds over these features are set manually by the system builder to exclude non-neutral utterances. \n",
      " For the current work we perform utterance selection using an active learning approach, with uncertainty sampling. \n",
      " Rather than being required to tune thresholds manually, the system builder is presented with example utterances and asked to indicate whether or not they are spoken in a neutral style. \n",
      " The interface therefore insulates the user from the details of the features used, and lets the user focus on what should be key : their intuitive response to hearing [speech samples]. \n",
      " The procedure we used is as follows : 1) [Feature extraction] First, frame-[level features] (F 0, energy and [spectral] tilt – approximated by 1st [mel cepstral coefficient]) are obtained, from which utterance-[level features] are computed. \n",
      " The fact that no thresholds need to be manually tuned means that we can afford to use a great many [more features] than the 9 employed in. \n",
      " Our feature set is based on the one described in : we compute mean, standard deviation, range, slope, minimum and maximum (6-level factor) for F 0, [spectral] tilt, and energy (3-level factor) in the following sub-segments of each utterance : entire utterance, 1st and 2nd halfs, all 4 quarters, first and last 100ms, first and last 200ms (11-level factor), giving a total of 198 features. \n",
      " 2) Initial labelling The user is presented with the [audio] of s randomly-selected seed utterances from the [whole corpus] (via a text-based user interface) and asked to [label] them keep or discard – utterances are [labelled] with the user’s decision. \n",
      " 3) Classifier training A classifier is trained on the [labelled] examples. \n",
      " Our choice of classifier is a bagged ensemble of [decision trees] because it can be trained quickly (allowing online active learning in real time), is robust against [noisy features] and able to accept unnormalised input variables, and mixtures of discrete and continuous input variables (allowing a great many different [acoustic features] to be used, and different types of features), allows the space of utterances to be partitioned recursively (enabling complex interactions between features to be detected), and provides robust estimates of class probabilities (important for step 4). \n",
      " 4) Uncertainty sampling The set of u uncertain examples (utterances about which the classifier is most uncertain – in the present case, the utterances which have closest to 0.5 keep probability). \n",
      " The utterances in this set are presented to the user for labelling. \n",
      " 5) Steps 3 and 4 are repeated as many times as time allows. \n",
      " 6) The set of utterances either [labelled] keep by the user are kept for training, as well as enough of the utterances to which the trained classifier gives the highest keep probability to, to make up the desired quantity of [training data]. \n",
      " For the work presented here, s was set to 15 and u was set to 1. \n",
      " That is, the user was asked to provide 15 [labels] at the outset, and presented with a single uncertain example at each iteration. \n",
      " The stopping criterion we used in this work was to limit the number of iterations to 15 – in the present, utterance selection time was limited to approximately 20 minutes [per] language, and 15 was found to be a reasonable number of iterations in that time. \n",
      " Informal comparison suggested the approach outlined is beneficial for this task, but in ongoing work we are testing this rigorously and comparing uncertainty sampling with random sampling, as well as applying our active learning tool to other [TTS tasks]. \n",
      "\n",
      " [Front-end construction] with unsupervised learning \n",
      "\n",
      " The [TTS front-end building tools] used for this work are based on ideas outlined in and applied to Spanish [TTS] in. \n",
      " Input to the system consists of the [audio] of utterances selected as described in Section 3.1, together with their text transcription (aligned at the utterance level) : in the present case, these are taken from the [Tundra corpus], and had been obtained as summarised in Section 2. \n",
      " As an additional input, 5 million words of running [text data] were obtained from Wikipedia in the [target languages] for construction of the wordand letterrepresentations described below. \n",
      " Text which is input to the system is assumed to be UTF-8 encoded : given UTF-8 text, text processing is fully automatic and makes use of a theoretically universal resource : the [Unicode database]. \n",
      "\n",
      " Figure : Use of a letter space to replace [phonetic] knowledge in [decision-tree based state]-tying. \n",
      " Shown here are 2 dimensions of the actual letter space induced in training the Romanian system described in the paper. \n",
      " The 3 lines bisecting the space represent the 3 questions actually asked in the uppermost fragment (first three ‘ generations’) of the state-tying [decision tree] for the central state of the model for [spectral envelope features]. Letters shown in black are ‘ heard’ by the system (i.e. are present in the transcriptions of the [audio training data]) but ones shown in grey are only ‘ seen’ (i.e. appear only in textual [training data]) and are mainly [foreign language] letters. \n",
      "\n",
      " Unicode character properties are used to tokenise the text and characterise tokens as words, whitespace, punctuation etc. \n",
      " Our modules have so far been successfully applied to a variety of alphabetic (Latin-based, Cyrillic) and alphasyllabic (Brahmic) scripts. \n",
      " Our [front-ends] currently expect text without abbreviations, numerals, and symbols (e.g. for currency) which require expansion ; however, the [lightly supervised learning] of modules to expand such non-standard words is an active topic of research, and we hope to integrate such modules into our [toolkit] in the near future. \n",
      " A [letter-based approach] is used, in which the names of letters are used directly as the names of [speech] modelling units (in place of the [phonemes] of a conventional [front-end]). \n",
      " This has given good results for languages with transparent alphabetic orthographies such as Romanian, Spanish and Finnish, and can give acceptable results even for languages with less transparent orthographies, such as English. \n",
      " The induced [front-ends make use] of no expert-specified categories of letter and word, such as [phonetic] categories (vowel, nasal, approximant, etc.) and part of [speech] categories (noun, verb, adjective, etc.). \n",
      " Instead, features that are designed to stand in for such expert knowledge but which are derived fully automatically from the distributional analysis of unannotated text ([speech transcriptions] and Wikipedia text) are used. \n",
      " The distributional analysis is conducted via [vector] space models (VSMs) ; the VSM was originally applied to the characterisation of documents for purposes of Information Retrieval. \n",
      " VSMs are applied to [TTS] in, where models are built at various levels of analysis (letter, word and utterance) from large bodies of unlabelled text. \n",
      " To build these models, co-occurrence statistics are gathered in matrix form to produce high-dimensional representations of the distributional behaviour of e.g. word and letter types in the corpus. \n",
      " Lower-dimensional representations are obtained by approximately factorising the matrix of raw cooccurrence counts by the application of slim singular value decomposition. \n",
      " This distributional analysis places textual objects in a continuous-valued space, which is then partitioned by [decision tree] questions during the training of [TTS system] components such as [acoustic models] for synthesis or [decision trees] for pause prediction. \n",
      " For the [present voices], a VSM of letters was constructed by producing a matrix of counts of immediate left and right co-occurrences of each letter type, and from this matrix a 5-dimensional space was produced to characterise letters. \n",
      " Token co-occurrence was counted with the nearest left and right neighbour tokens (excluding whitespace tokens) ; co-occurrence was counted with the most frequent 250 tokens in the corpus. \n",
      " A 10-dimensional space was produced to characterise tokens. \n",
      " Two dimensions of the letter space induced in training the Romanian system are shown in Figure 1. \n",
      " It can be seen that in these dimensions of the space, vowel and consonant symbols are clearly separable. \n",
      " When a [decision tree] for clustering [acoustic model] states is built and allowed to query items’ positions in these 2 dimensions, it can use all partitions of the space orthogonal to its axes. \n",
      " A [decision tree] question such as Is the letter’s value in VSM dimension 3 < -0.03 ? is very nearly equivalent to a question based on [linguistic knowledge] such as Is the letter a consonant ? The categories of vowel and consonant are useful for clustering [acoustic models], and so [decision trees] actually built using this space use such partitions of the space : the 3 lines shown bisecting the space in the figure represent the 3 questions actually asked in the uppermost fragment (first three ‘ generations’) of the state-tying [decision tree] for the central state of the model for [spectral envelope features]. \n",
      " Distributional analysis places linguistic or textual units in a continuous space which is then partitioned on acoustic evidence. \n",
      " The space constrains the possible groupings of objects that can be considered during [decision tree] growing. \n",
      " Distributional analysis also allows splits made to generalise to items that are ‘ seen’ by the system in [text data] but not ‘ heard’ in the [audio data]. \n",
      " This is most obviously useful where units such as words are concerned, where many items not present in the [training speech corpus] are likely to occur at run-time. \n",
      " It can, however, also be useful where letters are concerned, and some examples that illustrate our models’ ability to generalise beyond what is heard can be seen in the letter space shown in Figure 1. \n",
      " There, letters shown in black are ‘ heard’ by the system but ones shown in grey are only ‘ seen’ – these are mainly due to [foreign language] words within Romanian Wikipedia entries. \n",
      " It can be seen that unheard foreign vowels such as á and ö are suitably placed near the Romanian vowels, and unheard consonants such as ß and q are placed near the consonants that are actually heard. \n",
      " Splits such as those shown – made only on the basis of the heard items – therefore generalise to unheard items. \n",
      " In the case of letters, this allows rare and foreign letters to be handled despite their absence in the transcriptions of [acoustic training data]. \n",
      " It can also allow better handling of non-standard spellings : in the case of the vowel î (i with circumflex), there is a variant (with inverted breve instead of circumflex) which is not present in any of the [speech transcriptions] but which is used in a few Wikipedia articles. \n",
      " From Figure 1 it can be seen that almost identical representations are learned for these two letters, meaning a [decision tree] built using those representations will be able to handle the variant form correctly at run-time, even though no instances of that variant were seen in the transcription of the [speech training corpus]. \n",
      " The front ends make use of [decision trees] to predict pauses at the junctures between words. \n",
      " Data for training these trees are acquired automatically by force-aligning the [training data] with their transcriptions, and allowing the optional insertion of silence between words. \n",
      " The independent variables used by the trees are whether words are separated by punctuation or space, and the [VSM features] of the tokens preceding and following the juncture. \n",
      " A rich set of contexts is created using the results of the analysis described here for each letter token in the database. \n",
      " Features include the identity of the letter and the identities of its neighbours (within a 5-letter window), the VSM values of each of those letters, and the distance from and until a word boundary, pause, and utterance boundary. \n",
      " In the current systems, word [VSM features] are not included directly in the letter contexts, but are used by the [decision tree] for predicting pauses at runtime. \n",
      "\n",
      " [Back-end construction] \n",
      "\n",
      " For training the [waveform generation] modules for the 14 voices, the [waveforms] of the training corpora were parameterised almost as described in. \n",
      " The one difference is that instead of the committee of different [pitch]-trackers used in the earlier work, [pitch] tracks obtained from a glottal source signal estimated by glottal inverse filtering were used for their greater accuracy. \n",
      " For all systems, speaker-dependent [acoustic models] were built from this parameterised [speech data] and the annotation described in Section 3.2, using the speaker-dependent modelbuilding recipe described in. \n",
      " Static and interactive demos of the resulting voices are available at http://tundra.simple4all.org/demo. \n",
      " A screen shot of the geographically-organised demo page is shown in Figure 2. \n",
      "\n",
      " Figure : Demo screenshot : this geographical interface to voices can be found at http://tundra.simple4all.org/demo. \n",
      "\n",
      " System Evaluation \n",
      "\n",
      " Procedure \n",
      "\n",
      " We are primarily interested in having our systems produce [intelligible speech] ; evaluation therefore focused on the intelligibility of [TTS output] as measured by the word and letter error rates of listeners’ transcriptions of those outputs. \n",
      " Conventionally in [TTS evaluation], listeners are asked to transcribe semantically unpredictable sentences (SUS). \n",
      " However, such SUS are not currently available in all the [Tundra languages] and it is not trivial to construct new SUS, and so we resorted to using short natural sentences from the held-out test sets of the [Tundra corpus]. \n",
      " For all 14 [Tundra languages], 40 sentences were manually segmented from the held-out chapters of the relevant audiobook. \n",
      " Note that these test sets are distributed with the [Tundra corpus], and so the results presented below can be considered benchmarks for future work. \n",
      " An attempt was made to select sentences of 6–8 words in order to make the inherent difficulty of transcription as uniform as possible. \n",
      " However, in some languages these thresholds had to be relaxed ; Table 1 gives statistics of test-sentence lengths in all languages. \n",
      " Subjects for the evaluation were recruited through a webbased crowdsourcing service. \n",
      " The advert for the evaluation specified that [native speakers] of the [relevant language] were required ; in addition, participation in each part of the evaluation was restricted to users registered in countries where the [relevant language] is an official or [majority language]. \n",
      " We attempted to recruit listeners to evaluate all 14 systems built. \n",
      " However, as the option to restrict participation to workers registered in Denmark, Finland and Hungary was not available in the service we used, listening test for only 11 of the systems were publicised. \n",
      " The number of responses from participants varied greatly between languages. \n",
      " At the time of writing, responses from a sufficient number of listeners (25 +) had been collected in only 5 of the languages (Bulgarian, English, Italian, Polish and Romanian). \n",
      " Results for these five languages are presented here ; evaluation of the remaining voices is left for future work. \n",
      " In all languages, two conditions were evaluated : the natural [speech] of the natural sentences from the test set, and the [TTS system] reading the same text. \n",
      "\n",
      " Table : Statistics of [Tundra] test-sentence lengths (number of words) \n",
      "\n",
      " In the four languages of the [Simple4All] consortium members (including two of the languages for which results are presented here : Romanian and English), however, SUS were available, and so for those languages a third condition was evaluated : the [TTS system] producing SUS texts. \n",
      " This is designed to provide a way of broadly gauging the relative difficulty of transcribing natural and SUS sentences, although language and text differences mean it is obviously not advisable to treat extrapolation of the differences to the remaining languages with any great confidence. \n",
      " The evaluation was run as a set of webpages where participants were asked – using headphones – to listen to the samples and to type in what they heard. \n",
      " Multiple listens were allowed as some of the the natural sentences were longer than the short SUS we would typically use. \n",
      " For the first two conditions, a balanced design was used so that each listener heard each utterance text only once, while each text was heard an equal number of times in both conditions over the whole evaluation. \n",
      " Each listener heard 20 sentences spoken in each condition. \n",
      " For English and Romanian where the SUS condition was also included, listeners heard a further set of 20 SUS sentences. \n",
      "\n",
      " Results \n",
      "\n",
      " [Word error rates] for the first 2 conditions are shown in Figure 3. \n",
      " For all languages besides English, a similar pattern can be observed : listeners’ transcriptions of natural [speech] attain a [WER] of 8–12 %, and in all cases the [TTS system] attain [WERs] approximately 1.5 times worse. \n",
      " This is consistent with the difference between [WERs] for natural [speech] and decent benchmark systems in larger scale evaluations on standard corpora. \n",
      " For example, natural [speech] and the Festival benchmark system attained [WERs] of 17 % and 25 % respectively in the 2011 [Blizzard Challenge evaluation]. \n",
      " The results for English are the exception to the general pattern : the [WER] for [synthetic speech] is over 4 times worse than that of natural [speech]. \n",
      " From prior knowledge and from looking at listeners’ transcriptions, it seems clear that this is due to the fact that [TTS] is based on letters in a language with such an opaque [letter-to-sound relationship]. \n",
      " In all languages except Polish, the difference between the first two conditions (natural [speech] and [TTS]) found to be statistically significant (with α = 0.05) using the bootstrap procedure of. \n",
      " As expected, [WERs] for the SUS sentences are much higher than those for natural sentences : 24.8 % and 69.4 % for Romanian and English, respectively. \n",
      "\n",
      " Figure : [Word error rates] for [TTS systems] and natural [speech] for 5 of the 14 systems built from the [Tundra corpus]. \n",
      "\n",
      " Conclusions \n",
      "\n",
      " We have presented tools for building [TTS front-ends] in a way that exploits unsupervised learning techniques to side-step the need for [language-specific expert knowledge] and resources such as pronunciation lexicons, [phoneme] inventories and part of [speech] taggers. \n",
      " We have shown how the tools were applied to the languages of the [Tundra corpus] to produce [TTS systems] in 14 languages. \n",
      " As we had previously built the [Tundra corpus] from found data using minimal supervision and language specific knowledge, these [TTS systems] represent the output of our entire pipeline of tools, and show the type of voice which any interested developer should be able to build using our [toolkit] (which will be made freely available) despite a lack of [language-specific] or [speech technology expertise], if a source of [speech and text data] can be found. \n",
      " Five of the voices were evaluated in a listening test for intelligibility, which we consider to show that systems of reasonable quality can be built by applying our tools to publicly available [audiobook data], assuming orthographies of similar transparency to those of Bulgarian, Italian, Polish and Romanian. \n",
      " While evaluation of the remaining systems that can be heard in the demo is still ongoing, the results for five languages published here – having been obtained from a standardised, publicly [available corpus] – are intended to be useful benchmarks against which future work can be compared. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('lexicon.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/20.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    text_dataframe.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    annotate(data, text_dataframe)\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
