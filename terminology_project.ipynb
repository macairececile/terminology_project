{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: Cécile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "#     print(len(original))\n",
    "#     print(len(lemma))\n",
    "#     print(len(t))\n",
    "#     print(len(pos))\n",
    "#     print(len(new_pos))\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 216,
=======
   "execution_count": 4,
>>>>>>> e7ad802de0ec54bed7ecc6b6ee399fd1a292051d
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_adj = ['multilingual', 'autoregressive', 'monolingual', 'supervised', 'unsupervised', 'acoustic', 'phonetic', 'cross-lingual',\n",
    "           'intelligible', 'unlabelled', 'labelled', 'accented', 'bilingual', 'training', 'generated', 'fluent',\n",
    "           'neural', 'artificial', 'bidirectional', 'gated', 'attentional', 'substantial']\n",
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['lemma'][i+2] == 'by' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+5] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+6] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+6] = text_dataframe['tokens'][i+6]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+5] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '[' + text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i + 4] = text_dataframe['tokens'][i + 4] + ']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A' or text_dataframe['pattern'][i + 3] == 'V') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'voice' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i != 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 231,
=======
   "execution_count": 5,
>>>>>>> e7ad802de0ec54bed7ecc6b6ee399fd1a292051d
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['accent', 'accuracy', 'activation', 'adaptation', 'algorithm', 'alignment', 'approach', \n",
    "          'architecture', 'attribute', 'boundary', 'cell', 'class', 'classifier', 'cluster', 'component', \n",
    "          'concatenation', 'content', 'contour', 'control', 'conversion', 'coverage', 'detection', \n",
    "          'detection', 'device', 'dictionary', 'embedding', 'encoding', 'engineering', 'entry', 'error', \n",
    "          'evaluation', 'experiment', 'expertise', 'file', 'filter', 'form', 'framework', 'function', \n",
    "          'generation', 'implementation', 'improvement', 'inference', 'input', 'kernel', 'layer', 'learning', \n",
    "          'length', 'location', 'mapping', 'method', 'model', 'module', 'naturalness', 'network', \n",
    "          'nonlinearity', 'optimization', 'output', 'pair', 'parameter', 'pipeline', 'posterior', 'prediction', \n",
    "          'process', 'processing', 'quality', 'realization', 'recognition', 'representation', 'research', \n",
    "          'result', 'sample', 'score', 'sequence', 'set', 'setting', 'signal', 'string', 'study', 'symbol', \n",
    "          'synthesis', 'synthesizer', 'system', 'task', 'technique', 'technique', 'technology', 'token', 'tool', \n",
    "          'toolkit', 'training', 'transcription', 'transfer', 'transform', 'translation', 'value']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i != 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 232,
=======
   "execution_count": 6,
>>>>>>> e7ad802de0ec54bed7ecc6b6ee399fd1a292051d
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' ’', '’')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 235,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> e7ad802de0ec54bed7ecc6b6ee399fd1a292051d
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Text Preprocessing for [Speech Synthesis] \n",
      "\n",
      " Abstract \n",
      "\n",
      " In this paper we describe our text preprocessing modules for English [text-to-speech synthesis]. \n",
      " These modules comprise [rule-based text normalization] subsuming sentence segmentation and normalization of non-standard words, statistical [part-of-speech tagging], and statistical [syllabification], [grapheme-to-phoneme conversion], and word stress assignment relying in parts on [rule-based morphological analysis]. \n",
      "\n",
      " Introduction \n",
      "\n",
      " Text preprocessing for English [text-to-speech (TTS) synthesis] in general consists of the following steps : [Text Normalization]. \n",
      " This first step subsumes sentence segmentation, tokenizing, and normalization of nonstandard words. \n",
      " For sentence segmentation the main problem is the ambiguity of the period, marking sentence boundaries or abbreviations, sometimes even simultaneously (it is 5 p.m.). \n",
      " For period disambiguation an identification of abbreviations is needed as well as a disambiguation of capitalized words (proper names vs.sentence initial words, thus words following a sentence boundary period). \n",
      " Complications arise from abbreviations that do not differ from ordinary sentence final words (no. also being an abbreviation of number) and from the fact that also proper names can occur in sentence initial position. \n",
      " [Rule-based systems] for heuristic period disambiguation operate on local grammars containing abstract contexts for within-sentence periods and sentence boundaries (Cherry and Vesterman, 1991 ; Aberdeen et al., 1995). \n",
      " Mikheev’s (2002) [rule-based segmentation] is preceded by capitalized word disambiguation. \n",
      " [Machine learning approaches] as the [decision tree classifier] in Riley (1989) use [context features] such as word lengths, capitalization, and word occurrence probabilities on both sides of the period in question. \n",
      " Current systems achieve an error rate down to less than 1 %. \n",
      " Tokenizing in its simplest form is achieved by splitting the text at white spaces and at punctuation marks, that do not belong to abbreviations identified in the preceding step. \n",
      " Non-standard words are tokens to be expanded to an appropriate orthographic form before [grapheme-to-phoneme conversion]. \n",
      " Their normalization includes amongst others number conversion, homograph disambiguation (Henry, Scene, Mr), expansion of abbreviations and symbols, and appropriate treatment of acronyms (some have to be spelled, others not), and email and URL addresses. \n",
      " A token might be split into several words by these operations. \n",
      " Normalization is a difficult task, since creation of the nonstandard word types mentioned above is arbitrarily productive and therefore not to be solved solely by table lookup. \n",
      " Furthermore [phonetic realization] is highly context dependent, examples are the homographs above or digit strings which can be realized either as numbers, phone numbers or years. \n",
      " While most of the normalization systems tackle this problem by heuristic disambiguation and expansion rules, e.g. Black et al. (1999), there are also some [language modeling] and [machine learning approaches] for normalization subtasks. \n",
      " For example in Sproat et al. (2001) word normalization is amongst others formulated in terms of maximizing the conditional probability of a normalized word sequence given an observed token sequence. \n",
      "\n",
      " [Part-of-Speech Tagging] \n",
      "\n",
      " [Part-of-Speech tagging] means word class assignment to each token. \n",
      " Its input is given by the tokenized text. \n",
      " Taggers have to cope with unknown words (OOV problem) and ambiguous word-tag mappings. \n",
      " [Rule-based approaches] like ENGTWOL (Voutilainen, 1995) operate on a) dictionaries containing word forms together with the associated POS [labels] and morphologic and [syntactical features] and b) context sensitive rules to choose the appropriate [labels] during application. \n",
      " In [statistical approaches] (Jelinek, 1985) generally the most probable tag sequence given the observed word sequence is estimated. \n",
      " In [transformation-based tagging] (Brill, 1995) a [hybrid approach] can be found, where disambiguation rules are derived by statistical means. \n",
      " [Grapheme-to-Phoneme Conversion] Since hand-crafted rule generation for [language processing] is very timeconsuming and corresponding systems are highly language dependent, most of the current [G2P systems] are purely data-driven (see e.g. Yvon (1994) for an overview over some machine learning approaches to [G2P conversion]). \n",
      " Accounting for the influence of [morphology] and syllable structure can improve performance of [G2P conversion] (Reichel and Schiel, 2005). \n",
      " Word Stress The assignment of English word stress relies on [phonological], morphological, and word [class features]. \n",
      " The crucial [phonological feature] is syllable weight : heavy syllables rather attract stress than weak ones. \n",
      " Amongst the [morphological features] are affix types (stressed vs. unstressed vs. pre-stressed) and the position within a compound. \n",
      " Word class and word syllable length determine default stress patterns. \n",
      " Metrical [phonology] (Liberman and Prince, 1977) accounts for lots of these factors and is a useful framework for [rule-based approaches] to word stress assignment. \n",
      " Among the data-driven approaches are [neural networks] (Gupta and Touretzky, 1994) predicting stress patterns given syllable weight patterns and instance-based learning (Daelemans and van den Bosch, 1997) which matches new words against words with an already known stress pattern. \n",
      " In the following sections our [TTS text preprocessing modules] are presented. \n",
      "\n",
      " [Text Normalization] \n",
      "\n",
      " Identification of Proper Names, Acronyms and Abbreviations \n",
      "\n",
      " Since retrieval of proper names, acronyms and abbreviations is crucial for appropriate sentence segmentation and normalization of non-standard words, this task is carried out prior to [text normalization]. \n",
      " Due to the high productivity of these word classes simple table lookup is insufficient and has to be augmented by following procedures. \n",
      " Proper names All those tokens are considered as proper names that occur only and at least twice in capitalized form. \n",
      " Only occurrences in unambiguous environments are counted, that means not behind a period except for periods of prepositional titles like Mr, Dr, etc. \n",
      " Abbreviations Token t is identified as an abbreviation, if 1) it has not been classified as a proper name and 2) it ends with a period and 3) one of the following conditions is fulfilled : \n",
      " • t contains another period (e.g.), or \n",
      " • the string of t preceding the period consists of just one small letter, or \n",
      " • t contains no vowel (exception qu.) and at least one small letter (vs. acronyms, numbers), or \n",
      " • the letter sequence of t indicates a violation of [phonotactics] (see below). \n",
      " Acronyms Token t is identified as an acronym, if it has not been classified as an proper name or abbreviation and has not been classified as a roman number (using local grammars) and if one of the following conditions holds : \n",
      " • t consists entirely of consonants, or \n",
      " • t consists entirely of capitals (except I), or \n",
      " • t is preceded by the article an and does not start with a vowel, or \n",
      " • t is preceded by the article a and starts with a vowel (except u), or \n",
      " • the letter sequence of t indicates a violation of [phonotactics]. \n",
      " Violation of [Phonotactics] The [phonotactics] exploited here is related to the sonority-based syllable definition according to which a syllable is characterized by a sonority peak facultatively preceded by a rise and followed by a decline of sonority (in case of presence of head and coda, respectively). \n",
      " A letter sequence of a token indicates a violation of [phonotactics] if 1) the first (resp. last) letter can be associated with a [phoneme] of higher sonority than that of a fricative (which can occur as a syllable appendix), and 2) the sonority of that [phoneme] is higher than the [phoneme] associated with the following (resp. preceding) letter, and 3) none of the two letters in focus can be associated with a syllable nucleus. \n",
      " For word beginnings vowel letters are associated with syllable nuclei, for word endings also < m >, < n > ; < l > is not treated as a nucleus associate since syllabic /l/ is represented by le in English orthography. \n",
      " Examples : incl. is identified as an abbreviation while wrists. and fascism. are treated as standard words followed by a period. \n",
      "\n",
      " Sentence Segmentation \n",
      "\n",
      " The hand-crafted binary [decision tree] in Figure 1 guides the decision whether or not token t i is followed by a sentence boundary. \n",
      " i is ranging over the tokens of the present tokenization of the text at white spaces and unambiguous punctuation. \n",
      " The sentence segmentation completes the tokenization process. \n",
      "\n",
      " Normalization of non-standard words \n",
      "\n",
      " For space reasons we present just a selection of our normalization procedures here. \n",
      " Numbers In general the following number transformations are carried out : roman numbers are converted to arabic numbers by calculation and arabic numbers are converted to letters by finite state [transducers] for cardinal and ordinal numbers. \n",
      " The identification of roman numbers and the distinction of cardinals and ordinals is guided by local grammars. \n",
      " Cardinal numbers are disambiguated whether to be pronounced as one number, as a date, or digit by digit through pattern matching and examination of the text environment regarding e.g. date-related or phone number cues. \n",
      " Dates are further completed by prepositions and articles accordingly. \n",
      " E.g. 12 Feb becomes on the twelve of February, but on being omitted if a preposition is already given. \n",
      " Abbreviations and Acronyms Unknown abbreviations are spelled. \n",
      " Unknown acronyms are spelled if indicated by a preceding indefinite article or by violation of [phonotactics] (incl. lack of vowels ; see above). \n",
      " Otherwise they are pronounced as standard words. \n",
      " This acronym examination also takes place for each part of a hyphenated compound (CD-Rom) and within URLs and email addresses. \n",
      "\n",
      " [Part-of-Speech Tagging] \n",
      "\n",
      " Our approach for [POS tagging] described in more detail in Reichel (2005) is statistical and can be seen as a generalization of the classical [Markov tagger] presented by Jelinek (1985). \n",
      " The P (w|t) emission probabilities of word w given tag t are replaced by a linear interpolation of tag emission probabilities given a list of representations of w, that are connected to automatically derived word suffixes. \n",
      " Since in [English language] suffixes also store word class information and are observed in the [training data] with a high probability, the OOV problem can be reduced this way. \n",
      " However, no [linguistic knowledge] is needed, hence our approach is [language independent]. \n",
      "\n",
      " Basic Form of a [Markov POS Tagger] \n",
      "\n",
      " The aim is to estimate the probable tag sequence T̂ given word sequence W : \n",
      " To estimate P (T |W) first a reformulation is needed by applying Bayes Formula, which leads to : \n",
      " given that the denominator P (W) is constant. \n",
      " Further two simplifying assumptions are to be made to get reliable \n",
      " counts for the probability estimations : \n",
      " • Probability of word w i depends only on its tag t i. \n",
      " • Probability of tag t i depends only on a limited tag history. \n",
      " The resulting formula is thus : \n",
      " T̂ = arg max t 1... t n n h Y i=1 P (t i |t-history i) P (w i |t i) i (3) \n",
      " T̂ is retrieved using the [Viterbi algorithm] ([Viterbi], 1967). \n",
      "\n",
      " Generalizations of the basic model \n",
      "\n",
      " First P (t i |t-history i) is replaced by a linearly interpolated [trigram model] \n",
      " Xju j P (t i |t-history ij), j ranging from unigram to trigram tag history. Further w i is replaced by a list of word representations leading to a reformulation of P (w i |t i): \n",
      " P (w i) X v k P (t i |w-representation ik) \n",
      " applying again Bayes Formula and linear interpolation. \n",
      " Our model is thus given by : \n",
      "\n",
      " The interpolation weights u j and v k are calculated via the EM algorithm (Dempster et al., 1977). \n",
      " In order to reduce calculation effort in application, just for unknown words the probabilities are calculated for all POS tags. \n",
      " For known words just the POS tags co-occurring with them in the [training corpus] are taken into consideration. \n",
      " Our [training data] comprises 620000 tokens (including punctuation) taken from prose of the 20th century and pretagged by the TnT tagger (Brants, 2000) using the Penn tag set (Marcus et al., 1995). \n",
      "\n",
      " Word representations \n",
      "\n",
      " The representation of words seen in the [training data] is simply the word form. \n",
      " For OOV cases the representation is given by two string suffixes which are determined by Normalized Backward Successor Variety (NBSV). \n",
      " The Successor Variety (SV) of a string is defined as the number of different characters that follow the string in a given lexicon. \n",
      " This concept is adopted from [stemming] procedures like the Peak and Plateau algorithm of Nascimento and da Cunha (1998). \n",
      " Backward SV means that the SVs are calculated from reversed strings in order to increase the probability to separate linguistically meaningful suffixes. \n",
      " In our approach the SVs are weighted with respect to the mean SV at the corresponding string position to eliminate positional effects. \n",
      " The mean SV is highest in the beginning and declines continuously while moving forward in the word string. \n",
      " The lexicon of reversed words is represented in the form of a trie (cf. Figure 2), in which the SV at a given state is the number of all outgoing transitions. \n",
      " NBSV peaks are treated as [morpheme boundaries]. \n",
      " Since this method is knowledge free, of course not all of the obtained segments necessarily correspond to linguistic meaningful entities as might be suggested by Figure 2. \n",
      "\n",
      " [Grapheme-to-Phoneme Conversion] \n",
      "\n",
      " Our [G2P approach] is data-driven ; as a classifier we use the C4.5 [decision tree] (Quinlan, 1993). \n",
      " We treat the conversion as a one-to-one mapping from the set of [graphemes] to the set of [phonemes] (UK [SAMPA]). \n",
      " To cope with any n-to-n relation the [phoneme set] also comprises the empty [phoneme] as well as [phoneme clusters]. \n",
      " A canonical [pronunciation dictionary] containing 61340 entries is used for training and lookup at application time. \n",
      "\n",
      " Alignment \n",
      "\n",
      " The first step for creating the [grapheme-to-phoneme converter] was to align the [phoneme string] and the orthographic string of each [pronunciation dictionary entry]. \n",
      " Inspired by the work of Daelemans and van den Bosch (1997) an initial co-occurrence matrix between letters and [phonemes] was estimated. \n",
      " This was done by diagonally aligning the letters and [phonemes] of each entry (see Figure 3). \n",
      " For each [phoneme] a triangular window with an area of 1 and a width of 5 letters was centered at the diagonal in order to spread the probability of co-occurence to adjacent letters. \n",
      " The values of the initial co-occurrence matrix are converted into probabilities and used in a [Dynamic Programming algorithm] to find the most likely alignment for each [pronunciation dictionary entry]. \n",
      " The [DP algorithm] is designed to align either the empty [phoneme], ore one [phoneme], or a [phoneme cluster] to each letter. \n",
      " In order to get a left-aligned [phoneme string] which is necessary for its alignment with morphologic segments (see below), heuristic [post-processing] was applied. \n",
      "\n",
      " [Syllable Segmentation] \n",
      "\n",
      " Since syllable structure influences [G2P conversion] and is furthermore needed for word stress assignment (see below), [syllable segmentation] is carried out in advance. \n",
      " Also for [syllable segmentation] a C4.5 [decision tree] is trained deciding for each letter whether or not a syllable boundary follows. \n",
      " The current letter as well as the surrounding letters within a window of length 9 are used as features. \n",
      " For model development the same dictionary is used as above, 80 % of the entries taken for training and 20 % for testing. \n",
      " The resulting [decision tree] yields a letter error rate of 1.2 % and a [word error rate] of 8.6 % on the [test data]. \n",
      "\n",
      " Features \n",
      "\n",
      " To map a [grapheme] g on the corresponding [phoneme], the [decision tree] is supplied with 24 features : \n",
      " • [graphemes] within a window of length 9 centered at g \n",
      " • information whether or not a syllable boundary follows for each [grapheme] within that window \n",
      " • position of g within the current syllable (head, nucleus, coda) \n",
      " • type of the current syllable (onset / null onset, open / closed) \n",
      " • relative position of g within the word • [phoneme] history of length 3. \n",
      "\n",
      " Word stress assignment \n",
      "\n",
      " In our approach word stress is assigned again by a C4.5 [decision tree] deciding for each syllable whether or not being stressed. \n",
      " Since English word stress is governed by [phonology], [morphology], and word class (see above) the classifier should be provided by features of all three domains. \n",
      " The [phonological features] are derived from [syllabification] and [G2P conversion], word [class features] from [POS tagging]. \n",
      " To obtain [morphologic features] some morphologic analysis has to be carried out. \n",
      "\n",
      " Morphologic segmentation \n",
      "\n",
      " The segmentation algorithm we used here is a simplified version of the procedure presented in Reichel and Weilhammer (2004). \n",
      " It consists of two stages : lexicon construction and segmentation. \n",
      " Since it requires some knowledge about affixation it is applicable for [different languages] just in combination with language dependent stemmers and affix lexica. \n",
      "\n",
      " Lexicon construction \n",
      "\n",
      " The lexicon initially comprises English prefixes and suffixes and the linking [morpheme] ‘ -’. \n",
      " It is then augmented by [stems] and [prefix-stem concatenations] of nouns, verbs, adjectives, and adverbs resulting from the application of a slightly modified [Porter stemmer] (Porter, 1980) for suffix separation. \n",
      " Table 1 shows the [morpheme classes] of the lexicon entries. \n",
      "\n",
      " Table : [Morpheme classes]. w : word left unchanged by the [Porter stemmer]. \n",
      "\n",
      " Segmentation \n",
      "\n",
      " Each word w is [stemmed] by the [Porter stemmer]. \n",
      " Then the [stem] and the suffix string are further segmented by the function segmentation (see Figure 4) the following way : \n",
      " Each input s is recursively divided into string prefixes and suffixes from left to right until a permitted segmentation is achieved or until the end of s is reached. \n",
      " In the course of the recursion, a boundary dividing the current string into prefix and suffix is accepted if 1) the prefix is found in the lexicon, 2) there exists a permitted segmentation for the suffix or (if not) the suffix is found in the lexicon, and just for [stem] segmentation, 3) the sequence ‘ prefix class + class of first suffix segment’ is not in conflict with simplified English morphotactics as represented by the automaton in Figure 5. \n",
      " On a random test sample of 1000 word types our system yields a word accuracy of 79.6 % for completely correct morphologic analysis. \n",
      " Future improvements can be achieved by modifying the [Porter stemmer] in order to cope with short ly-adverbs and comparative adjectives. \n",
      "\n",
      " Features \n",
      "\n",
      " For each syllable s the following features are used for word stress assignment : \n",
      " • word class \n",
      " • [syllable features] \n",
      " – syllable weight (reduced, light, heavy) \n",
      " – syllable type (onset / null onset, open / closed) \n",
      " – word syllable length \n",
      " • [morphologic features] (and features derived from morphologic segmentation) \n",
      " – class of the [morpheme] containing the nucleus of s (cf. Table 1). \n",
      " Prefixes and suffixes are further divided into stressed and unstressed affixes (suffixes : also pre-stressed). \n",
      " – index of current compound part \n",
      " – absolute and relative position of s within whole word and respective compound part \n",
      " – only stressable syllable (binary ; nucleus in stressed affix or in only stressable [morpheme]) \n",
      " Syllable weight is extracted within a 5 syllable window centered on s, [morpheme class] within a 3 [morpheme] window centered on the [morpheme] containing the nucleus of s. \n",
      "\n",
      " Results \n",
      "\n",
      " [Evaluation data] taken from the “ European Parliament Plenary Session ” (EPPS) corpus was provided by ELDA. \n",
      " ELDA also carried out the evaluations, but due to some convention differences (see below) we had to revise the results. \n",
      "\n",
      " [Text Normalization] \n",
      "\n",
      " Sentence Segmentation [End-of]-sentence detection was evaluated for 500 sentences. \n",
      " Given two errors the error rate amounts to 0.4 %. \n",
      " Word Normalization The normalization of non-standard words was evaluated for acronyms, for number, time, date, year, and money expressions, as well as for hybrid word forms like e.g. letter-digit combinations. \n",
      " The [word error rate] for non-standard words adds up to 28.9 %. \n",
      "\n",
      " [Part-of-Speech Tagging] \n",
      "\n",
      " The [evaluation data] comprises 10000 words extracted randomly from 100000 running words. \n",
      " Tagset Mapping Different tagsets were used for training and evaluation. \n",
      " Evaluation was carried out using the UK TC-STAR Grammatical POS tagset, but since no appropriate training material was available we worked with the standard PENN tagset (Marcus et al., 1995). \n",
      " The problem to map from our tagset to the one of TC-STAR was not solely solvable by simple table lookup but was also connected to disambiguation of adjectives and ordinal numbers, of prepositions and subordinating conjunctions, and of auxiliaries and full verbs. \n",
      " Disambiguation was carried out by local grammars. \n",
      " Note that disambiguation was not possible in some cases. \n",
      " Results After POS mapping and removal of further systematic tagset differences the [word error rate] amounts 6.5 %. \n",
      " Since more tagset inconsistencies are likely, this result has to be taken preliminarily. \n",
      "\n",
      " [Grapheme-to-Phoneme Conversion] \n",
      "\n",
      " Evaluation was carried out for common words (3808 types), geographic locations (1870 types), and English proper names (2237 types). \n",
      " Due to different treatment of syllabic consonants (marked by “ = ” by ELDA) we recalculated the error rates after having marked the syllabic consonants from our [G2P output] accordingly, which is allowed due to the redundancy of this marking. \n",
      " The overall results including [syllable segmentation] and word stress placement can be found in Table 2. \n",
      "\n",
      " Table : Error rates for the text processing tasks ; sentence error rate for sentence segmentation, [word error rate] otherwise. \n",
      "\n",
      " Discussion \n",
      "\n",
      " Our submodules for [TTS text preprocessing] presented here are partly data-driven as for [POS tagging], [syllable segmentation], and [grapheme-to-phoneme conversion] and partly [rule-based] as for [text normalization]. \n",
      " For word stress assignment we have chosen a [hybrid approach] using a [statistical classifier] fed by features partially derived by a rulebased morphologic analysis. \n",
      " In order to improve the modules’ adaptabilities to [other languages] the amount of needed [linguistic knowledge] should be reduced. \n",
      " Concerning [morphology] we intend to adopt the automatic induction method used to derive word representations for [POS tagging] for a complete [morphological analysis]. \n",
      " Furthermore it is to investigate if this morphologic analysis could be helpful not only for word stress assignment but also for [G2P conversion], for which – being provided with morphological information – an improvement had already been shown for German (Reichel and Schiel, 2005). \n",
      " Special effort is to be invested in the conversion of geographic location and proper names, for which the results are far away from satisfying. \n",
      " Due to the tagset inconsistencies, the [POS tagging results] should be regarded rather as preliminary and recalculated given a unique tagset used for both training and testing. \n",
      " For [G2P conversion] it should also be tested if training and test material are created following the same conventions, which is not clear per se due to their different origins. \n",
      "\n"
=======
      "[TUNDRA] : A [Multilingual Corpus] of Found Data for [TTS Research] Created with [Light Supervision] \n",
      "\n",
      " Abstract        \n",
      " [Simple4All Tundra] (version 1.0) is the first release of a standardised [multilingual corpus] designed for [text-to-speech research] with imperfect or found data. \n",
      " The corpus consists of approximately 60 hours of [speech data] from audiobooks in 14 languages, as well as utterance-level alignments obtained with a lightly-supervised process. \n",
      " Future versions of the corpus will include finer-grained alignment and prosodic annotation, all of which will be made freely available. \n",
      " This paper gives a general outline of the data collected so far, as well as a detailed description of how this has been done, emphasizing the [minimal language-specific knowledge] and manual intervention used to compile the corpus. \n",
      " To demonstrate its potential use, [text-to-speech systems] have been built for all languages using unsupervised or [lightly supervised methods], also briefly presented in the paper. \n",
      " Index Terms : [multilingual corpus], [light supervision], [imperfect data], found data, [text-to-speech], [audiobook data] \n",
      "\n",
      " Introduction \n",
      " Building a [text-to-speech (TTS) conversion] system for a [new language] has in the past been an expensive and time-consuming activity. \n",
      " Using data-driven methods to build, for example, a [statistical parametric waveform] generation module or [TTS back-end], can alleviate to some extent the lack of expert [linguistic knowledge]. \n",
      " Even then, however, a recording script must be prepared, a [voice talent] recruited and high-quality [speech recording] carefully supervised. \n",
      " Also problematic is the text-processing component of the system, i.e. the [TTS front-end], if none is available for the [target language]. \n",
      " A [front-end] is made up of [rule-based] or statistical modules ; acquiring the expert knowledge required either to manually specify those rules, or to annotate a learning sample on which to train the [statistical models], represents a major obstacle to creating a [TTS system] for a new [target language] and requires highly specialised knowledge. \n",
      " Such non-trivial tasks include, for example, specifying a [phoneme]-set or part of [speech] (POS) tag-set for a language where one has not already been defined ; annotating plain text with POS tags, as required to train a [POS tagger] and annotating the surface forms of words with [phonemes] to build a pronunciation lexicon. \n",
      " One of the primary goals of the project Simple4All1 is to produce freely available tools for building [TTS systems] with little or no expert supervision from freely available existing data. \n",
      " These tools enable us to sidestep the expense associated with engineering a [speech corpus] in each new [target language] from scratch, in the case where data is not readily available. \n",
      " Our [toolkit] includes modules for handling imperfect recording conditions, segmenting [audio] into manageable chunks, and aligning those chunks with a chapter or book-level text transcription. \n",
      " We here explain how these tools have been applied to existing [audiobook data] in 14 languages, most of it freely available, to create a [multilingual corpus] with minimal manual intervention and [language-specific expert knowledge]. \n",
      " The result of this processing is a standardised [multilingual database] of ‘ found’ data, which we release under the name [Tundra]. \n",
      " There has been much recent interest in in using found data to produce [TTS systems], in particular, [speech data] from audiobook recordings. \n",
      " We note that the [Arctic databases] have provided a valuable resource for research into [TTS] using conventional purpose-recorded databases, in that they are freely available and serve as a common point of reference for benchmarking. \n",
      " In view of this significant and growing interest in building [TTS systems] from found data, we feel there is a need for a similarly standardised and freely-[available corpus] of found data. We present [Tundra] to the [TTS] researchommunity in the hope that it can start to fill that need. \n",
      " Our [toolkit] also includes modules for selecting a subset of utterances with a uniform speaking style, and constructing [TTS systems] from text and [speech data] without reliance on [language-specific expert knowledge] or on conventional linguistic resources such as lexicons, phonesets, [part-of-speech taggersetc]. \n",
      " In order to show that it is feasible to build voices on corpora built with such minimal expert supervision, we also present a demonstration of [TTS systems] that we have built by applying these tools to [Tundra]. \n",
      " We do not present detailed explanation, evaluation and analysis of these demo systems here due to space limitations, and refer interested readers to, where such details will be given. \n",
      " An initial public version of the [Simple4All tools] used to compile the corpus and build the [demo voices] is due to be released in November 2013. \n",
      "\n",
      " www.simple4all.org/    \n",
      "\n",
      " [Corpus Construction] \n",
      " In this section we describe the pipeline of [data processing] involved in building the [Tundra corpus], from [speech] denoisingand deverberation to [lightly supervised speech] and text alignment. \n",
      " All the steps presented in the following subsections are based solely on found [speech] and text resources and could be easily applied to any other resource, even by non-expert users. \n",
      " As regards [language dependency], the only step which requires familiarity with at least the script of the [target language] is the first step of matching 10 minutes of [speech] with an orthographic transcript. \n",
      " All the other processes can be performed by the users with little or no training in [speech processing] and without relying on any [target language] knowledge. \n",
      "\n",
      " [Speech] Pre-processing \n",
      " Conventional [TTS] corpora deliver [speech] recorded in noise-free non-reverberant environments, and thus lead to high-quality [synthetic speech]. \n",
      " Found data, on the other hand are usually recorded in sub-optimal conditions, and without professional recording equipment. \n",
      " Therefore, when building [TTS systems] on this type of data, some pre-processing steps are in order. \n",
      " For [Tundra], recordings which casual listening suggested were sub-optimal went through the following pre-processing steps, applied to each recording session individually,2 so that variations in between them can be normalised : \n",
      " Noise reduction : uses a multi-band noise gate removal with a 20dB noise reduction threshold, a frequency smoothing of 150 Hz and 0.15 second decay time. \n",
      " The noise profile was selected from the initial silence segments of each [speech file]. \n",
      " Normalisation : DC offset was removed, and the recordings were normalised to a maximum amplitude of -0.1 dB, so that the average energy level is the same across different recording sessions. \n",
      " Deverberation : was performed using a RMS based algorithm, with a smoothing of 40 ms and a release of 400ms. \n",
      "\n",
      " [Lightly-supervised Audio Segmentation] \n",
      " Current [parametric TTS systems] generally use [training data] which is segmented into sentence-length chunks, and rarely make use of contexts beyond the current sentence. \n",
      " The small length of the [training data] is also a limitation of the forced alignment algorithm while training. \n",
      " Although several algorithms have been proposed to enable the use of longer [speech] segments, we still consider that sentence-length utterances are the building blocks of [TTS], and longer segments can be easily obtained by concatenating the former, thus ensuring a paragraph or maybe chapter level analysis or training. \n",
      " presents a [lightly supervised method] for the segmentation of [speech] into sentences. The method uses a small amount of manually [labelled] data, in which the silence between sentences is marked for around 5 to 10 minutes of [speech]. \n",
      " Silence marking is a trivial task and requires no technical knowledge. \n",
      " Using the initial [training data], standard [Gaussian] mixture models (GMMs) with 16 components are trained for [speech] and silence respectively. \n",
      " The observation [vectors] consist of energy, 12 dimensional MFCCs, their [delta features], and the number of zero crossings in a frame. \n",
      " The distinction between [speech] and silence is made by calculating the [log likelihood] ratio (LLR) of each frame. \n",
      " The framewise LLR is smoothed using a moving median filter. \n",
      " While doing sentence level segmentation, an important aspect is to discriminate between within-sentence breaks, and sentence boundary breaks. \n",
      " Therefore, the trained GMMs likelihood scores are evaluated on the [training data], and the durations of the sentence boundary silence segments and the durations of within-sentence silence segments are computed. \n",
      " Two [Gaussian] PDFs are then fitted to the two model durations. \n",
      " The intersection point of the two PDFs is used as a duration threshold to classify silent segments as either sentence-internal or sentence boundary breaks. \n",
      " Results presented in showed that this method when applied to an English audiobook, successfully identified most of the sentence boundaries. \n",
      " We also evaluate it in this paper by comparing [speech]-based segmentation results against the text based ones. \n",
      "\n",
      " Audiobooks are usually distributed in chapter-size chunks which correspond to one recording session. \n",
      "\n",
      " [Lightly-supervised Speech] and Text Alignment \n",
      " In   we first introduced a method for the automatic alignment of [speech data] with unsynchronised, imperfect transcripts, for a domain where no initial [acoustic models] are available. \n",
      " As opposed to, where existing high-quality acoustic and [language models] are used, our method requires only relatively low-quality [grapheme]-based [acoustic models] trained solely on the [speech] resource to be aligned. \n",
      " To overcome the lack of good [acoustic models], the [ASR] decoding network is limited to a sequence of words derived from the approximate transcript, similar to. \n",
      " This sequence is called a skip network. \n",
      " The confidence of the alignment is ranked based on the acoustic scores obtained in the decoding process with different degrees of freedom included in the skip network. \n",
      " Manual intervention is limited to matching the first 10 minutes of [speech] with the correct text transcription, to provide data for training the initial [acoustic models], similar to. \n",
      " This feature makes the method easily applicable in any language employing an alphabetic writing system, and enables the use of found data without the hassle of manually transcribing its entirety. \n",
      " Initial results on the English audiobook A Tramp Abroad by Mark Twain3 showed an average 55 % [confident data], with a [WER] of 1 % and SER of 8 %. \n",
      " Since then, the [acoustic model training] has been extended to [tri-grapheme] and [lightly supervised] discriminative training, which led to an average of75 % [confident data] with similar word and sentence error rates. \n",
      " One major loss in sentence accuracy rates is due to utterance initial and final word deletions and insertions, which can not be correctly detected by the current confidence measure. However, previous studies showed that phone errors less than 1 % do not degrade the quality of the [synthetic speech]. \n",
      " The output of the alignment process is a set of segmented [speech files] with their corresponding orthographic transcripts, including punctuation, and also a time alignment of the segments within the initial [speech data]. \n",
      "\n",
      " The Corpus \n",
      " The procedures described above have been applied to a number of freely available found resources. \n",
      " Audiobooks were a first choice, as they are a readily available in [multiple languages] and are generally read by a [single speaker] and recorded with equipment of at least reasonable quality. \n",
      " Another advantage would be that by using cohesive and expressive [spoken data] as the basis for training a [TTS system] might yield more cohesive and expressive multi-utterance [TTS output], fact which explains the high interest in them lately. \n",
      " This latter advantage is not especially made use of in the [demo voices] presented here, but is the subject of on-going work for us elsewhere. \n",
      " To emphasise the utility of audiobooks in [TTS] systems, in Fig.   we present a comparison between standard [TTS] corpora and audiobooks with respect to logF0 in 4 [different languages]. \n",
      " The standard [TTS] corpora are : a subset of the database called ‘ Nina’ in, a subset of a corpus of Finnish [speech] recorded from a [female speaker] specifically for [TTS] purposes, SEV neutral [ 19 ] and RSS [ 20 ]. \n",
      "\n",
      " http://librivox.org/a-tramp-abroad-by-mark-twain/ \n",
      "\n",
      "\n",
      " Table : [Simple4All Tundra Corpus] overview \n",
      "\n",
      " Figure : logF0 comparison of conventional [TTS] corpora versus [audiobook data] in four languages : English (EN), Spanish (ES), Finnish (FI) and Romanian (RM). \n",
      " A denotes the [audiobook data], and S denotes the standard [TTS database]. \n",
      " The standard [corpora speaker] genders are the same as the selected audiobooks. \n",
      "\n",
      " Figure : logF0 boxplots for all languages. [Language codes] are given in Table \n",
      "\n",
      " It can be easily observed that the audiobooks have a greater standard deviation compared with conventional corpora, which means that they could easily provide a much richer prosodic context. \n",
      " This aspect can also be noticed from Fig. where logF0 distributions are plotted for all the languages of the corpus. \n",
      " As a result, [Tundra] 1.0 includes 14 audiobooks in 14 languages : Bulgarian, Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Polish, Portuguese, Romanian, Russian and Spanish. \n",
      " [Language selection] was based on the availability of both [speech and text data], as well as the language having an alphabetic writing system (in this case, Latin and Cyrillic alphabets). \n",
      " Important resources for these are the Librivox and Gutenberg4 projects, which are the sources for most of the data used to compile [Tundra]. \n",
      " The complete list [speech] and text sources can be found here http://tundra.simple4all.org/. \n",
      "\n",
      " http://librivox.org and http://gutenberg.org/ \n",
      "\n",
      " Table presents an overview of the [entire corpus], including title and author of the audiobook, [speaker gender] and total duration. \n",
      " There are 8 male and 6 [female speakers], and the aligned corpus amounts to approximately 60 hours of [speech]. \n",
      " For the final set of utterances included in this corpus, each audiobook underwent the steps described in the Section and which are schematically depicted in Fig. \n",
      " Audiobook chapters were converted from mp3 to wav format and then cleaned if the overall quality was considered low. \n",
      " The first 10 minutes of [speech] were then annotated with silence segments and manually transcribed. \n",
      " Manual transcription proved to be a trivial task, and based on the book text, the authors were able to perform it, although they do not speak most of the languages included in the corpus. \n",
      " For the Cyrillic writing [system languages] (i.e. Bulgarian and Russian), [native speakers] were asked to correct an initial transcription provided by the authors. \n",
      " Data was then segmented using the VAD algorithm, and the resulting number of [speech] utterances is presented in Table alongside the text-based segmentation. \n",
      "\n",
      " For example, the Spanish and [Romanian data] are professional recordings which did not require any pre-processing. \n",
      " We currently decide whether to pre-process recordings based on informal listening, but aim to automate this with an objective measure of [speech quality] in future versions of our [toolkit].               \n",
      "\n",
      " The difference between the number of VAD and text utterances results from the writing style of the book (i.e. mostly dialogue, or mostly descriptive) and the fact that in the alignment process, in order to obtain the [most data] from the audiobook, segmented utterances which are shorter than a specified threshold (5 seconds for these data) are concatenated. \n",
      " After the alignment process, an average of 68 % of the data were considered confident and included in the [final corpus]. \n",
      " Table   presents the duration of the aligned data and its percentage from the total duration. This percentage appears to be highly dependent on : \n",
      " a) the total amount of data available : see the low percentage of the Danish audiobook which has only 2.1 hours ; \n",
      " b) [speaker gender] : [female voices] seem to have a lower alignment percentage ; \n",
      " c) [grapheme-to-phoneme language complexity] : see English and French versus Italian and German ; \n",
      " and d) [speaker characteristics] : speaking rhythm, degree of expresivity, as well as [general voice] quality also affect the results. \n",
      " SER and [WER] values for the aligned audiobooks could not be exactly determined, as this would have required their full manual transcription, which is outside the scope of this [corpus building] procedure. \n",
      " However, one chapter from each audiobook in the languages spoken by the authors was evaluated, and the errors tend to be similar to those in, meaning a less than 1 % [WER] and a 8 % SER. \n",
      " Higher error rates were reported for the noisier [speech data] (see Table for general signal-to-noise ratios). \n",
      " To be useful as a standardised [TTS corpus], [Tundra] is also partitioned into training and test sets. \n",
      " To ensure a satisfactory amount of testing data even for the shortest audiobook, the [test data] were selected from the final chapters / parts of the audiobooks, so that they amount to at least 10 % of the aligned duration of it. \n",
      " The entire segmented and aligned corpus, along with the chapter-wise time alignment and training / test set division of can be downloaded from http://tundra.simple4all.org \n",
      "\n",
      " Spanish and Romanian also have very simple [G2P] rules, but the speakers’ greater expressivity limits the alignner’s performance. \n",
      " This being a subjective measure, we encourage readers to listen to samples of the audiobooks. \n",
      "         \n",
      " Figure : Outline of [corpus construction] and [voice building] \n",
      "\n",
      " Demo \n",
      " To show the feasibility of using a corpus that has been compiled with such minimal intervention and [language-specific expertise], we have used it to build demo [TTS voices] in the corpuslanguages. \n",
      " To build these voices we first select a subset of utterances spoken in a homogenous style using a slightly supervised active learning-based approach. \n",
      " We then employ a [toolkit] which has been specifically designed to construct [TTS front-ends] while making as few implicit assumptions about the [target language] as possible, and to be configurable with minimal effort and expert knowledge to suit arbitrary new [target languages]. \n",
      " The modules of our [toolkit] therefore rely where possible on resources which are intended to be universal. \n",
      " For example, to tokenise input text we rely on character properties given in the Unicode [character database] – a regular expression defined over these properties has so far produced sensible tokenisations in a variety of alphabetic (Latin-based, Cyrillic) and alphasyllabic (Brahmic) scripts.) \n",
      " A [letter-based approach] is used, in which the names of letters are used directly as the names of [speech] modelling units (in place of the [phonemes] of a conventional [front-end]). \n",
      " This has given good results for languages with transparent alphabetic orthographies such as Romanian, Spanish and Finnish, and can give acceptable results even for languages with less transparent orthographies, such as English. \n",
      " Furthermore, our tools make no use of expert-specified categories of letter and word, such as [phonetic] categories (vowel, nasal, approximant, etc.) and part of [speech] categories (noun, verb, adjective, etc.). \n",
      " Instead, we use features that are designed to stand in for such expert knowledge but which are derived fully automatically from the distributional analysis of plain text in the [target language]. \n",
      " Samples of the voices can be heard at http://tundra.simple4all.org/demo/. \n",
      " For reasons of space we refer readers interested in full presentation and evaluation of thesesystems to. \n",
      "\n",
      " Conclusion \n",
      " We have introduced a first version of the [Simple4All Tundra corpus], and described its construction from readily available [speech data]. \n",
      " 14 audiobooks in 14 languages have been so far included in the corpus along with their orthographic transcripts. \n",
      " [Tundra] will be extended in the future with other types of imperfect, found data, such as lectures, or parliamentary [speech], data which have a higher degree of spontaneity and expressivity. \n",
      " We will also aim at making available finer-grained alignments of the data, and also more elaborate prosodic annotations, such as style diarisation, emphasis or sentiment analysis. \n",
      " The [TTS systems] built from this corpus demonstrate a first application of   the [Tundra corpus], and support its usefulness. \n",
      "\n",
      " Acknowledgements \n",
      " The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement No 287678. \n",
      " The research presented here has made use of the resources provided by the Edinburgh Compute and [Data Facility] (ECDF : http://www.ecdf.ed.ac.uk). \n",
      " The ECDF is partially supported by the eDIKT initiative (http://www.edikt.org.uk). \n",
      " We would like to thank Mihai Nae from Cartea Sonora for releasing the [Romanian data], as well as to all the volunteers at Librivox and Gutenberg for dedicating their time to distribute this wide variety of data.\n"
>>>>>>> e7ad802de0ec54bed7ecc6b6ee399fd1a292051d
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
<<<<<<< HEAD
    "    init_data = read_data('tts-lexicon4_3_cecile.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/20.txt')\n",
=======
    "    init_data = read_data('lexicon.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/1.txt')\n",
>>>>>>> e7ad802de0ec54bed7ecc6b6ee399fd1a292051d
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    text_dataframe.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    annotate(data, text_dataframe)\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
