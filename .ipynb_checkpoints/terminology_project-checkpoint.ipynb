{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: Cécile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "#     print(len(original))\n",
    "#     print(len(lemma))\n",
    "#     print(len(t))\n",
    "#     print(len(pos))\n",
    "#     print(len(new_pos))\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_adj = ['multilingual', 'autoregressive', 'monolingual', 'supervised', 'unsupervised', 'acoustic', 'phonetic', 'cross-lingual',\n",
    "           'intelligible', 'unlabelled', 'labelled', 'accented', 'bilingual', 'training', 'generated', 'fluent',\n",
    "           'neural', 'artificial', 'bidirectional', 'gated', 'attentional', 'substantial']\n",
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['lemma'][i+2] == 'by' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+5] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+6] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+6] = text_dataframe['tokens'][i+6]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+5] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '[' + text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i + 4] = text_dataframe['tokens'][i + 4] + ']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A' or text_dataframe['pattern'][i + 3] == 'V') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'voice' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i != 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['accent', 'accuracy', 'activation', 'adaptation', 'algorithm', 'alignment', 'approach', \n",
    "          'architecture', 'attribute', 'boundary', 'cell', 'class', 'classifier', 'cluster', 'component', \n",
    "          'concatenation', 'content', 'contour', 'control', 'conversion', 'coverage', 'detection', \n",
    "          'detection', 'device', 'dictionary', 'embedding', 'encoding', 'engineering', 'entry', 'error', \n",
    "          'evaluation', 'experiment', 'expertise', 'file', 'filter', 'form', 'framework', 'function', \n",
    "          'generation', 'implementation', 'improvement', 'inference', 'input', 'kernel', 'layer', 'learning', \n",
    "          'length', 'location', 'mapping', 'method', 'model', 'module', 'naturalness', 'network', \n",
    "          'nonlinearity', 'optimization', 'output', 'pair', 'parameter', 'pipeline', 'posterior', 'prediction', \n",
    "          'process', 'processing', 'quality', 'realization', 'recognition', 'representation', 'research', \n",
    "          'result', 'sample', 'score', 'sequence', 'set', 'setting', 'signal', 'string', 'study', 'symbol', \n",
    "          'synthesis', 'synthesizer', 'system', 'task', 'technique', 'technique', 'technology', 'token', 'tool', \n",
    "          'toolkit', 'training', 'transcription', 'transfer', 'transform', 'translation', 'value']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i != 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' ’', '’')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing for [Speech Synthesis] \n",
      "\n",
      " Abstract \n",
      "\n",
      " In this paper we describe our text preprocessing modules for English [text-to-speech synthesis]. \n",
      " These modules comprise [rule-based text normalization] subsuming sentence segmentation and normalization of non-standard words, statistical [part-of-speech tagging], and statistical [syllabification], [grapheme-to-phoneme conversion], and word stress assignment relying in parts on [rule-based morphological analysis]. \n",
      "\n",
      " Introduction \n",
      "\n",
      " Text preprocessing for English [text-to-speech (TTS) synthesis] in general consists of the following steps : [Text Normalization]. \n",
      " This first step subsumes sentence segmentation, tokenizing, and normalization of nonstandard words. \n",
      " For sentence segmentation the main problem is the ambiguity of the period, marking sentence boundaries or abbreviations, sometimes even simultaneously (it is 5 p.m.). \n",
      " For period disambiguation an identification of abbreviations is needed as well as a disambiguation of capitalized words (proper names vs.sentence initial words, thus words following a sentence boundary period). \n",
      " Complications arise from abbreviations that do not differ from ordinary sentence final words (no. also being an abbreviation of number) and from the fact that also proper names can occur in sentence initial position. \n",
      " [Rule-based systems] for heuristic period disambiguation operate on local grammars containing abstract contexts for within-sentence periods and sentence boundaries (Cherry and Vesterman, 1991 ; Aberdeen et al., 1995). \n",
      " Mikheev’s (2002) [rule-based segmentation] is preceded by capitalized word disambiguation. \n",
      " [Machine learning approaches] as the [decision tree classifier] in Riley (1989) use [context features] such as word lengths, capitalization, and word occurrence probabilities on both sides of the period in question. \n",
      " Current systems achieve an error rate down to less than 1 %. \n",
      " Tokenizing in its simplest form is achieved by splitting the text at white spaces and at punctuation marks, that do not belong to abbreviations identified in the preceding step. \n",
      " Non-standard words are tokens to be expanded to an appropriate orthographic form before [grapheme-to-phoneme conversion]. \n",
      " Their normalization includes amongst others number conversion, homograph disambiguation (Henry, Scene, Mr), expansion of abbreviations and symbols, and appropriate treatment of acronyms (some have to be spelled, others not), and email and URL addresses. \n",
      " A token might be split into several words by these operations. \n",
      " Normalization is a difficult task, since creation of the nonstandard word types mentioned above is arbitrarily productive and therefore not to be solved solely by table lookup. \n",
      " Furthermore [phonetic realization] is highly context dependent, examples are the homographs above or digit strings which can be realized either as numbers, phone numbers or years. \n",
      " While most of the normalization systems tackle this problem by heuristic disambiguation and expansion rules, e.g. Black et al. (1999), there are also some [language modeling] and [machine learning approaches] for normalization subtasks. \n",
      " For example in Sproat et al. (2001) word normalization is amongst others formulated in terms of maximizing the conditional probability of a normalized word sequence given an observed token sequence. \n",
      "\n",
      " [Part-of-Speech Tagging] \n",
      "\n",
      " [Part-of-Speech tagging] means word class assignment to each token. \n",
      " Its input is given by the tokenized text. \n",
      " Taggers have to cope with unknown words (OOV problem) and ambiguous word-tag mappings. \n",
      " [Rule-based approaches] like ENGTWOL (Voutilainen, 1995) operate on a) dictionaries containing word forms together with the associated POS [labels] and morphologic and [syntactical features] and b) context sensitive rules to choose the appropriate [labels] during application. \n",
      " In [statistical approaches] (Jelinek, 1985) generally the most probable tag sequence given the observed word sequence is estimated. \n",
      " In [transformation-based tagging] (Brill, 1995) a [hybrid approach] can be found, where disambiguation rules are derived by statistical means. \n",
      " [Grapheme-to-Phoneme Conversion] Since hand-crafted rule generation for [language processing] is very timeconsuming and corresponding systems are highly language dependent, most of the current [G2P systems] are purely data-driven (see e.g. Yvon (1994) for an overview over some machine learning approaches to [G2P conversion]). \n",
      " Accounting for the influence of [morphology] and syllable structure can improve performance of [G2P conversion] (Reichel and Schiel, 2005). \n",
      " Word Stress The assignment of English word stress relies on [phonological], morphological, and word [class features]. \n",
      " The crucial [phonological feature] is syllable weight : heavy syllables rather attract stress than weak ones. \n",
      " Amongst the [morphological features] are affix types (stressed vs. unstressed vs. pre-stressed) and the position within a compound. \n",
      " Word class and word syllable length determine default stress patterns. \n",
      " Metrical [phonology] (Liberman and Prince, 1977) accounts for lots of these factors and is a useful framework for [rule-based approaches] to word stress assignment. \n",
      " Among the data-driven approaches are [neural networks] (Gupta and Touretzky, 1994) predicting stress patterns given syllable weight patterns and instance-based learning (Daelemans and van den Bosch, 1997) which matches new words against words with an already known stress pattern. \n",
      " In the following sections our [TTS text preprocessing modules] are presented. \n",
      "\n",
      " [Text Normalization] \n",
      "\n",
      " Identification of Proper Names, Acronyms and Abbreviations \n",
      "\n",
      " Since retrieval of proper names, acronyms and abbreviations is crucial for appropriate sentence segmentation and normalization of non-standard words, this task is carried out prior to [text normalization]. \n",
      " Due to the high productivity of these word classes simple table lookup is insufficient and has to be augmented by following procedures. \n",
      " Proper names All those tokens are considered as proper names that occur only and at least twice in capitalized form. \n",
      " Only occurrences in unambiguous environments are counted, that means not behind a period except for periods of prepositional titles like Mr, Dr, etc. \n",
      " Abbreviations Token t is identified as an abbreviation, if 1) it has not been classified as a proper name and 2) it ends with a period and 3) one of the following conditions is fulfilled : \n",
      " • t contains another period (e.g.), or \n",
      " • the string of t preceding the period consists of just one small letter, or \n",
      " • t contains no vowel (exception qu.) and at least one small letter (vs. acronyms, numbers), or \n",
      " • the letter sequence of t indicates a violation of [phonotactics] (see below). \n",
      " Acronyms Token t is identified as an acronym, if it has not been classified as an proper name or abbreviation and has not been classified as a roman number (using local grammars) and if one of the following conditions holds : \n",
      " • t consists entirely of consonants, or \n",
      " • t consists entirely of capitals (except I), or \n",
      " • t is preceded by the article an and does not start with a vowel, or \n",
      " • t is preceded by the article a and starts with a vowel (except u), or \n",
      " • the letter sequence of t indicates a violation of [phonotactics]. \n",
      " Violation of [Phonotactics] The [phonotactics] exploited here is related to the sonority-based syllable definition according to which a syllable is characterized by a sonority peak facultatively preceded by a rise and followed by a decline of sonority (in case of presence of head and coda, respectively). \n",
      " A letter sequence of a token indicates a violation of [phonotactics] if 1) the first (resp. last) letter can be associated with a [phoneme] of higher sonority than that of a fricative (which can occur as a syllable appendix), and 2) the sonority of that [phoneme] is higher than the [phoneme] associated with the following (resp. preceding) letter, and 3) none of the two letters in focus can be associated with a syllable nucleus. \n",
      " For word beginnings vowel letters are associated with syllable nuclei, for word endings also < m >, < n > ; < l > is not treated as a nucleus associate since syllabic /l/ is represented by le in English orthography. \n",
      " Examples : incl. is identified as an abbreviation while wrists. and fascism. are treated as standard words followed by a period. \n",
      "\n",
      " Sentence Segmentation \n",
      "\n",
      " The hand-crafted binary [decision tree] in Figure 1 guides the decision whether or not token t i is followed by a sentence boundary. \n",
      " i is ranging over the tokens of the present tokenization of the text at white spaces and unambiguous punctuation. \n",
      " The sentence segmentation completes the tokenization process. \n",
      "\n",
      " Normalization of non-standard words \n",
      "\n",
      " For space reasons we present just a selection of our normalization procedures here. \n",
      " Numbers In general the following number transformations are carried out : roman numbers are converted to arabic numbers by calculation and arabic numbers are converted to letters by finite state [transducers] for cardinal and ordinal numbers. \n",
      " The identification of roman numbers and the distinction of cardinals and ordinals is guided by local grammars. \n",
      " Cardinal numbers are disambiguated whether to be pronounced as one number, as a date, or digit by digit through pattern matching and examination of the text environment regarding e.g. date-related or phone number cues. \n",
      " Dates are further completed by prepositions and articles accordingly. \n",
      " E.g. 12 Feb becomes on the twelve of February, but on being omitted if a preposition is already given. \n",
      " Abbreviations and Acronyms Unknown abbreviations are spelled. \n",
      " Unknown acronyms are spelled if indicated by a preceding indefinite article or by violation of [phonotactics] (incl. lack of vowels ; see above). \n",
      " Otherwise they are pronounced as standard words. \n",
      " This acronym examination also takes place for each part of a hyphenated compound (CD-Rom) and within URLs and email addresses. \n",
      "\n",
      " [Part-of-Speech Tagging] \n",
      "\n",
      " Our approach for [POS tagging] described in more detail in Reichel (2005) is statistical and can be seen as a generalization of the classical [Markov tagger] presented by Jelinek (1985). \n",
      " The P (w|t) emission probabilities of word w given tag t are replaced by a linear interpolation of tag emission probabilities given a list of representations of w, that are connected to automatically derived word suffixes. \n",
      " Since in [English language] suffixes also store word class information and are observed in the [training data] with a high probability, the OOV problem can be reduced this way. \n",
      " However, no [linguistic knowledge] is needed, hence our approach is [language independent]. \n",
      "\n",
      " Basic Form of a [Markov POS Tagger] \n",
      "\n",
      " The aim is to estimate the probable tag sequence T̂ given word sequence W : \n",
      " To estimate P (T |W) first a reformulation is needed by applying Bayes Formula, which leads to : \n",
      " given that the denominator P (W) is constant. \n",
      " Further two simplifying assumptions are to be made to get reliable \n",
      " counts for the probability estimations : \n",
      " • Probability of word w i depends only on its tag t i. \n",
      " • Probability of tag t i depends only on a limited tag history. \n",
      " The resulting formula is thus : \n",
      " T̂ = arg max t 1... t n n h Y i=1 P (t i |t-history i) P (w i |t i) i (3) \n",
      " T̂ is retrieved using the [Viterbi algorithm] ([Viterbi], 1967). \n",
      "\n",
      " Generalizations of the basic model \n",
      "\n",
      " First P (t i |t-history i) is replaced by a linearly interpolated [trigram model] \n",
      " Xju j P (t i |t-history ij), j ranging from unigram to trigram tag history. Further w i is replaced by a list of word representations leading to a reformulation of P (w i |t i): \n",
      " P (w i) X v k P (t i |w-representation ik) \n",
      " applying again Bayes Formula and linear interpolation. \n",
      " Our model is thus given by : \n",
      "\n",
      " The interpolation weights u j and v k are calculated via the EM algorithm (Dempster et al., 1977). \n",
      " In order to reduce calculation effort in application, just for unknown words the probabilities are calculated for all POS tags. \n",
      " For known words just the POS tags co-occurring with them in the [training corpus] are taken into consideration. \n",
      " Our [training data] comprises 620000 tokens (including punctuation) taken from prose of the 20th century and pretagged by the TnT tagger (Brants, 2000) using the Penn tag set (Marcus et al., 1995). \n",
      "\n",
      " Word representations \n",
      "\n",
      " The representation of words seen in the [training data] is simply the word form. \n",
      " For OOV cases the representation is given by two string suffixes which are determined by Normalized Backward Successor Variety (NBSV). \n",
      " The Successor Variety (SV) of a string is defined as the number of different characters that follow the string in a given lexicon. \n",
      " This concept is adopted from [stemming] procedures like the Peak and Plateau algorithm of Nascimento and da Cunha (1998). \n",
      " Backward SV means that the SVs are calculated from reversed strings in order to increase the probability to separate linguistically meaningful suffixes. \n",
      " In our approach the SVs are weighted with respect to the mean SV at the corresponding string position to eliminate positional effects. \n",
      " The mean SV is highest in the beginning and declines continuously while moving forward in the word string. \n",
      " The lexicon of reversed words is represented in the form of a trie (cf. Figure 2), in which the SV at a given state is the number of all outgoing transitions. \n",
      " NBSV peaks are treated as [morpheme boundaries]. \n",
      " Since this method is knowledge free, of course not all of the obtained segments necessarily correspond to linguistic meaningful entities as might be suggested by Figure 2. \n",
      "\n",
      " [Grapheme-to-Phoneme Conversion] \n",
      "\n",
      " Our [G2P approach] is data-driven ; as a classifier we use the C4.5 [decision tree] (Quinlan, 1993). \n",
      " We treat the conversion as a one-to-one mapping from the set of [graphemes] to the set of [phonemes] (UK [SAMPA]). \n",
      " To cope with any n-to-n relation the [phoneme set] also comprises the empty [phoneme] as well as [phoneme clusters]. \n",
      " A canonical [pronunciation dictionary] containing 61340 entries is used for training and lookup at application time. \n",
      "\n",
      " Alignment \n",
      "\n",
      " The first step for creating the [grapheme-to-phoneme converter] was to align the [phoneme string] and the orthographic string of each [pronunciation dictionary entry]. \n",
      " Inspired by the work of Daelemans and van den Bosch (1997) an initial co-occurrence matrix between letters and [phonemes] was estimated. \n",
      " This was done by diagonally aligning the letters and [phonemes] of each entry (see Figure 3). \n",
      " For each [phoneme] a triangular window with an area of 1 and a width of 5 letters was centered at the diagonal in order to spread the probability of co-occurence to adjacent letters. \n",
      " The values of the initial co-occurrence matrix are converted into probabilities and used in a [Dynamic Programming algorithm] to find the most likely alignment for each [pronunciation dictionary entry]. \n",
      " The [DP algorithm] is designed to align either the empty [phoneme], ore one [phoneme], or a [phoneme cluster] to each letter. \n",
      " In order to get a left-aligned [phoneme string] which is necessary for its alignment with morphologic segments (see below), heuristic [post-processing] was applied. \n",
      "\n",
      " [Syllable Segmentation] \n",
      "\n",
      " Since syllable structure influences [G2P conversion] and is furthermore needed for word stress assignment (see below), [syllable segmentation] is carried out in advance. \n",
      " Also for [syllable segmentation] a C4.5 [decision tree] is trained deciding for each letter whether or not a syllable boundary follows. \n",
      " The current letter as well as the surrounding letters within a window of length 9 are used as features. \n",
      " For model development the same dictionary is used as above, 80 % of the entries taken for training and 20 % for testing. \n",
      " The resulting [decision tree] yields a letter error rate of 1.2 % and a [word error rate] of 8.6 % on the [test data]. \n",
      "\n",
      " Features \n",
      "\n",
      " To map a [grapheme] g on the corresponding [phoneme], the [decision tree] is supplied with 24 features : \n",
      " • [graphemes] within a window of length 9 centered at g \n",
      " • information whether or not a syllable boundary follows for each [grapheme] within that window \n",
      " • position of g within the current syllable (head, nucleus, coda) \n",
      " • type of the current syllable (onset / null onset, open / closed) \n",
      " • relative position of g within the word • [phoneme] history of length 3. \n",
      "\n",
      " Word stress assignment \n",
      "\n",
      " In our approach word stress is assigned again by a C4.5 [decision tree] deciding for each syllable whether or not being stressed. \n",
      " Since English word stress is governed by [phonology], [morphology], and word class (see above) the classifier should be provided by features of all three domains. \n",
      " The [phonological features] are derived from [syllabification] and [G2P conversion], word [class features] from [POS tagging]. \n",
      " To obtain [morphologic features] some morphologic analysis has to be carried out. \n",
      "\n",
      " Morphologic segmentation \n",
      "\n",
      " The segmentation algorithm we used here is a simplified version of the procedure presented in Reichel and Weilhammer (2004). \n",
      " It consists of two stages : lexicon construction and segmentation. \n",
      " Since it requires some knowledge about affixation it is applicable for [different languages] just in combination with language dependent stemmers and affix lexica. \n",
      "\n",
      " Lexicon construction \n",
      "\n",
      " The lexicon initially comprises English prefixes and suffixes and the linking [morpheme] ‘ -’. \n",
      " It is then augmented by [stems] and [prefix-stem concatenations] of nouns, verbs, adjectives, and adverbs resulting from the application of a slightly modified [Porter stemmer] (Porter, 1980) for suffix separation. \n",
      " Table 1 shows the [morpheme classes] of the lexicon entries. \n",
      "\n",
      " Table : [Morpheme classes]. w : word left unchanged by the [Porter stemmer]. \n",
      "\n",
      " Segmentation \n",
      "\n",
      " Each word w is [stemmed] by the [Porter stemmer]. \n",
      " Then the [stem] and the suffix string are further segmented by the function segmentation (see Figure 4) the following way : \n",
      " Each input s is recursively divided into string prefixes and suffixes from left to right until a permitted segmentation is achieved or until the end of s is reached. \n",
      " In the course of the recursion, a boundary dividing the current string into prefix and suffix is accepted if 1) the prefix is found in the lexicon, 2) there exists a permitted segmentation for the suffix or (if not) the suffix is found in the lexicon, and just for [stem] segmentation, 3) the sequence ‘ prefix class + class of first suffix segment’ is not in conflict with simplified English morphotactics as represented by the automaton in Figure 5. \n",
      " On a random test sample of 1000 word types our system yields a word accuracy of 79.6 % for completely correct morphologic analysis. \n",
      " Future improvements can be achieved by modifying the [Porter stemmer] in order to cope with short ly-adverbs and comparative adjectives. \n",
      "\n",
      " Features \n",
      "\n",
      " For each syllable s the following features are used for word stress assignment : \n",
      " • word class \n",
      " • [syllable features] \n",
      " – syllable weight (reduced, light, heavy) \n",
      " – syllable type (onset / null onset, open / closed) \n",
      " – word syllable length \n",
      " • [morphologic features] (and features derived from morphologic segmentation) \n",
      " – class of the [morpheme] containing the nucleus of s (cf. Table 1). \n",
      " Prefixes and suffixes are further divided into stressed and unstressed affixes (suffixes : also pre-stressed). \n",
      " – index of current compound part \n",
      " – absolute and relative position of s within whole word and respective compound part \n",
      " – only stressable syllable (binary ; nucleus in stressed affix or in only stressable [morpheme]) \n",
      " Syllable weight is extracted within a 5 syllable window centered on s, [morpheme class] within a 3 [morpheme] window centered on the [morpheme] containing the nucleus of s. \n",
      "\n",
      " Results \n",
      "\n",
      " [Evaluation data] taken from the “ European Parliament Plenary Session ” (EPPS) corpus was provided by ELDA. \n",
      " ELDA also carried out the evaluations, but due to some convention differences (see below) we had to revise the results. \n",
      "\n",
      " [Text Normalization] \n",
      "\n",
      " Sentence Segmentation [End-of]-sentence detection was evaluated for 500 sentences. \n",
      " Given two errors the error rate amounts to 0.4 %. \n",
      " Word Normalization The normalization of non-standard words was evaluated for acronyms, for number, time, date, year, and money expressions, as well as for hybrid word forms like e.g. letter-digit combinations. \n",
      " The [word error rate] for non-standard words adds up to 28.9 %. \n",
      "\n",
      " [Part-of-Speech Tagging] \n",
      "\n",
      " The [evaluation data] comprises 10000 words extracted randomly from 100000 running words. \n",
      " Tagset Mapping Different tagsets were used for training and evaluation. \n",
      " Evaluation was carried out using the UK TC-STAR Grammatical POS tagset, but since no appropriate training material was available we worked with the standard PENN tagset (Marcus et al., 1995). \n",
      " The problem to map from our tagset to the one of TC-STAR was not solely solvable by simple table lookup but was also connected to disambiguation of adjectives and ordinal numbers, of prepositions and subordinating conjunctions, and of auxiliaries and full verbs. \n",
      " Disambiguation was carried out by local grammars. \n",
      " Note that disambiguation was not possible in some cases. \n",
      " Results After POS mapping and removal of further systematic tagset differences the [word error rate] amounts 6.5 %. \n",
      " Since more tagset inconsistencies are likely, this result has to be taken preliminarily. \n",
      "\n",
      " [Grapheme-to-Phoneme Conversion] \n",
      "\n",
      " Evaluation was carried out for common words (3808 types), geographic locations (1870 types), and English proper names (2237 types). \n",
      " Due to different treatment of syllabic consonants (marked by “ = ” by ELDA) we recalculated the error rates after having marked the syllabic consonants from our [G2P output] accordingly, which is allowed due to the redundancy of this marking. \n",
      " The overall results including [syllable segmentation] and word stress placement can be found in Table 2. \n",
      "\n",
      " Table : Error rates for the text processing tasks ; sentence error rate for sentence segmentation, [word error rate] otherwise. \n",
      "\n",
      " Discussion \n",
      "\n",
      " Our submodules for [TTS text preprocessing] presented here are partly data-driven as for [POS tagging], [syllable segmentation], and [grapheme-to-phoneme conversion] and partly [rule-based] as for [text normalization]. \n",
      " For word stress assignment we have chosen a [hybrid approach] using a [statistical classifier] fed by features partially derived by a rulebased morphologic analysis. \n",
      " In order to improve the modules’ adaptabilities to [other languages] the amount of needed [linguistic knowledge] should be reduced. \n",
      " Concerning [morphology] we intend to adopt the automatic induction method used to derive word representations for [POS tagging] for a complete [morphological analysis]. \n",
      " Furthermore it is to investigate if this morphologic analysis could be helpful not only for word stress assignment but also for [G2P conversion], for which – being provided with morphological information – an improvement had already been shown for German (Reichel and Schiel, 2005). \n",
      " Special effort is to be invested in the conversion of geographic location and proper names, for which the results are far away from satisfying. \n",
      " Due to the tagset inconsistencies, the [POS tagging results] should be regarded rather as preliminary and recalculated given a unique tagset used for both training and testing. \n",
      " For [G2P conversion] it should also be tested if training and test material are created following the same conventions, which is not clear per se due to their different origins. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('tts-lexicon4_3_cecile.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/20.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    text_dataframe.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    annotate(data, text_dataframe)\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
