Non-[Autoregressive PROPN (B) Neural PROPN (I) Text NOUN - to ADP - Speech NOUN (I)] 


Abstract PROPN (O) 
 In ADP (O) this DET (O) work NOUN , (O) we PRON (O) propose NOUN (O) [ParaNet NOUN (B)] , a DET (O) [non ADJ - autoregressive ADJ (B) seq2seq NOUN (I) model NOUN (I)] that DET (O) converts NOUN (O) text NOUN (O) to ADP (O) [spectrogram NOUN (B)] . 
 It PRON (O) is AUX (O) fully ADV (O) convolutional ADJ (O) and CCONJ (O) brings VERB (O) 46.7 NUM (O) times NOUN (O) speed NOUN - up NOUN (O) over ADP (O) the DET (O) lightweight NOUN (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM (O) at ADP (O) synthesis NOUN , (O) while SCONJ (O) obtaining VERB (O) reasonably ADV (O) good NOUN (O) [speech NOUN (B) quality NOUN (I)] . 
 [ParaNet PROPN (B)] also ADV (O) produces VERB (O) stable ADJ (O) alignment NOUN (O) between ADP (O) text NOUN (O) and CCONJ (O) [speech NOUN (B)] on ADP (O) the DET (O) challenging NOUN (O) test NOUN (O) sentences NOUN (O) by ADP (O) iteratively ADV (O) improving VERB (O) the DET (O) attention NOUN (O) in ADP (O) a NOUN (O) layer NOUN - by ADP - layer NOUN (O) manner NOUN . (O) 
 Furthermore ADV , (O) we PRON (O) build NOUN (O) the DET (O) parallel NOUN (O) [text NOUN - to ADP - speech NOUN (B) system NOUN (I)] and CCONJ (O) test NOUN (O) various ADJ (O) [parallel NOUN (B) neural NOUN (I) vocoders NOUN (I)] , which DET (O) can VERB (O) [synthesize VERB (B) speech NOUN (I)] from ADP (O) text NOUN (O) through ADP (O) a NOUN (O) single ADJ (O) [feed NOUN - forward NOUN (B) pass VERB (I)] . 
 We PRON (O) also ADV (O) explore VERB (O) a NOUN (O) novel NOUN (O) VAE PROPN - based VERB (O) approach NOUN (O) to ADP (O) train NOUN (O) the DET (O) inverse NOUN (O) autoregressive ADJ (O) flow NOUN (O) (IAF PROPN) (O) based VERB (O) parallel NOUN (O) [vocoder NOUN (B)] from ADP (O) scratch NOUN , (O) which DET (O) avoids VERB (O) the DET (O) need NOUN (O) for ADP (O) distillation NOUN (O) from ADP (O) a NOUN (O) separately ADV (O) trained VERB (O) [WaveNet PROPN (B)] as SCONJ (O) previ NOUN - ous ADJ (O) work NOUN . (O) 

Introduction NOUN (O) 
 [Text NOUN - to ADP - speech NOUN (B)] ([TTS PROPN (B)]) , also ADV (O) called VERB (O) [speech NOUN (B) synthesis NOUN (I)] , has AUX (O) long ADJ (O) been AUX (O) a NOUN (O) vital ADJ (O) tool NOUN (O) in ADP (O) a NOUN (O) variety NOUN (O) of ADP (O) applications NOUN , (O) such ADJ (O) as SCONJ (O) human ADJ - computer NOUN (O) interactions NOUN , (O) virtual ADJ (O) assistant NOUN , (O) and CCONJ (O) content NOUN (O) creation NOUN . (O) 
 Traditional ADJ (O) [TTS PROPN (B) systems NOUN (I)] are AUX (O) based VERB (O) on ADP (O) multi ADJ - stage NOUN (O) hand NOUN - engineered VERB (O) pipelines NOUN . (O) 
 In ADP (O) recent ADJ (O) years NOUN , (O) [deep ADJ (B) neural NOUN (I) networks NOUN (I)] based VERB (O) [autoregressive ADJ (B) models NOUN (I)] have AUX (O) attained VERB (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB , (O) including VERB (O) high ADJ - fidelity NOUN (O) [audio NOUN (B) synthesis NOUN (I)] , and CCONJ (O) much ADJ (O) simpler ADJ (O) [seq2seq NOUN (B) pipelines NOUN (I)] . 
 In ADP (O) particular ADJ , (O) one NUM (O) of ADP (O) the DET (O) most ADJ (O) popular ADJ (O) [neural NOUN (B) TTS PROPN (I) pipeline NOUN (I)] (a.k.a ADJ . (O) â€œ PUNCT (O) [end NOUN - to ADP - end NOUN (B)] ") PUNCT (O) consists VERB (O) of ADP (O) two NUM (O) components NOUN (O) : (i NOUN) (O) an DET (O) [autoregressive ADJ (B) seq2seq NOUN (I) model NOUN (I)] that DET (O) generates VERB (O) [mel X (B) spectrogram PROPN (I)] from ADP (O) text NOUN , (O) and CCONJ (O) (ii PROPN) (O) an DET (O) [autoregressive ADJ (B) neural NOUN (I) vocoder NOUN (I)] (e.g. ADV , (O) [WaveNet PROPN (B)]) that DET (O) synthesizes NOUN (O) [raw ADJ (B) waveform PROPN (I)] from ADP (O) [mel X (B) spectrogram PROPN (I)] . 

Equal ADJ (O) contribution NOUN . (O) Baidu PROPN (O) Research NOUN , (O) 1195 NUM (O) Bordeaux NOUN (O) Dr PROPN , (O) Sunnyvale PROPN , (O) CA PROPN . (O) [Speech NOUN (B) samples NOUN (I)] can VERB (O) be AUX (O) found VERB (O) in ADP (O) : https://parallel-neural-tts-demo.github.io/. NOUN (O) Correspondence PROPN (O) to ADP (O) : Wei PROPN (O) Ping NOUN (O) < weiping.thu@gmail.com PROPN (O) > . (O) 

This DET (O) pipeline NOUN (O) requires VERB (O) much ADJ (O) less ADJ (O) expert NOUN (O) knowledge NOUN (O) and CCONJ (O) only ADV (O) needs VERB (O) pairs NOUN (O) of ADP (O) [audio NOUN (B)] and CCONJ (O) transcript NOUN (O) as SCONJ (O) [training NOUN (B) data NOUN (I)] . 
 However ADV , (O) the DET (O) autoregressive ADJ (O) nature NOUN (O) of ADP (O) these DET (O) models NOUN (O) makes VERB (O) them PRON (O) quite ADV (O) slow ADJ (O) at ADP (O) synthesis NOUN , (O) because SCONJ (O) they PRON (O) operate VERB (O) sequentially ADV (O) at ADP (O) a NOUN (O) high ADJ (O) temporal NOUN (O) resolution NOUN (O) of ADP (O) [waveform PROPN (B) samples NOUN (I)] and CCONJ (O) [spectrogram NOUN (B)] . 
 Most ADJ (O) recently ADV , (O) several ADJ (O) models NOUN (O) are AUX (O) proposed VERB (O) for ADP (O) [parallel NOUN (B) waveform PROPN (I) generation NOUN (I)] . 
 In ADP (O) the DET (O) [end NOUN - to ADP - end NOUN (B) pipeline NOUN (I)] , the DET (O) models NOUN (O) (e.g. ADV , (O) [ClariNet PROPN (B)] , WaveFlow PROPN) (O) still ADV (O) rely VERB (O) on ADP (O) autoregressive ADJ (O) component NOUN (O) to ADP (O) predict VERB (O) [spectrogram NOUN (B) features VERB (I)] (e.g. ADV , (O) 100 NUM (O) frames VERB (O) per ADP (O) second ADV) . (O) 
 In ADP (O) the DET (O) [linguistic NOUN (B) feature NOUN (I)]-based PROPN pipeline NOUN , (O) the DET (O) models NOUN (O) (e.g. ADV , (O) Parallel PROPN (O) [WaveNet PROPN (B)] , GAN-[TTS PROPN (B)]) are AUX (O) conditioned VERB (O) on ADP (O) aligned VERB (O) [linguistic NOUN (B) features VERB (I)] from ADP (O) [phoneme NOUN (B) duration NOUN (I) model NOUN (I)] and CCONJ (O) F0 NOUN (O) from ADP (O) frequency NOUN (O) model NOUN , (O) which DET (O) are AUX (O) recurrent NOUN (O) or CCONJ (O) [autoregressive ADJ (B) models NOUN (I)] . 
 Both DET (O) of ADP (O) these DET (O) [TTS PROPN (B) pipelines NOUN (I)] can VERB (O) be AUX (O) slow ADJ (O) at ADP (O) synthesis NOUN (O) on ADP (O) modern NOUN (O) hardware NOUN (O) optimized VERB (O) for ADP (O) parallel NOUN (O) execution NOUN . (O) 
  
In ADP (O) this DET (O) work NOUN , (O) we PRON (O) present NOUN (O) a NOUN (O) [fully ADV (B) parallel NOUN (I) neural PROPN (I) TTS PROPN (I) system NOUN (I)] by ADP (O) proposing VERB (O) a NOUN (O) [non ADJ - autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] . 
 Our DET (O) major NOUN (O) contributions NOUN (O) are AUX (O) as SCONJ (O) follows VERB (O) : 
 We PRON (O) propose NOUN (O) [ParaNet NOUN (B)] , a DET (O) [non ADJ - autoregressive ADJ (B) attention NOUN - based VERB (I) architecture NOUN (I)] for ADP (O) [text NOUN - to ADP - speech NOUN (B)] , which DET (O) is AUX (O) fully ADV (O) convolutional ADJ (O) and CCONJ (O) converts NOUN (O) text NOUN (O) to ADP (O) [mel X (B) spectrogram PROPN (I)] . 
 It PRON (O) runs VERB (O) 254.6 NUM (O) times NOUN (O) faster ADV (O) than SCONJ (O) real ADJ - time NOUN (O) at ADP (O) synthesis NOUN (O) on ADP (O) a NOUN (O) 1080 NUM (O) Ti PROPN (O) [GPU PROPN (B)] , and CCONJ (O) brings VERB (O) 46.7 NUM (O) times NOUN (O) speed NOUN - up NOUN (O) over ADP (O) its DET (O) [autoregressive ADJ (B) counterpart NOUN (I)] , while SCONJ (O) obtaining VERB (O) reasonably ADV (O) good NOUN (O) [speech NOUN (B) quality NOUN (I)] using VERB (O) [neural NOUN (B) vocoders NOUN (I)] . 
 [ParaNet PROPN (B)] distills VERB (O) the DET (O) attention NOUN (O) from ADP (O) the DET (O) [autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] , and CCONJ (O) iteratively ADV (O) refines NOUN (O) the DET (O) alignment NOUN (O) between ADP (O) text NOUN (O) and CCONJ (O) [spectrogram NOUN (B)] in ADP (O) a NOUN (O) layer NOUN - by ADP - layer NOUN (O) manner NOUN . (O) 
 It PRON (O) can VERB (O) produce NOUN (O) more ADJ (O) stable ADJ (O) attentions NOUN (O) than SCONJ (O) [autoregressive ADJ (B) Deep ADJ (I) Voice PROPN (I)] 3 NUM (O)     on ADP (O) the DET (O) challenging NOUN (O) test NOUN (O) sentences NOUN , (O) because SCONJ (O) it PRON (O) does AUX (O) not PART (O) have AUX (O) the DET (O) discrepancy NOUN (O) between ADP (O) the DET (O) teacher NOUN - forced VERB (O) training NOUN (O) and CCONJ (O) autoregressive ADJ (O) inference NOUN . (O) 
 We PRON (O) build NOUN (O) the DET (O) [fully ADV (B) parallel NOUN (I) neural PROPN (I) TTS PROPN (I) system NOUN (I)] by ADP (O) combining VERB (O) [ParaNet NOUN (B)] with ADP (O) [parallel NOUN (B) neural NOUN (I) vocoder NOUN (I)] , thus ADV (O) it PRON (O) can VERB (O) generate NOUN (O) [speech NOUN (B)] from ADP (O) text NOUN (O) through ADP (O) a NOUN (O) single ADJ (O) [feed NOUN - forward NOUN (B) pass VERB (I)] . 
 We PRON (O) investigate VERB (O) several ADJ (O) parallel NOUN (O) [vocoders NOUN (B)] , including VERB (O) the DET (O) distilled PROPN (O) [IAF PROPN (B) vocoder NOUN (I)] and CCONJ (O) Wave PROPN - Glow PROPN . (O) To ADP (O) explore VERB (O) the DET (O) possibility NOUN (O) of ADP (O) training NOUN (O) [IAF PROPN (B) vocoder NOUN (I)] without ADP (O) distillation NOUN , (O) we PRON (O) also ADV (O) propose NOUN (O) an DET (O) alternative NOUN (O) approach NOUN , (O) [WaveVAE NUM (B)] , which DET (O) can VERB (O) be AUX (O) trained VERB (O) from ADP (O) scratch NOUN (O) within ADP (O) the DET (O) [variational NOUN (B) autoencoder NOUN (I)] (VAE PROPN) (O) framework NOUN (O) ([Kingma PROPN (B)] & Welling PROPN , (O) 2014 NUM) . (O) 

We PRON (O) organize VERB (O) the DET (O) rest NOUN (O) of ADP (O) paper NOUN (O) as SCONJ (O) follows VERB . (O) Section NOUN (O) discusses VERB (O) related ADJ (O) work NOUN . (O) We PRON (O) introduce VERB (O) the DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I) architecture NOUN (I)] in ADP (O) Section NOUN . (O) 
 We PRON (O) discuss NOUN (O) [parallel NOUN (B) neural NOUN (I) vocoders NOUN (I)] in ADP (O) Section NOUN , (O) and CCONJ (O) report NOUN (O) experimental NOUN (O) settings NOUN (O) and CCONJ (O)     results NOUN (O) in ADP (O) Section NOUN . (O) We PRON (O) conclude VERB (O) the DET (O) paper NOUN (O) in ADP (O) Section NOUN . (O) 

Related VERB (O) work NOUN (O) 
 [Neural PROPN (B) speech NOUN (I) synthesis NOUN (I)] has AUX (O) obtained VERB (O) the DET (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB (O) and CCONJ (O) gained VERB (O) a NOUN (O) lot NOUN (O) of ADP (O) attention NOUN . (O) 
 Several ADJ (O) [neural NOUN (B) TTS PROPN (I) systems NOUN (I)] were AUX (O) proposed VERB , (O) including VERB (O) [WaveNet PROPN (B)] , [Deep ADJ (B) Voice PROPN (I)] , 
 [Deep ADJ (B) Voice PROPN (I)] 2 NUM , (O) 
 [Deep ADJ (B) Voice PROPN (I)] 3 NUM , (O) [Tacotron PROPN (B)] , 
 [Tacotron PROPN (B) 2 NUM (I)] , Char2Wav PROPN , (O) 
 VoiceLoop PROPN , (O) [WaveRNN PUNCT (B)] , 
 [ClariNet PROPN (B)] , and CCONJ (O) [Transformer NOUN (B) TTS PROPN (I)] . 
 In ADP (O) particular ADJ , (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM , (O) [Tacotron PROPN (B)] and CCONJ (O) Char2Wav PROPN (O) employ NOUN (O) [seq2seq NOUN (B) framework NOUN (I)] with ADP (O) the DET (O) [attention NOUN (B) mechanism NOUN (I)] , yielding VERB (O) much ADJ (O) simpler ADJ (O) pipeline NOUN (O) compared VERB (O) to ADP (O) traditional ADJ (O) multi ADJ - stage NOUN (O) pipeline NOUN . (O) 
 Their DET (O) excellent ADJ (O) extensibility NOUN (O) leads VERB (O) to ADP (O) promising NOUN (O) results NOUN (O) for ADP (O) several ADJ (O) challenging NOUN (O) tasks NOUN , (O) such ADJ (O) as SCONJ (O) [voice NOUN (B) cloning VERB (I)] . 
 All DET (O) of ADP (O) these DET (O) state NOUN - of ADP - the DET - art NOUN (O) systems NOUN (O) are AUX (O) based VERB (O) on ADP (O) [autoregressive ADJ (B) models NOUN (I)] . 

[RNN PROPN - based VERB (B) autoregressive ADJ (I) models NOUN (I)] , such ADJ (O) as SCONJ (O) [Tacotron PROPN (B)] and CCONJ (O) [WaveRNN PUNCT (B)] , lack NOUN (O) parallelism NOUN (O) at ADP (O) both DET (O) training NOUN (O) and CCONJ (O) synthesis NOUN . (O) 
 [CNN PROPN - based VERB (B) autoregressive ADJ (I) models NOUN (I)] , such ADJ (O) as SCONJ (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM (O) and CCONJ (O) [WaveNet PROPN (B)] , enable VERB (O) parallel NOUN (O) processing NOUN (O) at ADP (O) training NOUN , (O) but CCONJ (O) they PRON (O) still ADV (O) operate VERB (O) sequentially ADV (O) at ADP (O) synthesis NOUN (O) since SCONJ (O) each DET (O) output NOUN (O) element NOUN (O) must VERB (O) be AUX (O) generated VERB (O) before ADP (O) it PRON (O) can VERB (O) be AUX (O) passed VERB (O) in ADP (O) as SCONJ (O) input NOUN (O) at ADP (O) the DET (O) next ADJ (O) time NOUN - step NOUN . (O) 
 Recently ADV , (O) there PRON (O) are AUX (O) some DET (O) [non ADJ - autoregressive ADJ (B) models NOUN (I)] proposed VERB (O) for ADP (O) [neural NOUN (B) machine NOUN (I) translation NOUN (I)] . trains NOUN (O) a NOUN (O) [feed NOUN - forward NOUN (B) neural NOUN (I) network NOUN (I)] conditioned VERB (O) on ADP (O) fertility NOUN (O) values NOUN , (O) which DET (O) are AUX (O) obtained VERB (O) from ADP (O) an DET (O) external ADJ (O) alignment NOUN (O) system NOUN . (O) 
 proposes NOUN (O) a NOUN (O) latent NOUN (O) variable NOUN (O) model NOUN (O) for ADP (O) fast VERB (O) decoding VERB , (O) while SCONJ (O) it PRON (O) remains VERB (O) autoregressiveness NOUN (O) between ADP (O) latent NOUN (O) variables NOUN . (O) 
 iteratively ADV (O) refines NOUN (O) the DET (O) output NOUN (O) sequence NOUN (O) through ADP (O) a NOUN (O) denoising VERB (O) autoencoder NOUN (O) framework NOUN . (O) 
 Arguably ADV , (O) [non ADJ - autoregressive ADJ (B) model NOUN (I)] plays VERB (O) a NOUN (O) more ADJ (O) important ADJ (O) role NOUN (O) in ADP (O) [text NOUN - to ADP - speech NOUN (B)] , where ADV (O) the DET (O) output NOUN (O) [speech NOUN (B) spectrogram NOUN (I)] usually ADV (O) consists VERB (O) of ADP (O) hundreds NOUN (O) of ADP (O) time NOUN - steps NOUN (O) for ADP (O) a NOUN (O) short ADJ (O) text NOUN (O) input NOUN (O) with ADP (O) a NOUN (O) few ADJ (O) words NOUN . (O) 
 Our DET (O) work NOUN (O) is AUX (O) one NUM (O) of ADP (O) the DET (O) first ADJ (O) [non ADJ - autoregressive ADJ (B) seq2seq NOUN (I) model NOUN (I)] for ADP (O) [TTS PROPN (B)] and CCONJ (O) provides VERB (O) as SCONJ (O) much ADJ (O) as SCONJ (O) 46.7 NUM (O) times NOUN (O) speed NOUN - up NOUN (O) at ADP (O) synthesis NOUN (O) over ADP (O) its DET (O) [autoregressive ADJ (B) counterpart NOUN (I)] . 
 There PRON (O) is AUX (O) a NOUN (O) concurrent ADJ (O) work NOUN , (O) which DET (O) is AUX (O) based VERB (O) on ADP (O) the DET (O) [autoregressive ADJ (B) transformer NOUN (I) TTS PROPN (I)]     and CCONJ (O) can VERB (O) generate NOUN (O) [mel X (B) spectrogram PROPN (I)] in ADP (O) parallel NOUN . (O) 
 Our DET (O) [ParaNet NOUN (B)] is AUX (O) fully ADV (O) convolutional ADJ (O) and CCONJ (O) lightweight NOUN . (O) 
 In ADP (O) contrast NOUN (O) to ADP (O) Fast-[Speech NOUN (B)] , it PRON (O) has AUX (O) half NOUN (O) of ADP (O) model NOUN (O) parameters NOUN , (O) requires VERB (O) smaller ADJ (O) [batch NOUN (B) size NOUN (I)]     for ADP (O) training NOUN (O) and CCONJ (O) provides VERB (O) faster ADV (O) speed NOUN (O) at ADP (O) synthesis NOUN (O) (see VERB (O) Table NOUN (O) for ADP (O) detailed ADJ (O) comparison NOUN) . (O) 

Flow NOUN - based VERB (O) generative PROPN (O) models NOUN (O)     transform NOUN (O) a NOUN (O) simple ADJ (O) initial NOUN (O) distribution NOUN (O) into ADP (O) a NOUN (O) more ADJ (O) complex NOUN (O) one NUM (O) by ADP (O) applying VERB (O) a NOUN (O) series NOUN (O) of ADP (O) invertible ADJ (O) transformations NOUN . (O) 
 In ADP (O) previous ADJ (O) work NOUN , (O) flow NOUN - based VERB (O) models NOUN (O) have AUX (O) obtained VERB (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB (O) for ADP (O) [parallel NOUN (B) waveform PROPN (I) synthesis NOUN (I)] . 
 [Variational PROPN (B) autoencoder NOUN (I)] (VAE PROPN) (O)     has AUX (O) been AUX (O) applied VERB (O) for ADP (O) representation NOUN (O) learning NOUN (O) of ADP (O) natural ADJ (O) [speech NOUN (B)] for ADP (O) years NOUN . (O) 
 It PRON (O) models NOUN (O) either CCONJ (O) the DET (O) generative PROPN (O) process NOUN (O) of ADP (O) [raw ADJ (B) waveform PROPN (I)] . 
 In ADP (O) previous ADJ (O) work NOUN , (O) autoregressive ADJ (O) or CCONJ (O) [recurrent NOUN (B) neural NOUN (I) networks NOUN (I)] are AUX (O) employed VERB (O) as SCONJ (O) the DET (O) [decoder NOUN (B)] of ADP (O) VAE PROPN , (O) but CCONJ (O) they PRON (O) can VERB (O) be AUX (O) quite ADV (O) slow ADJ (O) at ADP (O) synthesis NOUN . (O) 
 In ADP (O) this DET (O) work NOUN , (O) we PRON (O) employ NOUN (O) a NOUN (O) [feed NOUN - forward NOUN (B)] IAF PROPN (O) as SCONJ (O) the DET (O) [decoder NOUN (B)] , which DET (O) enables VERB (O) [parallel NOUN (B) waveform PROPN (I) synthesis NOUN (I)] . 

[Text NOUN - to ADP - spectrogram NOUN (B) model NOUN (I)] 
 Our DET (O) parallel NOUN (O) [TTS PROPN (B) system NOUN (I)] has AUX (O) two NUM (O) components NOUN (O) : a DET (O) [feed NOUN - forward NOUN (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] , and CCONJ (O) a NOUN (O) parallel NOUN (O) wave NOUN - form NOUN (O) synthesizer NOUN (O) conditioned VERB (O) on ADP (O) [mel X (B) spectrogram PROPN (I)] . 
 In ADP (O) this DET (O) section NOUN , (O) we PRON (O) first ADJ (O) present NOUN (O) an DET (O) [autoregressive ADJ (B) model NOUN (I)] derived VERB (O) from ADP (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM (O) (DV3 PROPN) . (O) 
 We PRON (O) then ADV (O) introduce VERB (O) [ParaNet NOUN (B)] , a DET (O) [non ADJ - autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] (see VERB (O) Figure NOUN) . (O) 

[Autoregressive PROPN (B) architecture NOUN (I)] 
 Our DET (O) [autoregressive ADJ (B) model NOUN (I)] is AUX (O) based VERB (O) on ADP (O) DV3 PROPN , (O) a NOUN (O) convolutional ADJ (O) [text NOUN - to ADP - spectrogram NOUN (B) architecture NOUN (I)] , which DET (O) consists VERB (O) of ADP (O) three NUM (O) components NOUN (O) : 
 â€¢ X (O) [Encoder PROPN (B)] : A NOUN (O) [convolutional NOUN (B) encoder NOUN (I)] , which DET (O) takes VERB (O) text NOUN (O) inputs VERB (O) and CCONJ (O) encodes VERB (O) them PRON (O) into ADP (O) internal ADJ (O) hidden VERB (O) representation NOUN . (O) 
 â€¢ X (O) [Decoder PROPN (B)] : A NOUN (O) causal NOUN (O) convolutional ADJ (O) [decoder NOUN (B)] , which DET (O) decodes VERB (O) the DET (O) [encoder NOUN (B) representation NOUN (I)] with ADP (O) an DET (O) [attention NOUN (B) mechanism NOUN (I)] to PART (O) [log PROPN - mel PROPN (B) spectragrams NOUN (I)] in ADP (O) an DET (O) autoregressive ADJ (O) manner NOUN (O) with ADP (O) an DET (O) ` 1 NUM (O) loss NOUN . (O) 
 It PRON (O) starts VERB (O) with ADP (O) a NOUN (O) 1 NUM (O) Ã— PROPN (O) 1 NUM (O) convolution NOUN (O) to ADP (O) preprocess NOUN (O) the DET (O) input NOUN (O) [log PROPN - mel PROPN (B) spectrograms VERB (I)] . 
 â€¢ X (O) Converter PROPN (O) : A NOUN (O) non ADJ - causal ADJ (O) convolutional ADJ (O) post NOUN (O) processing NOUN (O) network NOUN , (O) which DET (O) processes VERB (O) the DET (O) hidden VERB (O) representation NOUN (O) from ADP (O) the DET (O) [decoder NOUN (B)] using VERB (O) both DET (O) past ADJ (O) and CCONJ (O) future NOUN (O) context NOUN (O) information NOUN (O) and CCONJ (O) predicts VERB (O) the DET (O) log NOUN - linear NOUN (O) [spectrograms NOUN (B)] with ADP (O) an DET (O) ` 1 NUM (O) loss NOUN . (O) 
 It PRON (O) enables VERB (O) bidirectional ADJ (O) processing NOUN . (O) 
 All DET (O) these DET (O) components NOUN (O) use NOUN (O) the DET (O) same ADJ (O) 1-D NUM (O) [convolution NOUN (B) block NOUN (I)] with ADP (O) a NOUN (O) gated VERB (O) linear NOUN (O) unit NOUN (O) as SCONJ (O) in ADP (O) DV3 PROPN (O) (see VERB (O) Figure NOUN (O) (b NOUN) (O) for ADP (O) more ADJ (O) details NOUN) . (O) 
 The DET (O) major NOUN (O) difference NOUN (O) between ADP (O) our DET (O) model NOUN (O) and CCONJ (O) DV3 PROPN (O) is AUX (O) the DET (O) [decoder NOUN (B) architecture NOUN (I)] . 
 The DET (O) [decoder NOUN (B)] of ADP (O) DV3 PROPN (O) has AUX (O) multiple NOUN (O) [attention NOUN - based VERB (B) layers NOUN (I)] , where ADV (O) each DET (O) layer NOUN (O) consists VERB (O) of ADP (O) a NOUN (O) causal NOUN (O) [convolution NOUN (B) block NOUN (I)] followed VERB (O) by ADP (O) an DET (O) [attention NOUN (B) block NOUN (I)] . 
 To PART (O) simplify NOUN (O) the DET (O) [attention NOUN (B) distillation NOUN (I)] described VERB (O) in ADP (O) Section NOUN , (O) our DET (O) [autoregressive ADJ (B) decoder NOUN (I)] has AUX (O) only ADV (O) one NUM (O) [attention NOUN (B) block NOUN (I)] at ADP (O) its DET (O) first ADJ (O) layer NOUN . (O) 
 We PRON (O) find VERB (O) that DET (O) reducing VERB (O) the DET (O) number NOUN (O) of ADP (O) [attention NOUN (B) blocks PROPN (I)] does AUX (O) not PART (O) hurt VERB (O) the DET (O) generated VERB (O) [speech NOUN (B) quality NOUN (I)] in ADP (O) general ADJ . (O) 


Figure PROPN . (O) (a X) (O) [Autoregressive PROPN (B) seq2seq NOUN (I) model NOUN (I)] . The DET (O) dashed VERB (O) line NOUN (O) depicts VERB (O) the DET (O) autoregressive ADJ (O) decoding VERB (O) of ADP (O) [mel X (B) spectrogram PROPN (I)] at ADP (O) inference NOUN . (O) 
 (b PROPN) (O) [Non ADJ - autoregressive ADJ (B) ParaNet PROPN (I) model NOUN (I)] , which DET (O) distills VERB (O) the DET (O) attention NOUN (O) from ADP (O) a NOUN (O) pretrained VERB (O) [autoregressive ADJ (B) model NOUN (I)] . 

Figure NOUN . (O) (a X) (O) Architecture PROPN (O) of ADP (O) [ParaNet NOUN (B)] . Its DET (O) [encoder NOUN (B)] provides VERB (O) key NOUN (O) and CCONJ (O) value NOUN (O) as SCONJ (O) the DET (O) textual NOUN (O) representation NOUN . (O) The DET (O) first ADJ (O) [attention NOUN (B) block NOUN (I)] in ADP (O) [decoder NOUN (B)] 
 gets VERB (O) positional NOUN (O) encoding NOUN (O) as SCONJ (O) the DET (O) query NOUN (O) and CCONJ (O) is AUX (O) followed VERB (O) by ADP (O) non ADJ - causal ADJ (O) [convolution NOUN (B) blocks PROPN (I)] and CCONJ (O) [attention NOUN (B) blocks PROPN (I)] . 
 (b NOUN) (O) [Convolution NOUN (B) block NOUN (I)] appears VERB (O) in ADP (O) both DET (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B)] . It PRON (O) consists VERB (O) of ADP (O) a NOUN (O) 1-D NUM (O) convolution NOUN (O) with ADP (O) a NOUN (O) gated VERB (O) linear NOUN (O) unit NOUN (O) (GLU PROPN) (O) and CCONJ (O) a NOUN (O) [residual ADJ (B) connection NOUN (I)] . 

[Non ADJ - autoregressive ADJ (B) architecture NOUN (I)] 
 The DET (O) proposed VERB (O) [ParaNet NOUN (B)] (see VERB (O) Figure NOUN) (O) uses VERB (O) the DET (O) same ADJ (O) [encoder NOUN (B) architecture NOUN (I)] as SCONJ (O) the DET (O) [autoregressive ADJ (B) model NOUN (I)] . 
 The DET (O) [decoder NOUN (B)] of ADP (O) [ParaNet NOUN (B)] , conditioned VERB (O) solely ADV (O) on ADP (O) the DET (O) hidden VERB (O) representation NOUN (O) from ADP (O) the DET (O) [encoder NOUN (B)] , predicts VERB (O) the DET (O) entire ADJ (O) sequence NOUN (O) of ADP (O) [log PROPN - mel PROPN (B) spectrograms VERB (I)] in ADP (O) a NOUN (O) [feed NOUN - forward NOUN (B) manner NOUN (I)] . 
 As SCONJ (O) a NOUN (O) result VERB , (O) both DET (O) its DET (O) training NOUN (O) and CCONJ (O) synthesis NOUN (O) can VERB (O) be AUX (O) done VERB (O) in ADP (O) parallel NOUN . (O) 
 Specially ADV , (O) we PRON (O) make VERB (O) the DET (O) following VERB (O) major NOUN (O) architecture NOUN (O) modifications NOUN (O) from ADP (O) the DET (O) [autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] to PART (O) the DET (O) [non ADJ - autoregressive ADJ (B) model NOUN (I)] : 
 [Non ADJ - autoregressive ADJ (B) decoder NOUN (I)] : Without ADP (O) the DET (O) autoregressive ADJ (O) generative ADJ (O) constraint NOUN , (O) the DET (O) [decoder NOUN (B)] can VERB (O) use NOUN (O) non ADJ - causal ADJ (O) [convolution NOUN (B) blocks PROPN (I)] to PART (O) take VERB (O) advantage NOUN (O) of ADP (O) future NOUN (O) context NOUN (O) information NOUN (O) and CCONJ (O) to ADP (O) improve VERB (O) model NOUN (O) performance NOUN . (O) 
 In ADP (O) addition NOUN (O) to ADP (O) [log PROPN - mel PROPN (B) spectrograms VERB (I)] , it PRON (O) also ADV (O) predicts VERB (O) log NOUN - linear NOUN (O) [spectrograms NOUN (B)] with ADP (O) an DET (O) ` 1 NUM (O) loss NOUN (O) for ADP (O) slightly ADV (O) better ADJ (O) performance NOUN . (O) We PRON (O) also ADV (O) remove VERB (O) the DET (O) 1 NUM (O) Ã— PROPN (O) 1 NUM (O) convolution NOUN (O) at ADP (O) the DET (O) beginning NOUN , (O) because SCONJ (O) the DET (O) [decoder NOUN (B)] does AUX (O) not PART (O) take VERB (O) [log PROPN - mel PROPN (B) spectrograms VERB (I)] as SCONJ (O) input NOUN . (O) 
 No NOUN (O) converter NOUN (O) : [Non ADJ - autoregressive ADJ (B) model NOUN (I)] removes VERB (O) the DET (O) non ADJ - causal ADJ (O) converter NOUN (O) since SCONJ (O) it PRON (O) already ADV (O) employs VERB (O) a NOUN (O) non ADJ - causal ADJ (O) [decoder NOUN (B)] . 
 Note NOUN (O) that SCONJ , (O) the DET (O) major NOUN (O) motivation NOUN (O) of ADP (O) introducing VERB (O) non ADJ - causal ADJ (O) converter NOUN (O) in ADP (O) DV3 PROPN (O) is AUX (O) to ADP (O) refine NOUN (O) the DET (O) [decoder NOUN (B) predictions NOUN (I)] based VERB (O) on ADP (O) bidirectional ADJ (O) context NOUN (O) information NOUN (O) provided VERB (O) by ADP (O) non ADJ - causal ADJ (O) convolutions NOUN . (O) 

Parallel PROPN (O) [attention NOUN (B) mechanism NOUN (I)] 
 It PRON (O) is AUX (O) challenging NOUN (O) for ADP (O) the DET (O) [feed NOUN - forward NOUN (B) model NOUN (I)] to PART (O) learn VERB (O) the DET (O) accurate ADJ (O) alignment NOUN (O) between ADP (O) the DET (O) input NOUN (O) text NOUN (O) and CCONJ (O) output NOUN (O) [spectrogram NOUN (B)] . 
 In ADP (O) particular ADJ , (O) we PRON (O) need NOUN (O) the DET (O) full ADJ (O) parallelism NOUN (O) within ADP (O) the DET (O) [attention NOUN (B) mechanism NOUN (I)] . 
 For ADP (O) example NOUN , (O) the DET (O) location NOUN - sensitive ADJ (O) attention NOUN (O)     improves VERB (O) attention NOUN (O) stability NOUN , (O) but CCONJ (O) it PRON (O) performs VERB (O) sequentially ADV (O) at ADP (O) both DET (O) training NOUN (O) and CCONJ (O) synthesis NOUN , (O) because SCONJ (O) it PRON (O) uses VERB (O) the DET (O) cumulative ADJ (O) [attention NOUN (B) weights NOUN (I)] from ADP (O) previous ADJ (O) [decoder NOUN (B)] time NOUN (O) steps NOUN (O) as SCONJ (O) an DET (O) [additional ADJ (B) feature NOUN (I)] for ADP (O) the DET (O) next ADJ (O) time NOUN (O) step NOUN . (O) 
 Previous ADJ (O) [non ADJ - autoregressive ADJ (B) decoders NOUN (I)] rely VERB (O) on ADP (O) an DET (O) external ADJ (O) alignment NOUN (O) system NOUN , (O) or CCONJ (O) an DET (O) autoregressive ADJ (O) latent NOUN (O) variable NOUN (O) model NOUN . (O) 

Figure NOUN . (O) Our DET (O) [ParaNet NOUN (B)] iteratively ADV (O) refines NOUN (O) the DET (O) [attention NOUN (B) alignment NOUN (I)] in ADP (O) a NOUN (O) layer NOUN - by ADP - layer NOUN (O) way NOUN . (O) One NUM (O) can VERB (O) see VERB (O) the DET (O) 1st NOUN (O) layer NOUN (O) attention NOUN (O) is AUX (O) mostly ADV (O) 
 dominated VERB (O) by ADP (O) the DET (O) positional NOUN (O) encoding NOUN (O) prior ADV . (O) It PRON (O) becomes VERB (O) more ADJ (O) and CCONJ (O) more ADJ (O) confident ADJ (O) about ADP (O) the DET (O) alignment NOUN (O) in ADP (O) the DET (O) subsequent ADJ (O) layers NOUN . (O) 

In ADP (O) this DET (O) work NOUN , (O) we PRON (O) present NOUN (O) several ADJ (O) simple ADJ (O) & effective ADJ (O) techniques NOUN , (O) which DET (O) could VERB (O) obtain VERB (O) accurate ADJ (O) and CCONJ (O) stable ADJ (O) [attention NOUN (B) alignment NOUN (I)] . 
 In ADP (O) particular ADJ , (O) our DET (O) [non ADJ - autoregressive ADJ (B) decoder NOUN (I)] can VERB (O) iteratively ADV (O) refine NOUN (O) the DET (O) [attention NOUN (B) alignment NOUN (I)] between ADP (O) text NOUN (O) and CCONJ (O) [mel X (B) spectrogram PROPN (I)] in ADP (O) a NOUN (O) layer NOUN - by ADP - layer NOUN (O) manner NOUN (O) as SCONJ (O) illustrated VERB (O) in ADP (O) Figure NOUN . (O) 
 Specially ADV , (O) the DET (O) [decoder NOUN (B)] adopts VERB (O) a NOUN (O) dot NOUN - product NOUN (O) [attention NOUN (B) mechanism NOUN (I)] and CCONJ (O) consists VERB (O) of ADP (O) K PROPN (O) [attention NOUN (B) blocks PROPN (I)] (see VERB (O) Figure NOUN (O) (a NOUN)) , (O) where ADV (O) each DET (O) [attention NOUN (B) block NOUN (I)] uses VERB (O) the DET (O) per ADP - time NOUN - step NOUN (O) query NOUN (O) [vectors NOUN (B)] from ADP (O) [convolution NOUN (B) block NOUN (I)] and CCONJ (O) per ADP - time NOUN - step NOUN (O) key NOUN (O) [vectors NOUN (B)] from ADP (O) [encoder NOUN (B)] to PART (O) compute NOUN (O) the DET (O) [attention NOUN (B) weights NOUN (I)] (Ping PROPN (O) et NOUN (O) al PROPN . , (O) 2018b NUM) . (O) 
 The DET (O) [attention NOUN (B) block NOUN (I)] computes VERB (O) [context NOUN (B) vectors NOUN (I)] as SCONJ (O) the DET (O) weighted ADJ (O) average NOUN (O) of ADP (O) the DET (O) value NOUN (O) [vectors NOUN (B)] from ADP (O) the DET (O) [encoder NOUN (B)] . 
 The DET (O) [non ADJ - autoregressive ADJ (B) decoder NOUN (I)] starts VERB (O) with ADP (O) an DET (O) [attention NOUN (B) block NOUN (I)] , in ADP (O) which DET (O) the DET (O) query NOUN (O) [vectors NOUN (B)] are AUX (O) solely ADV (O) positional NOUN (O) encoding NOUN (O) (see VERB (O) Section NOUN (O) for ADP (O) details NOUN) . (O) 
 The DET (O) first ADJ (O) [attention NOUN (B) block NOUN (I)] then ADV (O) provides VERB (O) the DET (O) input NOUN (O) for ADP (O) the DET (O) [convolution NOUN (B) block NOUN (I)] at ADP (O) the DET (O) next ADJ (O) [attention NOUN - based VERB (B) layer NOUN (I)] . 

[ATTENTION NOUN (B) DISTILLATION NOUN (I)]                                         
We PRON (O) use NOUN (O) the DET (O) [attention NOUN (B) alignments NOUN (I)] from ADP (O) a NOUN (O) pretrained VERB (O) [autoregressive ADJ (B) model NOUN (I)] to PART (O) guide VERB (O) the DET (O) training NOUN (O) of ADP (O) [non ADJ - autoregressive ADJ (B) model NOUN (I)] . 
 Specifically ADV , (O) we PRON (O) minimize NOUN (O) the DET (O) cross NOUN (O) entropy NOUN (O) between ADP (O) the DET (O) [attention NOUN (B) distributions NOUN (I)] from ADP (O) the DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] and CCONJ (O) a NOUN (O) pretrained VERB (O) autoregressive ADJ (O) teacher NOUN . (O) 
 We PRON (O) denote NOUN (O) the DET (O) [attention NOUN (B) weights NOUN (I)] from ADP (O) the DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] (k NOUN) (O) as SCONJ (O) Wi PROPN , (O) j PROPN , (O) where ADV (O) i PRON (O) and CCONJ (O) j PROPN (O) index NOUN (O) the DET (O) time NOUN - step NOUN (O) of ADP (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B)] respectively ADV , (O) and CCONJ (O) k NOUN (O) refers VERB (O) to ADP (O) the DET (O) k PROPN - th NOUN (O) [attention NOUN (B) block NOUN (I)] within ADP (O) the DET (O) [decoder NOUN (B)] . 
 Note NOUN (O) that SCONJ , (O) the DET (O) [attention NOUN (B) weights NOUN (I)] (k NOUN) (O) { Wi PROPN , (O) j PROPN (O) } M PROPN (O) i=1 PROPN (O) form NOUN (O) a NOUN (O) valid ADJ (O) distribution NOUN . (O) We PRON (O) compute NOUN (O) the DET (O) attention NOUN (O) loss NOUN (O) as SCONJ (O) the DET (O) average NOUN (O) cross NOUN (O) entropy NOUN (O) between ADP (O) the DET (O) [ParaNet NOUN (B)] and CCONJ (O) teacher NOUN â€™s PUNCT (O) [attention NOUN (B) distributions NOUN (I)] :                                    
K PROPN (O) N NOUN (O) M PROPN (O) 1 NUM (O) XXX PROPN (O) (k NOUN) (O) latten PROPN (O) = âˆ’ PROPN (O) Wi PROPN , (O) jt PROPN (O) log NOUN (O) Wi PROPN , (O) j PROPN , (O) (1 NUM) (O)     KN PROPN (O)      j=1 PROPN (O) i=1 PROPN (O) k=1 X (O) 
 where ADV (O) Wi PROPN , (O) jt PROPN (O) are AUX (O) the DET (O) [attention NOUN (B) weights NOUN (I)] from ADP (O) the DET (O) autoregressive ADJ (O) teacher NOUN , (O) M PROPN (O) and CCONJ (O) N NOUN (O) are AUX (O) the DET (O) lengths PROPN (O) of ADP (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B)] , respectively ADV . (O) 
 Our DET (O) final ADJ (O) [loss NOUN (B) function NOUN (I)] is AUX (O) a NOUN (O) linear NOUN (O) combination NOUN (O) of ADP (O) latten NOUN (O) and CCONJ (O) ` 1 NUM (O) losses NOUN (O) from ADP (O) [spectrogram NOUN (B) predictions NOUN (I)] . We PRON (O) set NOUN (O) the DET (O) coefficient NOUN (O) of ADP (O) latten NOUN (O) as SCONJ , (O) and CCONJ (O) other ADJ (O) coefficients NOUN (O) as SCONJ (O) 1 NUM (O) in ADP (O) all DET (O) experiments NOUN . (O) 

POSITIONAL PROPN (O) ENCODING NOUN (O) 
 We PRON (O) use NOUN (O) a NOUN (O) similar ADJ (O) positional NOUN (O) encoding NOUN (O) as SCONJ (O) in ADP (O) DV3 PROPN (O) at ADP (O) every DET (O) [attention NOUN (B) block NOUN (I)] . 
 The DET (O) positional NOUN (O) encoding NOUN (O) is AUX (O) added VERB (O) to ADP (O) both DET (O) key NOUN (O) and CCONJ (O) query NOUN (O) [vectors NOUN (B)] in ADP (O) the DET (O) [attention NOUN (B) block NOUN (I)] , which DET (O) forms NOUN (O) an DET (O) inductive ADJ (O) bias NOUN (O) for ADP (O) monotonic PROPN (O) atten NOUN - tion NOUN . (O) 
 Note NOUN (O) that SCONJ , (O) the DET (O) [non ADJ - autoregressive ADJ (B) model NOUN (I)] solely ADV (O) relies VERB (O) on ADP (O) its DET (O) [attention NOUN (B) mechanism NOUN (I)] to PART (O) decode NOUN (O) [mel X (B) spectrograms VERB (I)] from ADP (O) the DET (O) encoded VERB (O) [textual NOUN (B) features VERB (I)] , without ADP (O) any DET (O) autoregressive ADJ (O) input NOUN . (O) 
 This DET (O) makes VERB (O) the DET (O) positional NOUN (O) encoding NOUN (O) even ADV (O) more ADJ (O) crucial ADJ (O) in ADP (O) guiding VERB (O) the DET (O) attention NOUN (O) to ADP (O) follow NOUN (O) a NOUN (O) monotonic ADJ (O) progression NOUN (O) over ADP (O) time NOUN (O) at ADP (O) the DET (O) beginning NOUN (O) of ADP (O) training NOUN . (O) 
 The DET (O) positional NOUN (O) encodings VERB (O) hp PROPN (O) (i PRON , (O) k NOUN) (O) = sin NOUN (O) (Ï‰s PROPN (O) i/10000k PUNCT (O) / d NOUN) (O) (for ADP (O) even ADV (O) i NOUN) , (O) and CCONJ (O) cos NOUN (O) (Ï‰s PROPN (O) i/10000k PUNCT (O) / d NOUN) (O) (for ADP (O) odd ADJ (O) i NOUN) , (O) where ADV (O) i PRON (O) is AUX (O) the DET (O) time NOUN - step NOUN (O) index NOUN , (O) k NOUN (O) is AUX (O) the DET (O) channel NOUN (O) index NOUN , (O) d NOUN (O) is AUX (O) the DET (O) total NOUN (O) number NOUN (O) of ADP (O) channels NOUN (O) in ADP (O) the DET (O) positional NOUN (O) encoding NOUN , (O) and CCONJ (O) Ï‰s NOUN (O) is AUX (O) the DET (O) position NOUN (O) rate NOUN (O) which DET (O) indicates VERB (O) the DET (O) average NOUN (O) slope NOUN (O) of ADP (O) the DET (O) line NOUN (O) in ADP (O) the DET (O) [attention NOUN (B) distribution NOUN (I)] and CCONJ (O) roughly ADV (O) corresponds VERB (O) to ADP (O) the DET (O) speed NOUN (O) of ADP (O) [speech NOUN (B)] . 
 We PRON (O) set NOUN (O) Ï‰s NOUN (O) in ADP (O) the DET (O) following VERB (O) ways NOUN (O) : 
 â€¢ X (O) For ADP (O) the DET (O) autoregressive ADJ (O) teacher NOUN , (O) Ï‰s NOUN (O) is AUX (O) set NOUN (O) to ADP (O) one NUM (O) for ADP (O) the DET (O) positional NOUN (O) encoding NOUN (O) of ADP (O) query NOUN . (O) For ADP (O) the DET (O) key NOUN , (O) it PRON (O) is AUX (O) set NOUN (O) to ADP (O) the DET (O) averaged VERB (O) ratio NOUN (O) of ADP (O) the DET (O) time NOUN - steps NOUN (O) of ADP (O) [spectrograms NOUN (B)] to PART (O) the DET (O) time NOUN - steps NOUN (O) of ADP (O) [textual NOUN (B) features VERB (I)] , which DET (O) is AUX (O) around ADP (O) 6.3 NUM (O) across ADP (O) our DET (O) [training NOUN (B) dataset NOUN (I)] . 
 Taking VERB (O) into ADP (O) account NOUN (O) that SCONJ (O) a NOUN (O) reduction NOUN (O) factor NOUN (O) of ADP (O) 4 NUM (O) is AUX (O) used VERB (O) to ADP (O) simplify NOUN (O) the DET (O) learning NOUN (O) of ADP (O) [attention NOUN (B) mechanism NOUN (I)]    , Ï‰s PROPN (O) is AUX (O) simply ADV (O) set NOUN (O) as SCONJ (O) 6.3/4 NUM (O) for ADP (O) the DET (O) key NOUN (O) at ADP (O) both DET (O) training NOUN (O) and CCONJ (O) synthesis NOUN . (O) 
 â€¢ X (O) For ADP (O) [ParaNet NOUN (B)] , Ï‰s PROPN (O) is AUX (O) also ADV (O) set NOUN (O) to ADP (O) one NUM (O) for ADP (O) the DET (O) query NOUN , (O) while SCONJ (O) Ï‰s NOUN (O) for ADP (O) the DET (O) key NOUN (O) is AUX (O) calculated VERB (O) differently ADV . (O) At ADP (O) training NOUN , (O) Ï‰s NOUN (O) is AUX (O) set NOUN (O) to ADP (O) the DET (O) ratio NOUN (O) of ADP (O) the DET (O) lengths PROPN (O) of ADP (O) [spectrograms NOUN (B)] and CCONJ (O) text NOUN (O) for ADP (O) each DET (O) individual NOUN (O) training NOUN (O) instance NOUN , (O) which DET (O) is AUX (O) also ADV (O) divided VERB (O) by ADP (O) a NOUN (O) reduction NOUN (O) factor NOUN (O) of ADP (O) 4 NUM . (O) 
 At ADP (O) synthesis NOUN , (O) we PRON (O) need NOUN (O) to ADP (O) specify VERB (O) the DET (O) length NOUN (O) of ADP (O) output NOUN (O) [spectrogram NOUN (B)] and CCONJ (O) the DET (O) corresponding VERB (O) Ï‰s PROPN , (O) which DET (O) actually ADV (O) controls VERB (O) the DET (O) [speech NOUN (B) rate NOUN (I)] of ADP (O) the DET (O) generated VERB (O) [audios NOUN (B)] (see VERB (O) Section NOUN (O) II PROPN (O) on ADP (O) demo NOUN (O) website NOUN) . (O) 
 In ADP (O) all DET (O) of ADP (O) our DET (O) experiments NOUN , (O) we PRON (O) simply ADV (O) set NOUN (O) Ï‰s NOUN (O) to ADP (O) be AUX (O) 6.3/4 NUM (O) as SCONJ (O) in ADP (O) [autoregressive ADJ (B) model NOUN (I)] , and CCONJ (O) the DET (O) length NOUN (O) of ADP (O) output NOUN (O) [spectrogram NOUN (B)] as SCONJ (O) 6.3/4 NUM (O) times NOUN (O) the DET (O) length NOUN (O) of ADP (O) input NOUN (O) text NOUN . (O) 

Such ADJ (O) a NOUN (O) setup NOUN (O) yields NOUN (O) an DET (O) initial NOUN (O) attention NOUN (O) in ADP (O) the DET (O) form NOUN (O) of ADP (O) a NOUN (O) diagonal ADJ (O) line NOUN (O) and CCONJ (O) guides NOUN (O) the DET (O) [non ADJ - autoregressive ADJ (B) decoder NOUN (I)] to PART (O) refine NOUN (O) its DET (O) [attention NOUN (B) layer NOUN (I)] by ADP (O) layer NOUN (O) (see VERB (O) Figure NOUN) . (O) 

ATTENTION NOUN (O) MASKING PROPN (O) 
 Inspired PROPN (O) by ADP (O) the DET (O) attention NOUN (O) masking NOUN (O) in ADP (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM , (O) we PRON (O) propose NOUN (O) an DET (O) attention NOUN (O) masking NOUN (O) scheme NOUN (O) for ADP (O) the DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] at ADP (O) synthesis NOUN (O) :                                                      
â€¢ X (O) For ADP (O) each DET (O) query NOUN (O) from ADP (O) [decoder NOUN (B)] , instead ADV (O) of ADP (O) computing NOUN (O) the DET (O) [softmax PROPN (B)] over ADP (O) the DET (O) entire ADJ (O) set NOUN (O) of ADP (O) [encoder NOUN (B)] key NOUN (O) [vectors NOUN (B)] , we PRON (O) compute NOUN (O) the DET (O) [softmax PROPN (B)] only ADV (O) over ADP (O) a NOUN (O) fixed VERB (O) window NOUN (O) centered VERB (O) around ADP (O) the DET (O) target NOUN (O) position NOUN (O) and CCONJ (O) going VERB (O) forward NOUN (O) and CCONJ (O) backward NOUN (O) several ADJ (O) time NOUN - steps NOUN . (O) 
 The DET (O) target NOUN (O) position NOUN (O) is AUX (O) calculated VERB (O) as SCONJ (O) biquery NOUN (O) Ã— PROPN (O) 4/6.3e NUM , (O) where ADV (O) iquery NOUN (O) is AUX (O) the DET (O) time NOUN - step NOUN (O) index NOUN (O) of ADP (O) the DET (O) query NOUN (O) [vector NOUN (B)] , and CCONJ (O) be AUX (O) is AUX (O) the DET (O) rounding VERB (O) operator NOUN . (O) 
 We PRON (O) observe VERB (O) that SCONJ (O) this DET (O) strategy NOUN (O) reduces VERB (O) serious ADJ (O) [attention NOUN (B) errors NOUN (I)] such ADJ (O) as SCONJ (O) repeating VERB (O) or CCONJ (O) skipping NOUN (O) words NOUN , (O) and CCONJ (O) also ADV (O) yields NOUN (O) clearer ADJ (O) pronunciations NOUN , (O) thanks NOUN (O) to ADP (O) its DET (O) more ADJ (O) condensed VERB (O) [attention NOUN (B) distribution NOUN (I)] . 
 Note NOUN (O) that SCONJ , (O) this DET (O) attention NOUN (O) masking NOUN (O) is AUX (O) shared VERB (O) across ADP (O) all DET (O) [attention NOUN (B) blocks PROPN (I)] once ADV (O) it PRON (O) is AUX (O) generated VERB , (O) and CCONJ (O) does AUX (O) not PART (O) prevent NOUN (O) the DET (O) parallel NOUN (O) synthesis NOUN (O) of ADP (O) the DET (O) [non ADJ - autoregressive ADJ (B) model NOUN (I)] . 

[Parallel PROPN (B) waveform PROPN (I) model NOUN (I)] 
 As SCONJ (O) an DET (O) indispensable ADJ (O) component NOUN (O) in ADP (O) our DET (O) parallel NOUN (O) [neural NOUN (B) TTS PROPN (I) system NOUN (I)] , the DET (O) [parallel NOUN (B) waveform PROPN (I) model NOUN (I)] converts VERB (O) the DET (O) [mel X (B) spectrogram PROPN (I)] predicted VERB (O) from ADP (O) [ParaNet NOUN (B)] into ADP (O) the DET (O) [raw ADJ (B) waveform PROPN (I)] . 
 In ADP (O) this DET (O) section NOUN , (O) we PRON (O) discuss NOUN (O) several ADJ (O) existing VERB (O) [parallel NOUN (B) waveform PROPN (I) models NOUN (I)] , and CCONJ (O) explore VERB (O) a NOUN (O) new ADJ (O) alternative NOUN (O) in ADP (O) the DET (O) system NOUN . (O) 

Flow NOUN - based VERB (O) [waveform PROPN (B) models NOUN (I)] 
 Inverse PROPN (O) autoregressive ADJ (O) flow NOUN (O) (IAF PROPN) (O) is AUX (O) a NOUN (O) special ADJ (O) type NOUN (O) of ADP (O) normalizing NOUN (O) flow NOUN (O) where ADV (O) each DET (O) invertible ADJ (O) transformation NOUN (O) is AUX (O) based VERB (O) on ADP (O) an DET (O) [autoregressive ADJ (B) neural NOUN (I) network NOUN (I)] . 
 IAF PROPN (O) performs VERB (O) synthesis NOUN (O) in ADP (O) parallel NOUN (O) and CCONJ (O) can VERB (O) easily ADV (O) reuse NOUN (O) the DET (O) expressive NOUN (O) [autoregressive ADJ (B) architecture NOUN (I)] , such ADJ (O) as SCONJ (O) [WaveNet PROPN (B)] , which DET (O) leads VERB (O) to ADP (O) the DET (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB (O) for ADP (O) [speech NOUN (B) synthesis NOUN (I)] . 
 However ADV , (O) the DET (O) likelihood NOUN (O) evaluation NOUN (O) in ADP (O) IAF PROPN (O) is AUX (O) autoregressive ADJ (O) and CCONJ (O) slow ADJ , (O) thus ADV (O) previous ADJ (O) training NOUN (O) methods NOUN (O) rely VERB (O) on ADP (O) probability NOUN (O) density NOUN (O) distillation NOUN (O) from ADP (O) a NOUN (O) pretrained VERB (O) [autoregressive ADJ (B) WaveNet PROPN (I)] . 
 This DET (O) two NUM - stage NOUN (O) distillation NOUN (O) process NOUN (O) complicates VERB (O) the DET (O) training NOUN (O) pipeline NOUN (O) and CCONJ (O) may VERB (O) introduce VERB (O) pathological ADJ (O) optimization NOUN . (O) 
 RealNVP PUNCT (O)     and CCONJ (O) Glow VERB (O) are AUX (O) different ADJ (O) types NOUN (O) of ADP (O) normalizing NOUN (O) flows VERB , (O) where ADV (O) both DET (O) synthesis NOUN (O) and CCONJ (O) likelihood NOUN (O) evaluation NOUN (O) can VERB (O) be AUX (O) performed VERB (O) in ADP (O) parallel NOUN (O) by ADP (O) enforcing VERB (O) bipartite NOUN (O) architecture NOUN (O) constraints NOUN . (O) 
 Most ADJ (O) recently ADV , (O) both DET (O) of ADP (O) them PRON (O) were AUX (O) applied VERB (O) as SCONJ (O) [parallel NOUN (B) neural NOUN (I) vocoders NOUN (I)] and CCONJ (O) can VERB (O) be AUX (O) trained VERB (O) from ADP (O) scratch NOUN . (O) However ADV , (O) these DET (O) models NOUN (O) are AUX (O) less ADJ (O) expressive NOUN (O) than SCONJ (O) their DET (O) autoregressive ADJ (O) and CCONJ (O) IAF PROPN (O) counterparts NOUN (O) 
 One NUM (O) can VERB (O) find VERB (O) a NOUN (O) detailed ADJ (O) analysis NOUN (O) in ADP (O) WaveFlow PROPN (O) paper NOUN . (O) 
 In ADP (O) general ADJ , (O) these DET (O) bipartite NOUN (O) flows VERB (O) require VERB (O) larger ADJ (O) number NOUN (O) of ADP (O) layers NOUN (O) and CCONJ (O) hidden VERB (O) units NOUN , (O) which DET (O) lead NOUN (O) to ADP (O) huge ADJ (O) number NOUN (O) of ADP (O) parameters NOUN . (O) For ADP (O) example NOUN , (O) a NOUN (O) [WaveGlow PROPN (B) vocoder NOUN (I)]     has AUX (O) 87.88 NUM (O) M PROPN (O) parameters NOUN , (O) whereas SCONJ (O) [IAF PROPN (B) vocoder NOUN (I)] has AUX (O) much ADJ (O) smaller ADJ (O) footprint NOUN (O) with ADP (O) only ADV (O) 2.17 NUM (O) M PROPN (O) parameters NOUN , (O) making NOUN (O) it PRON (O) more ADJ (O) preferred ADJ (O) in ADP (O) production NOUN (O) deployment NOUN . (O) 

[WaveVAE NUM (B)] 
 Given VERB (O) the DET (O) advantage NOUN (O) of ADP (O) [IAF PROPN (B) vocoder NOUN (I)] , it PRON (O) is AUX (O) interesting ADJ (O) to ADP (O) investigate VERB (O) whether SCONJ (O) it PRON (O) can VERB (O) be AUX (O) trained VERB (O) without ADP (O) the DET (O) density NOUN (O) distillation NOUN . (O) 
 One NUM (O) related ADJ (O) work NOUN (O) trains NOUN (O) IAF PROPN (O) within ADP (O) an DET (O) autoencoder NOUN . (O) 
 Our DET (O) method NOUN (O) uses VERB (O) the DET (O) VAE PROPN (O) framework NOUN , (O) thus ADV (O) it PRON (O) is AUX (O) termed VERB (O) as SCONJ (O) [WaveVAE NUM (B)] . 
 In ADP (O) contrast NOUN (O) to PART , (O) [WaveVAE NUM (B)] can VERB (O) be AUX (O) trained VERB (O) from ADP (O) scratch NOUN (O) by ADP (O) jointly ADV (O) optimizing NOUN (O) the DET (O) [encoder NOUN (B)] qÏ† PROPN (O) (z|x PUNCT , (O) c NOUN) (O) and CCONJ (O) [decoder NOUN (B)] pÎ¸ NOUN (O) (x|z PROPN , (O) c NOUN) , (O) where ADV (O) z NOUN (O) is AUX (O) latent NOUN (O) variables X (O) and CCONJ (O) c NOUN (O) is AUX (O) the DET (O) [mel X (B) spectrogram PROPN (I)] conditioner NOUN . (O) 
 We PRON (O) omit NOUN (O) c NOUN (O) for ADP (O) concise NOUN (O) notation NOUN (O) hereafter NOUN . (O) 

[ENCODER PROPN (B)] 
 The DET (O) [encoder NOUN (B)] of ADP (O) [WaveVAE NUM (B)] qÏ† PROPN (O) (z|x PUNCT) (O) is AUX (O) parameterized VERB (O) by ADP (O) a NOUN (O) [Gaussian PROPN (B) autoregressive ADJ (I) WaveNet PROPN (I)] that DET (O) maps NOUN (O) the DET (O) [ground NOUN (B) truth NOUN (I) audio NOUN (I)] x X (O) into ADP (O) the DET (O) same ADJ (O) length NOUN (O) latent NOUN (O) representation NOUN (O) z. NOUN (O) 
 Specifically ADV , (O) the DET (O) [Gaussian PROPN (B) WaveNet PROPN (I) models NOUN (I)] xt NOUN (O) given VERB (O) the DET (O) previous ADJ (O) samples NOUN (O) x NOUN (O) < t NOUN (O) as SCONJ (O) xt NOUN (O) âˆ¼ PROPN (O) 


Note NOUN (O) that SCONJ , (O) the DET (O) mean VERB (O) Âµ(x NOUN (O) < t NOUN (O) ; Ï† X) (O) and CCONJ (O) scale NOUN (O) Ïƒ(x PROPN (O) < t NOUN) (O) are AUX (O) applied VERB (O) for ADP (O) â€œ PUNCT (O) whitening NOUN (O) â€ PUNCT (O) the DET (O) posterior NOUN (O) distribution NOUN . (O) 
 We PRON (O) introduce VERB (O) a NOUN (O) trainable NOUN (O) scalar NOUN (O) Îµ NOUN (O) > 0 PUNCT (O) to ADP (O) decouple NOUN (O) the DET (O) global ADJ (O) variation NOUN , (O) which DET (O) will VERB (O) make VERB (O) optimization NOUN (O) process NOUN (O) easier ADJ . (O) 
 Given VERB (O) the DET (O) observed VERB (O) x CCONJ , (O) the DET (O) qÏ† PROPN (O) (z|x PUNCT) (O) admits VERB (O) parallel NOUN (O) sampling NOUN (O) of ADP (O) latents NOUN (O) z. NOUN (O) 
 One NUM (O) can VERB (O) build NOUN (O) the DET (O) connection NOUN (O) between ADP (O) the DET (O) [encoder NOUN (B)] of ADP (O) [WaveVAE NUM (B)] and CCONJ (O) the DET (O) teacher NOUN (O) model NOUN (O) of ADP (O) [ClariNet PROPN (B)] , as SCONJ (O) both DET (O) of ADP (O) them PRON (O) use NOUN (O) a NOUN (O) [Gaussian PROPN (B) WaveNet PROPN (I)] to PART (O) guide VERB (O) the DET (O) training NOUN (O) of ADP (O) IAF PROPN (O) for ADP (O) parallel NOUN (O) wave NOUN (O) generation NOUN . (O) 

[DECODER PROPN (B)] 
 Our DET (O) [decoder NOUN (B)] pÎ¸ NOUN (O) (x|z PUNCT) (O) is AUX (O) parameterized VERB (O) by ADP (O) the DET (O) one NUM - step NOUN - ahead NOUN (O) predictions NOUN (O) from ADP (O) an DET (O) IAF PROPN . (O) 
 We PRON (O) let VERB (O) z NOUN (O) (0 PUNCT) (O) = z NOUN (O) and CCONJ (O) apply VERB (O) a NOUN (O) stack NOUN (O) of ADP (O) IAF PROPN (O) transformations NOUN (O) from ADP (O) z NOUN (O) (0 PUNCT) (O) â†’ ADP ... PUNCT (O) z NOUN (O) (i NOUN) (O) â†’ ADP ... PUNCT (O) z NOUN (O) (n PROPN) , (O) and CCONJ (O) each DET (O) transformation NOUN (O) 

where ADV (O) Âµt PRON (O) = Âµ(z NOUN (O) < t NOUN (O) ; Î¸ X) (O) and CCONJ (O) Ïƒt NOUN (O) = Ïƒ(z PROPN (O) < t NOUN (O) ; Î¸ X) (O) are AUX (O) shifting NOUN (O) and CCONJ (O) scaling VERB (O) variables X (O) modeled VERB (O) by ADP (O) a NOUN (O) [Gaussian PROPN (B) WaveNet PROPN (I)] . 
 One NUM (O) can VERB (O) show NOUN (O) that SCONJ , (O) given VERB (O) z NOUN (O) (0 PUNCT) (O) âˆ¼ PROPN (O) N NOUN (O) (Âµ(0 PUNCT) , (O) Ïƒ NOUN (O) (0 PUNCT)) (O) from ADP (O) the DET (O) 

[Gaussian PROPN (B)] prior ADV (O) or CCONJ (O) [encoder NOUN (B)] , the DET (O) per ADP - step NOUN (O) p(zt NOUN (O) | z NOUN (O) < t NOUN) (O) also ADV (O) follows VERB (O) [Gaussian PROPN (B)] with ADP (O) scale NOUN (O) and CCONJ (O) mean VERB (O) as SCONJ , (O) 


Lastly ADV , (O) we PRON (O) set NOUN (O) x NOUN (O) =  NOUN (O) Â· PUNCT (O) Ïƒ NOUN (O) tot NOUN (O) + Âµtot ADJ , (O) where ADV (O)  NOUN (O) âˆ¼ PROPN (O) N NOUN (O) (0 NUM , (O) I) . (O) Thus ADV , (O) pÎ¸ NOUN (O) (x NOUN (O) | z PROPN) (O) = N NOUN (O) (Âµtot ADJ , (O) Ïƒ NOUN (O) tot NOUN) . (O) For ADP (O) the DET (O) generative PROPN (O) process NOUN , (O) we PRON (O) use NOUN (O) the DET (O) standard NOUN (O) [Gaussian PROPN (B)] prior ADV (O) p(z NOUN) (O) = N NOUN (O) (0 NUM , (O) I) . (O) 
                     
TRAINING NOUN (O) OBJECTIVE PROPN (O) 
 We PRON (O) maximize NOUN (O) the DET (O) evidence NOUN (O) lower ADJ (O) bound VERB (O) (ELBO PROPN) (O) for ADP (O) observed VERB (O) x NOUN (O) in ADP (O) VAE PROPN , (O) 
     
                                                                
where ADV (O) the DET (O) KL PROPN (O) divergence NOUN (O) can VERB (O) be AUX (O) calculated VERB (O) in ADP (O) closed VERB - form NOUN (O) as SCONJ (O) both DET (O) qÏ† PROPN (O) (z|x PUNCT) (O) and CCONJ (O) p(z NOUN) (O) are AUX (O) [Gaussians PROPN (B)] , 


                                                                           
The DET (O) reconstruction NOUN (O) term NOUN (O) in ADP (O) Eq PROPN . (O) is AUX (O) intractable ADJ (O) to ADP (O) compute NOUN (O) exactly ADV . (O) 
 We PRON (O) do AUX (O) stochastic NOUN (O) optimization NOUN (O) by ADP (O) drawing VERB (O) a NOUN (O) sample NOUN (O) z NOUN (O) from ADP (O) the DET (O) [encoder NOUN (B)] qÏ† PROPN (O) (z|x PUNCT) (O) through ADP (O) the DET (O) reparameterization NOUN (O) trick NOUN , (O) and CCONJ (O) evaluating VERB (O) the DET (O) likelihood NOUN (O) log NOUN (O) pÎ¸ NOUN (O) (x|z PUNCT) . (O) 
 To NOUN (O) avoid NOUN (O) the DET (O) â€œ PUNCT (O) posterior NOUN (O) collapse NOUN (O) â€ PUNCT , (O) in ADP (O) which DET (O) the DET (O) posterior NOUN (O) distribution NOUN (O) qÏ† PROPN (O) (z|x PUNCT) (O) quickly ADV (O) collapses VERB (O) to ADP (O) the DET (O) white NOUN (O) noise NOUN (O) prior ADJ (O) p(z NOUN) (O) at ADP (O) the DET (O) early ADV (O) stage NOUN (O) of ADP (O) training NOUN , (O) we PRON (O) apply VERB (O) the DET (O) annealing VERB (O) strategy NOUN (O) for ADP (O) KL PROPN (O) divergence NOUN , (O) where ADV (O) its DET (O) weight NOUN (O) is AUX (O) gradually ADV (O) increased VERB (O) from ADP (O) 0 PUNCT (O) to ADP (O) 1 NUM , (O) via ADP (O) a NOUN (O) [sigmoid NOUN (B) function NOUN (I)] .                    
Through ADP (O) it PRON , (O) the DET (O) [encoder NOUN (B)] can VERB (O) encode NOUN (O) sufficient ADJ (O) information NOUN (O) into ADP (O) the DET (O) latent NOUN (O) representations NOUN (O) at ADP (O) the DET (O) early ADV (O) training NOUN , (O) and CCONJ (O) then ADV (O) gradually ADV (O) regularize NOUN (O) the DET (O) latent NOUN (O) representation NOUN (O) by ADP (O) increasing VERB (O) the DET (O) weight NOUN (O) of ADP (O) the DET (O) KL PROPN (O) divergence NOUN . (O)                                              
[STFT PROPN (B) loss NOUN (I)] : Similar ADJ (O) to PART , (O) we PRON (O) also ADV (O) add VERB (O) a NOUN (O) short ADJ - term NOUN (O) [Fourier PROPN (B) transform NOUN (I)] ([STFT PROPN (B)]) loss NOUN (O) to ADP (O) improve VERB (O) the DET (O) quality NOUN (O) of ADP (O) [synthesized VERB (B) speech NOUN (I)] . 
 We PRON (O) define NOUN (O) the DET (O) [STFT PROPN (B) loss NOUN (I)] as SCONJ (O) the DET (O) summation NOUN (O) of ADP (O) ` 2 NUM (O) loss NOUN (O) on ADP (O) the DET (O) magnitudes VERB (O) of ADP (O) [STFT PROPN (B)] and CCONJ (O) ` 1 NUM (O) loss NOUN (O) on ADP (O) the DET (O) [log NOUN - magnitudes NOUN (B)] of ADP (O) [STFT PROPN (B)] between ADP (O) the DET (O) output NOUN (O) [audio NOUN (B)] and CCONJ (O) [ground NOUN (B) truth NOUN (I) audio NOUN (I)] . 
 For ADP (O) [STFT PROPN (B)] , we PRON (O) use NOUN (O) a NOUN (O) 12.5ms PROPN (O) frame NOUN - shift NOUN , (O) 50ms NOUN (O) Hanning PROPN (O) window NOUN (O) length NOUN , (O) and CCONJ (O) we PRON (O) set NOUN (O) the DET (O) FFT PROPN (O) size NOUN (O) to ADP (O) 2048 NUM . (O) 
 We PRON (O) consider VERB (O) two NUM (O) [STFT PROPN (B) losses NOUN (I)] in ADP (O) our DET (O) objective NOUN (O) : (i NOUN) (O) the DET (O) [STFT PROPN (B) loss NOUN (I)] between ADP (O) [ground NOUN (B) truth NOUN (I) audio NOUN (I)] and CCONJ (O) reconstructed VERB (O) [audio NOUN (B)] using VERB (O) [encoder NOUN (B)] qÏ† PROPN (O) (z|x PUNCT) (O) ; (ii PROPN) (O) the DET (O) [STFT PROPN (B) loss NOUN (I)] between ADP (O) [ground NOUN (B) truth NOUN (I) audio NOUN (I)] and CCONJ (O) synthesized VERB (O) [audio NOUN (B)] using VERB (O) the DET (O) prior ADJ (O) p(z NOUN) , (O) with ADP (O) the DET (O) purpose NOUN (O) of ADP (O) reducing VERB (O) the DET (O) ap PROPN (O) between ADP (O) reconstruction NOUN (O) and CCONJ (O) synthesis NOUN . (O) 
 Our DET (O) final ADJ (O) loss NOUN (O) is AUX (O) a NOUN (O) linear NOUN (O) combination NOUN (O) of ADP (O) VAE PROPN (O) objective NOUN (O) in ADP (O) Eq PROPN . (O) (4 NUM) (O) and CCONJ (O) the DET (O) [STFT PROPN (B) losses NOUN (I)] . 
 The DET (O) corresponding VERB (O) coefficients NOUN (O) are AUX (O) simply ADV (O) set NOUN (O) to ADP (O) be AUX (O) one NUM (O) in ADP (O) all DET (O) of ADP (O) our DET (O) experiments NOUN . (O) 

Experiment NOUN (O) 
 In ADP (O) this DET (O) section NOUN , (O) we PRON (O) present NOUN (O) several ADJ (O) experiments VERB (O) to ADP (O) evaluate VERB (O) the DET (O) proposed VERB (O) [ParaNet NOUN (B)] and CCONJ (O) [WaveVAE NUM (B)] . 
 Settings PROPN (O) 
 Data PROPN (O) : In ADP (O) our DET (O) experiment NOUN , (O) we PRON (O) use NOUN (O) an DET (O) internal ADJ (O) English PROPN (O) [speech NOUN (B) dataset NOUN (I)] containing VERB (O) about ADP (O) 20 NUM (O) hours NOUN (O) of ADP (O) [speech NOUN (B) data NOUN (I)] from ADP (O) a NOUN (O) [female NOUN (B) speaker NOUN (I)] with ADP (O) a NOUN (O) sampling NOUN (O) rate NOUN (O) of ADP (O) 48 NUM (O) kHz PROPN . (O) 
 We PRON (O) downsample NOUN (O) the DET (O) [audios NOUN (B)] to PART (O) 24 NUM (O) kHz PROPN . (O) 
 [Text NOUN - to ADP - spectrogram NOUN (B) models NOUN (I)] : For ADP (O) both DET (O) [ParaNet NOUN (B)] and CCONJ (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM (O) (DV3 PROPN) , (O) we PRON (O) use NOUN (O) the DET (O) mixed ADJ (O) representation NOUN (O) of ADP (O) characters NOUN (O) and CCONJ (O) [phonemes NOUN (B)] . 
 The DET (O) default NOUN (O) [hyperparameters NOUN (B)] of ADP (O) [ParaNet NOUN (B)] and CCONJ (O) DV3 PROPN (O) are AUX (O) provided VERB (O) in ADP (O)     Table NOUN . (O) 
 Both DET (O) [ParaNet NOUN (B)] and CCONJ (O) DV3 PROPN (O) are AUX (O) trained VERB (O) for ADP (O) 500000 NUM (O) steps VERB (O) using VERB (O) [Adam PROPN (B) optimizer NOUN (I)] . 
 We PRON (O) find VERB (O) that SCONJ (O) larger ADJ (O) kernel NOUN (O) width NOUN (O) and CCONJ (O) deeper NOUN (O) layers NOUN (O) generally ADV (O) help NOUN (O) improve VERB (O) the DET (O) performance NOUN (O) of ADP (O) [ParaNet NOUN (B)] . 
 In ADP (O) terms NOUN (O) of ADP (O) the DET (O) number NOUN (O) of ADP (O) parameters NOUN , (O) our DET (O) [ParaNet NOUN (B)] (17.61 NUM (O) M PROPN (O) params NOUN) (O) is AUX (O) 2.57Ã— NUM (O) larger ADJ (O) than SCONJ (O) the DET (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM (O) (6.85 NUM (O) M NOUN (O) params NOUN) (O) and CCONJ (O) 1.71Ã— PROPN (O) smaller ADJ (O) than SCONJ (O) the DET (O) [FastSpeech PROPN (B)] (30.1 NUM (O) M PROPN (O) params NOUN) . (O) 
 We PRON (O) use NOUN (O) an DET (O) [open ADJ (B) source NOUN (I)] reimplementation NOUN (O) of ADP (O) [FastSpeech PROPN (B)] 1 NUM (O) by ADP (O) adapting VERB (O) the DET (O) [hyperparameters NOUN (B)] for ADP (O) handling VERB (O) the DET (O) [24kHz NUM (B) dataset NOUN (I)] . 
 [Neural PROPN (B) vocoders NOUN (I)] : In ADP (O) this DET (O) work NOUN , (O) we PRON (O) compare VERB (O) various ADJ (O) [neural NOUN (B) vocoders NOUN (I) paired VERB (I)] with ADP (O) [text NOUN - to ADP - spectrogram NOUN (B) models NOUN (I)] , including VERB (O) [WaveNet PROPN (B)] , [ClariNet PROPN (B)] , [WaveVAE NUM (B)] , and CCONJ (O) [WaveGlow PROPN (B)] . 
 We PRON (O) train NOUN (O) all DET (O) [neural NOUN (B) vocoders NOUN (I)] on ADP (O) 8 NUM (O) Nvidia PROPN (O) 1080Ti PROPN (O) [GPUs NOUN (B)] using VERB (O) randomly ADV (O) chosen VERB (O) 0.5s X (O) [audio NOUN (B) clips NOUN (I)] . 
 We PRON (O) train NOUN (O) two NUM (O) 20-layer NUM (O) [WaveNets PROPN (B)] with ADP (O) residual ADJ (O) channel NOUN (O) 256 NUM (O) conditioned VERB (O) on ADP (O) the DET (O) predicted VERB (O) [mel X (B) spectrogram PROPN (I)] from ADP (O) [ParaNet NOUN (B)] and CCONJ (O) DV3 PROPN , (O) respectively ADV . (O) 
 We PRON (O) apply VERB (O) two NUM (O) layers NOUN (O) of ADP (O) [convolution NOUN (B) block NOUN (I)] to PART (O) process NOUN (O) the DET (O) predicted VERB (O) [mel X (B) spectrogram PROPN (I)] , and CCONJ (O) use NOUN (O) two NUM (O) layers NOUN (O) of ADP (O) transposed VERB (O) 2-D NUM (O) convolution NOUN (O) (in ADP (O) time NOUN (O) and CCONJ (O) frequency NOUN) (O) interleaved VERB (O) with ADP (O) leaky ADJ (O) [ReLU NOUN (B)] (Î± NOUN (O) = 0.4 NUM) (O) to ADP (O) upsample PROPN (O) the DET (O) outputs VERB (O) from ADP (O) [frame NOUN - level NOUN (B)] to PART (O) sample NOUN - level NOUN . (O) 
 We PRON (O) use NOUN (O) the DET (O) [Adam PROPN (B) optimizer NOUN (I)] ([Kingma PROPN (B)] & Ba PROPN , (O) 2015 NUM) (O) with ADP (O) a NOUN (O) [batch NOUN (B) size NOUN (I)] of ADP (O) 8 NUM (O) and CCONJ (O) a NOUN (O) [learning NOUN (B) rate NOUN (I)] of ADP (O) 0.001 NUM (O) at ADP (O) the DET (O) beginning NOUN , (O) which DET (O) is AUX (O) annealed VERB (O) by ADP (O) half NOUN (O) every DET (O) 200000 NUM (O) steps NOUN . (O) We PRON (O) train NOUN (O) the DET (O) models NOUN (O) for ADP (O) 1 NUM (O) M PROPN (O) steps NOUN . (O) 
 We PRON (O) use NOUN (O) the DET (O) same ADJ (O) IAF PROPN (O) architecture NOUN (O) as SCONJ (O) [ClariNet PROPN (B)] (Ping PROPN (O) et NOUN (O) al PROPN . , (O) 2018a NUM) . (O) 

https://github.com/xcmyz/FastSpeech NUM (O) 

 Table PROPN . (O) [Hyperparameters NOUN (B)] of ADP (O) [autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] and CCONJ (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] in ADP (O) the DET (O) experiment NOUN . (O) 



 Table PROPN . (O) The DET (O) model NOUN (O) footprint NOUN , (O) synthesis NOUN (O) time NOUN (O) for ADP (O) 1 NUM (O) second NOUN (O) [speech NOUN (B)] (on ADP (O) 1080Ti PROPN (O) with ADP (O) FP32 PROPN) , (O) and CCONJ (O) the DET (O) 5-scale NUM (O) [Mean PROPN (B) Opinion NOUN (I) Score PROPN (I)] ([MOS PROPN (B)]) ratings NOUN (O) with ADP (O) 95 NUM (O) % confidence NOUN (O) intervals NOUN (O) for ADP (O) comparison NOUN . (O) 




It PRON (O) consists VERB (O) of ADP (O) four NUM (O) stacked VERB (O) [Gaussian PROPN (B)] IAF PROPN (O) blocks PROPN , (O) which DET (O) are AUX (O) parameterized VERB (O) by ADP (O) 10 NUM , (O) 10 NUM , (O) 10 NUM , (O) 30-layer NUM (O) [WaveNets PROPN (B)] respectively ADV , (O) with ADP (O) the DET (O) 64 NUM (O) residual ADJ (O) & skip PROPN (O) channels NOUN (O) and CCONJ (O) filter NOUN (O) size NOUN (O) 3 NUM (O) in ADP (O) dilated VERB (O) convolutions NOUN . (O) 
 The DET (O) IAF PROPN (O) is AUX (O) conditioned VERB (O) on ADP (O) [log PROPN - mel PROPN (B) spectrograms VERB (I)] with ADP (O) two NUM (O) layers NOUN (O) of ADP (O) transposed VERB (O) 2-D NUM (O) convolution NOUN (O) as SCONJ (O) in ADP (O) [ClariNet PROPN (B)] . We PRON (O) use NOUN (O) the DET (O) same ADJ (O) teacher NOUN - student NOUN (O) setup NOUN (O) for ADP (O) [ClariNet PROPN (B)] as SCONJ (O) in ADP (O)     and CCONJ (O) we PRON (O) train NOUN (O) a NOUN (O) 20-layer NUM (O) [Gaussian PROPN (B) autoregressive ADJ (I) WaveNet PROPN (I)] as SCONJ (O) the DET (O) teacher NOUN (O) model NOUN . (O) 
 For ADP (O) the DET (O) [encoder NOUN (B)] in ADP (O) [WaveVAE NUM (B)] , we PRON (O) also ADV (O) use NOUN (O) a NOUN (O) 20-layers NUM (O) [Gaussian PROPN (B) WaveNet PROPN (I)] conditioned VERB (O) on ADP (O) [log PROPN - mel PROPN (B) spectrograms VERB (I)] . 
 For ADP (O) the DET (O) [decoder NOUN (B)] , we PRON (O) use NOUN (O) the DET (O) same ADJ (O) architecture NOUN (O) as SCONJ (O) the DET (O) distilled PROPN (O) IAF PROPN . (O) 
 Both DET (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B)] of ADP (O) [WaveVAE NUM (B)] share NOUN (O) the DET (O) same ADJ (O) conditioner NOUN (O) network NOUN . (O) Both DET (O) of ADP (O) the DET (O) distilled PROPN (O) IAF PROPN (O) and CCONJ (O) [WaveVAE NUM (B)] are AUX (O) trained VERB (O) on ADP (O) [ground NOUN - truth NOUN (B) mel PROPN (I) spectrogram PROPN (I)] . 
 We PRON (O) use NOUN (O) [Adam PROPN (B) optimizer NOUN (I)] with ADP (O) 1000000 NUM (O) steps VERB (O) for ADP (O) distilled PROPN (O) IAF PROPN . (O) 
 For ADP (O) [WaveVAE NUM (B)] , we PRON (O) train NOUN (O) it PRON (O) for ADP (O) 400000 NUM (O) because SCONJ (O) it PRON (O) converges NOUN (O) much ADJ (O) faster ADV . (O) 
 The DET (O) learning NOUN (O) rate NOUN (O) is AUX (O) set NOUN (O) to ADP (O) 0.001 NUM (O) at ADP (O) the DET (O) beginning NOUN (O) and CCONJ (O) annealed VERB (O) by ADP (O) half NOUN (O) every DET (O) 200000 NUM (O) steps NOUN (O) for ADP (O) both DET (O) models NOUN . (O) 
 We PRON (O) use NOUN (O) the DET (O) [open ADJ (B) source NOUN (I) implementation NOUN (I)] of ADP (O) [WaveGlow PROPN (B)] with ADP (O) default NOUN (O) [hyperparameters NOUN (B)] (residual ADJ (O) channel NOUN (O) 256 NUM) (O) 2 NUM , (O) except SCONJ (O) change NOUN (O) the DET (O) sampling NOUN (O) rate NOUN (O) from ADP (O) 22.05kHz NUM (O) to ADP (O) 24kHz NOUN , (O) FFT PROPN (O) window NOUN (O) length NOUN (O) from ADP (O) 1024 NUM (O) to ADP (O) 1200 NUM , (O) and CCONJ (O) FFT PROPN (O) window NOUN (O) shift NOUN (O) from ADP (O) 256 NUM (O) to ADP (O) 300 NUM (O) for ADP (O) handling VERB (O) the DET (O) [24kHz NUM (B) dataset NOUN (I)] . 
 The DET (O) model NOUN (O) is AUX (O) trained VERB (O) for ADP (O) 2 NUM (O) M PROPN (O) steps NOUN . (O) 

Results NOUN (O) 
 [Speech NOUN (B) quality NOUN (I)] : We PRON (O) use NOUN (O) the DET (O) crowdMOS X (O) [toolkit NOUN (B)] for ADP (O) subjective ADJ (O) [Mean PROPN (B) Opinion NOUN (I) Score PROPN (I) (MOS PROPN) (I) evaluation NOUN (I)] . We PRON (O) report NOUN (O) the DET (O) [MOS PROPN (B) results NOUN (I)] in ADP (O)     Table NOUN . (O) The DET (O) [ParaNet NOUN (B)] can VERB (O) provide VERB (O) comparable ADJ (O) quality NOUN (O) of ADP (O) [speech NOUN (B)] as SCONJ (O) the DET (O) autoregressive ADJ (O) DV3 PROPN (O) using VERB (O) [WaveNet PROPN (B) vocoder NOUN (I)] ([MOS PROPN (B)] : 4.09 NUM (O) vs. X (O) 4.01 NUM) . (O) 

https://github.com/NVIDIA/waveglow PROPN (O) 

 Table PROPN . (O) [Attention NOUN (B) error NOUN (I)] counts VERB (O) for ADP (O) [text NOUN - to ADP - spectrogram NOUN (B) models NOUN (I)] on ADP (O) the DET (O) 100-sentence NUM (O) test NOUN (O) set VERB . (O) One NUM (O) or CCONJ (O) more ADJ (O) mispronunciations NOUN , (O) skips VERB , (O) and CCONJ (O) repeats VERB (O) count NOUN (O) as SCONJ (O) a NOUN (O) single ADJ (O) mistake NOUN (O) per ADP (O) utterance NOUN . (O) 
 The DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] (17-layer NUM (O) [decoder NOUN (B)]) with ADP (O) attention NOUN (O) mask NOUN (O) obtains NOUN (O) the DET (O) fewest NOUN (O) [attention NOUN (B) errors NOUN (I)] in ADP (O) total NOUN . (O) 
 For ADP (O) ablation NOUN (O) study NOUN , (O) we PRON (O) include VERB (O) the DET (O) results VERB (O) for ADP (O) two NUM (O) additional ADJ (O) [ParaNet NOUN (B) models NOUN (I)] . They PRON (O) have AUX (O) 6 NUM (O) and CCONJ (O) 12 NUM (O) [decoder NOUN (B) layers NOUN (I)] and CCONJ (O) are AUX (O) denoted VERB (O) as SCONJ (O) ParaNet-6 PROPN (O) and CCONJ (O) ParaNet-12 PROPN , (O) respectively ADV . (O) 


When ADV (O) we PRON (O) use NOUN (O) the DET (O) [ClariNet PROPN (B) vocoder NOUN (I)] , [ParaNet NOUN (B)] can VERB (O) still ADV (O) provide VERB (O) reasonably ADV (O) good NOUN (O) [speech NOUN (B) quality NOUN (I)] ([MOS PROPN (B)] : 3.62 X) (O) as SCONJ (O) a NOUN (O) fully ADV (O) [feed NOUN - forward NOUN (B) TTS PROPN (I) system NOUN (I)] . [WaveVAE NUM (B)] obtains VERB (O) worse ADJ (O) results VERB (O) than SCONJ (O) distilled PROPN (O) [IAF PROPN (B) vocoder NOUN (I)] , but CCONJ (O) it PRON (O) can VERB (O) be AUX (O) trained VERB (O) from ADP (O) scratch NOUN (O) and CCONJ (O) simplifies NOUN (O) the DET (O) training NOUN (O) pipeline NOUN . (O) 
 When ADV (O) conditioned VERB (O) on ADP (O) predicted VERB (O) [mel X (B) spectrogram PROPN (I)] , [WaveGlow PROPN (B)] tends VERB (O) to ADP (O) produce NOUN (O) constant ADJ (O) frequency NOUN (O) artifacts NOUN . (O) 
 To NOUN (O) remedy NOUN (O) this DET , (O) we PRON (O) applied VERB (O) the DET (O) denoising VERB (O) function NOUN (O) with ADP (O) strength NOUN (O) 0.1 NUM , (O) as SCONJ (O) recommended VERB (O) in ADP (O) the DET (O) repository NOUN (O) of ADP (O) [WaveGlow PROPN (B)] . 
 It PRON (O) is AUX (O) effective ADJ (O) when ADV (O) the DET (O) predicted VERB (O) [mel X (B) spectrograms VERB (I)] are AUX (O) from ADP (O) DV3 PROPN , (O) but CCONJ (O) not PART (O) effective ADJ (O) when ADV (O) the DET (O) predicted VERB (O) [mel X (B) spectrograms VERB (I)] are AUX (O) from ADP (O) [ParaNet NOUN (B)] . As SCONJ (O) a NOUN (O) result VERB , (O) the DET (O) [MOS PROPN (B) score NOUN (I)] degrades NOUN (O) seriously ADV . (O) 
 We PRON (O) add VERB (O) the DET (O) comparison NOUN (O) with ADP (O) [FastSpeech PROPN (B)] after ADP (O) the DET (O) paper NOUN (O) submission NOUN . (O) Because SCONJ (O) it PRON (O) is AUX (O) costly ADJ (O) to ADP (O) relaunch NOUN (O) the DET (O) [MOS PROPN (B) evaluations NOUN (I)] of ADP (O) all DET (O) the DET (O) models NOUN , (O) we PRON (O) perform VERB (O) a NOUN (O) separate ADJ (O) [MOS PROPN (B) evaluation NOUN (I)] for ADP (O) [FastSpeech PROPN (B)] . 
 Note NOUN (O) that SCONJ , (O) the DET (O) group NOUN (O) of ADP (O) human NOUN (O) raters NOUN (O) can VERB (O) be AUX (O) different ADJ (O) on ADP (O) Mechanical PROPN (O) Turk PROPN , (O) and CCONJ (O) the DET (O) subjective ADJ (O) scores NOUN (O) may VERB (O) not PART (O) be AUX (O) directly ADV (O) comparable ADJ . (O) 
 One NUM (O) can VERB (O) find VERB (O) the DET (O) [synthesized VERB (B) speech NOUN (I) samples NOUN (I)] in ADP (O) : https://parallel-neural-tts-demo.github.io/. NOUN (O)                                
                                                                    
Synthesis PROPN (O) speed NOUN (O) : We PRON (O) test NOUN (O) synthesis NOUN (O) speed NOUN (O) of ADP (O) all DET (O) models NOUN (O) on ADP (O) NVIDIA PROPN (O) GeForce PROPN (O) GTX PROPN (O) 1080 NUM (O) Ti PROPN (O) with ADP (O) 32-bit NUM (O) floating VERB (O) point(FP32 PROPN) (O) arithmetic ADJ . (O) 
 We PRON (O) compare VERB (O) the DET (O) [ParaNet NOUN (B)] with ADP (O) the DET (O) autoregressive ADJ (O) DV3 PROPN (O) in ADP (O) terms NOUN (O) of ADP (O) inference NOUN (O) latency NOUN . (O) 
 We PRON (O) construct VERB (O) a NOUN (O) custom NOUN (O) 15-sentence NUM (O) test NOUN (O) set NOUN (O) (see VERB (O) Appendix PROPN (O) A NOUN) (O) and CCONJ (O) run NOUN (O) inference NOUN (O) for ADP (O) 50 NUM (O) runs VERB (O) on ADP (O) each DET (O) of ADP (O) the DET (O) 15 NUM (O) sentences NOUN (O) ([batch NOUN (B) size NOUN (I)] is AUX (O) set NOUN (O) to ADP (O) 1 NUM) . (O) 
 The DET (O) average NOUN (O) [audio NOUN (B)] duration NOUN (O) of ADP (O) the DET (O) utterances VERB (O) is AUX (O) 6.11 NUM (O) seconds NOUN . (O) The DET (O) average NOUN (O) inference NOUN (O) latencies NOUN (O) over ADP (O) 50 NUM (O) runs VERB (O) and CCONJ (O) 15 NUM (O) sentences NOUN (O) are AUX (O) 0.024 NUM (O) and CCONJ (O) 1.12 NUM (O) seconds NOUN (O) for ADP (O) [ParaNet NOUN (B)] and CCONJ (O) DV3 PROPN , (O) respectively ADV . (O) 
 Hence ADV , (O) our DET (O) [ParaNet NOUN (B)] runs VERB (O) 254.6 NUM (O) times NOUN (O) faster ADV (O) than SCONJ (O) real ADJ - time NOUN (O) and CCONJ (O) brings VERB (O) about ADP (O) 46.7 NUM (O) times NOUN (O) speed NOUN - up NOUN (O) over ADP (O) its DET (O) small ADJ - footprint NOUN (O) [autoregressive ADJ (B) counterpart NOUN (I)] at ADP (O) synthesis NOUN . (O) 
 It PRON (O) also ADV (O) runs VERB (O) 1.58 NUM (O) times NOUN (O) faster ADV (O) than SCONJ (O) [FastSpeech PROPN (B)] . 
 We PRON (O) summarize VERB (O) synthesis NOUN (O) speed NOUN (O) of ADP (O) [TTS PROPN (B) systems NOUN (I)] in ADP (O)     Table NOUN . (O) 
 One NUM (O) can VERB (O) observe VERB (O) that SCONJ (O) the DET (O) latency NOUN (O) bottleneck NOUN (O) is AUX (O) the DET (O) [autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] , when ADV (O) the DET (O) system NOUN (O) uses VERB (O) [parallel NOUN (B) neural NOUN (I) vocoder NOUN (I)] . 
 The DET (O) [ClariNet PROPN (B)] and CCONJ (O) [WaveVAE NUM (B) vocoders NOUN (I)] have AUX (O) much ADJ (O) smaller ADJ (O) footprint NOUN (O) and CCONJ (O) faster ADV (O) synthesis NOUN (O) speed NOUN (O) than SCONJ (O) [WaveGlow PROPN (B)] . 

[Attention NOUN (B) error NOUN (I)] analysis NOUN (O) : In ADP (O) [autoregressive ADJ (B) models NOUN (I)] , there PRON (O) is AUX (O) a NOUN (O) noticeable ADJ (O) discrepancy NOUN (O) between ADP (O) the DET (O) teacher NOUN - forced VERB (O) training NOUN (O) and CCONJ (O) autoregressive ADJ (O) inference NOUN , (O) which DET (O) can VERB (O) yield NOUN (O) accumulated VERB (O) errors NOUN (O) along ADP (O) the DET (O) generated VERB (O) sequence NOUN (O) at ADP (O) synthesis NOUN . (O) 
 In ADP (O) [neural NOUN (B) TTS PROPN (I)] , this DET (O) discrepancy NOUN (O) leads VERB (O) to ADP (O) miserable ADJ (O) [attention NOUN (B) errors NOUN (I)] at ADP (O) autoregressive ADJ (O) inference NOUN , (O) including VERB (O) (i NOUN) (O) repeated VERB (O) words NOUN , (O) (ii PROPN) (O) mispronunciations NOUN , (O) and CCONJ (O) (iii PROPN) (O) skipped VERB (O) words NOUN (O) for ADP (O) detailed ADJ (O) examples NOUN) , (O) which DET (O) is AUX (O) a NOUN (O) critical ADJ (O) problem NOUN (O) for ADP (O) online NOUN (O) deployment NOUN (O) of ADP (O) [attention NOUN - based VERB (B) neural NOUN (I) TTS PROPN (I) systems NOUN (I)] . 
 We PRON (O) perform VERB (O) an DET (O) [attention NOUN (B) error NOUN (I)] analysis NOUN (O) for ADP (O) our DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] on ADP (O) a NOUN (O) 100-sentence NUM (O) test NOUN (O) set NOUN (O) (see VERB (O) Appendix PROPN (O) B) , (O) which DET (O) includes VERB (O) particularly ADV - challenging VERB (O) cases NOUN (O) from ADP (O) deployed VERB (O) [TTS PROPN (B)] systems NOUN (O) (e.g. ADV (O) dates VERB , (O) acronyms PROPN , (O) URLs PROPN , (O) repeated VERB (O) words NOUN , (O) proper NOUN (O) nouns NOUN , (O) and CCONJ (O) foreign ADJ (O) words NOUN) . (O) 
 In ADP (O)     Table NOUN , (O) we PRON (O) find VERB (O) that SCONJ (O) the DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] has AUX (O) much ADJ (O) fewer ADJ (O) [attention NOUN (B) errors NOUN (I)] than SCONJ (O) its DET (O) [autoregressive ADJ (B) counterpart NOUN (I)] at ADP (O) synthesis NOUN (O) (12 NUM (O) vs. X (O) 37 NUM) (O) without ADP (O) attention NOUN (O) mask NOUN . (O) 
 Although SCONJ (O) our DET (O) [ParaNet NOUN (B)] distills VERB (O) the DET (O) (teacher NOUN - forced VERB) (O) attentions NOUN (O) from ADP (O) an DET (O) [autoregressive ADJ (B) model NOUN (I)] , it PRON (O) only ADV (O) takes VERB (O) textual NOUN (O) inputs VERB (O) at ADP (O) both DET (O) training NOUN (O) and CCONJ (O) synthesis NOUN (O) and CCONJ (O) does AUX (O) not PART (O) have AUX (O) the DET (O) similar ADJ (O) discrepancy NOUN (O) as SCONJ (O) in ADP (O) [autoregressive ADJ (B) model NOUN (I)] . 
 In ADP (O) previous ADJ (O) work NOUN , (O) attention NOUN (O) masking NOUN (O) was AUX (O) applied VERB (O) to ADP (O) enforce NOUN (O) the DET (O) monotonic PROPN (O) attentions NOUN (O) and CCONJ (O) reduce VERB (O) [attention NOUN (B) errors NOUN (I)] , and CCONJ (O) was AUX (O) demonstrated VERB (O) to ADP (O) be AUX (O) effective ADJ (O) in ADP (O) [Deep ADJ (B) Voice PROPN (I)] 3 NUM . (O) 
 We PRON (O) find VERB (O) that SCONJ (O) our DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] still ADV (O) can VERB (O) have AUX (O) fewer ADJ (O) [attention NOUN (B) errors NOUN (I)] than SCONJ (O) autoregressive ADJ (O) DV3 PROPN (O) (6 NUM (O) vs. X (O) 8) NUM , (O) when ADV (O) both DET (O) of ADP (O) them PRON (O) use NOUN (O) the DET (O) attention NOUN (O) masking VERB . (O) 

Ablation NOUN (O) study NOUN (O) 
 We PRON (O) perform VERB (O) ablation NOUN (O) studies NOUN (O) to ADP (O) verify VERB (O) the DET (O) effectiveness NOUN (O) of ADP (O) several ADJ (O) techniques NOUN (O) used VERB (O) in ADP (O) [ParaNet NOUN (B)] , including VERB (O) [attention NOUN (B) distillation NOUN (I)] , positional ADJ (O) encoding NOUN , (O) and CCONJ (O) stacking VERB (O) [decoder NOUN (B) layers NOUN (I)] to PART (O) refine NOUN (O) the DET (O) [attention NOUN (B) alignment NOUN (I)] in ADP (O) a NOUN (O) layer NOUN - by ADP - layer NOUN (O) manner NOUN . (O) 
 We PRON (O) evaluate VERB (O) the DET (O) performance NOUN (O) of ADP (O) a NOUN (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I) model NOUN (I)] trained VERB (O) without ADP (O) [attention NOUN (B) distillation NOUN (I)] and CCONJ (O) find VERB (O) that SCONJ (O) it PRON (O) fails VERB (O) to ADP (O) learn VERB (O) meaningful ADJ (O) [attention NOUN (B) alignment NOUN (I)] . 
 The DET (O) synthesized VERB (O) [audios NOUN (B)] are AUX (O) unintelligible ADJ (O) and CCONJ (O) mostly ADV (O) pure ADJ (O) noise NOUN . (O) 
 Similarly ADV , (O) we PRON (O) train NOUN (O) another DET (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I) model NOUN (I)] without ADP (O) adding VERB (O) positional NOUN (O) encoding NOUN (O) in ADP (O) the DET (O) [attention NOUN (B) block NOUN (I)] . 
 The DET (O) resulting VERB (O) model NOUN (O) only ADV (O) learns VERB (O) very ADV (O) blurry NOUN (O) [attention NOUN (B) alignment NOUN (I)] and CCONJ (O) can VERB (O) not PART (O) synthesize VERB (O) [intelligible ADJ (B) speech NOUN (I)] . 
 Finally ADV , (O) we PRON (O) train NOUN (O) two NUM (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I) models NOUN (I)] with ADP (O) 6 NUM (O) and CCONJ (O) 12 NUM (O) [decoder NOUN (B) layers NOUN (I)] , respectively ADV , (O) and CCONJ (O) compare VERB (O) them PRON (O) with ADP (O) the DET (O) default NOUN (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I) model NOUN (I)] which DET (O) has AUX (O) 17 NUM (O) [decoder NOUN (B) layers NOUN (I)] . 
 We PRON (O) conduct NOUN (O) the DET (O) same ADJ (O) [attention NOUN (B) error NOUN (I)] analysis NOUN (O) on ADP (O) the DET (O) 100-sentence NUM (O) test NOUN (O) set NOUN (O) and CCONJ (O) the DET (O) results VERB (O) are AUX (O) shown VERB (O) in ADP (O)     Table NOUN . (O) 
 We PRON (O) find VERB (O) that DET (O) increasing VERB (O) the DET (O) number NOUN (O) of ADP (O) [decoder NOUN (B) layers NOUN (I)] for ADP (O) [non ADJ - autoregressive ADJ (B) ParaNet PROPN (I)] can VERB (O) reduce VERB (O) the DET (O) total NOUN (O) number NOUN (O) of ADP (O) [attention NOUN (B) errors NOUN (I)] , in ADP (O) both DET (O) cases NOUN (O) with ADP (O) and CCONJ (O) without ADP (O) applying VERB (O) attention NOUN (O) mask NOUN (O) at ADP (O) synthesis NOUN . (O) 

Conclusion NOUN (O) 
 In ADP (O) this DET (O) work NOUN , (O) we PRON (O) build NOUN (O) a NOUN (O) [feed NOUN - forward NOUN (B) neural NOUN (I) TTS PROPN (I) system NOUN (I)] by ADP (O) proposing VERB (O) a NOUN (O) [non ADJ - autoregressive ADJ (B) text NOUN - to ADP - spectrogram PROPN (I) model NOUN (I)] . 
 The DET (O) proposed VERB (O) [ParaNet NOUN (B)] obtains VERB (O) reasonably ADV (O) good NOUN (O) [speech NOUN (B) quality NOUN (I)] and CCONJ (O) brings VERB (O) 46.7 NUM (O) times NOUN (O) speed NOUN - up NOUN (O) over ADP (O) its DET (O) [autoregressive ADJ (B) counterpart NOUN (I)] at ADP (O) synthesis NOUN . (O) 
 We PRON (O) also ADV (O) compare VERB (O) various ADJ (O) [neural NOUN (B) vocoders NOUN (I)] within ADP (O) the DET (O) [TTS PROPN (B) system NOUN (I)] . 
 Our DET (O) results VERB (O) suggest VERB (O) that SCONJ (O) the DET (O) parallel NOUN (O) [vocoder NOUN (B)] is AUX (O) generally ADV (O) less ADJ (O) robust ADJ (O) than SCONJ (O) [WaveNet PROPN (B) vocoder NOUN (I)] , when ADV (O) the DET (O) [front ADJ - end NOUN (B) acoustic ADJ (I) model NOUN (I)] is AUX (O) [non ADJ - autoregressive ADJ (B)] . As SCONJ (O) a NOUN (O) result VERB , (O) it PRON (O) is AUX (O) interesting ADJ (O) to ADP (O) investigate VERB (O) small ADJ - footprint NOUN (O) and CCONJ (O) robust ADJ (O) [parallel NOUN (B) neural NOUN (I) vocoder NOUN (I)] (e.g. ADV , (O) WaveFlow PROPN) (O) in ADP (O) future NOUN (O) study NOUN . (O) 
